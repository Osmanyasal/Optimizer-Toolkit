#pragma once
#include <cstdint>
namespace optkit::amd64::fam10h{
	enum fam10h : uint64_t {
		DISPATCHED_FPU = 0x0, // Dispatched FPU Operations
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__OPS_ADD = 0x1, // Add pipe ops excluding load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__OPS_MULTIPLY = 0x2, // Multiply pipe ops excluding load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__OPS_STORE = 0x4, // Store pipe ops excluding load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__OPS_ADD_PIPE_LOAD_OPS = 0x8, // Add pipe load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__OPS_MULTIPLY_PIPE_LOAD_OPS = 0x10, // Multiply pipe load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__OPS_STORE_PIPE_LOAD_OPS = 0x20, // Store pipe load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM10H_DISPATCHED_FPU__ALL = 0x3f, // All sub-events selected
		CYCLES_NO_FPU_OPS_RETIRED = 0x1, // Cycles in which the FPU is Empty
		DISPATCHED_FPU_OPS_FAST_FLAG = 0x2, // Dispatched Fast Flag FPU Operations
		RETIRED_SSE_OPERATIONS = 0x3, // Retired SSE Operations
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__SINGLE_ADD_SUB_OPS = 0x1, // Single precision add/subtract ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__SINGLE_MUL_OPS = 0x2, // Single precision multiply ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__SINGLE_DIV_OPS = 0x4, // Single precision divide/square root ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__DOUBLE_ADD_SUB_OPS = 0x8, // Double precision add/subtract ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__DOUBLE_MUL_OPS = 0x10, // Double precision multiply ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__DOUBLE_DIV_OPS = 0x20, // Double precision divide/square root ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__OP_TYPE = 0x40, // FLOPS
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM10H_RETIRED_SSE_OPERATIONS__ALL = 0x7f, // All sub-events selected
		RETIRED_MOVE_OPS = 0x4, // Retired Move Ops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM10H_RETIRED_MOVE_OPS__LOW_QW_MOVE_UOPS = 0x1, // Merging low quadword move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM10H_RETIRED_MOVE_OPS__HIGH_QW_MOVE_UOPS = 0x2, // Merging high quadword move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM10H_RETIRED_MOVE_OPS__ALL_OTHER_MERGING_MOVE_UOPS = 0x4, // All other merging move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM10H_RETIRED_MOVE_OPS__ALL_OTHER_MOVE_UOPS = 0x8, // All other move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM10H_RETIRED_MOVE_OPS__ALL = 0xf, // All sub-events selected
		RETIRED_SERIALIZING_OPS = 0x5, // Retired Serializing Ops
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM10H_RETIRED_SERIALIZING_OPS__SSE_BOTTOM_EXECUTING_UOPS = 0x1, // SSE bottom-executing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM10H_RETIRED_SERIALIZING_OPS__SSE_BOTTOM_SERIALIZING_UOPS = 0x2, // SSE bottom-serializing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM10H_RETIRED_SERIALIZING_OPS__X87_BOTTOM_EXECUTING_UOPS = 0x4, // X87 bottom-executing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM10H_RETIRED_SERIALIZING_OPS__X87_BOTTOM_SERIALIZING_UOPS = 0x8, // X87 bottom-serializing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM10H_RETIRED_SERIALIZING_OPS__ALL = 0xf, // All sub-events selected
		FP_SCHEDULER_CYCLES = 0x6, // Number of Cycles that a Serializing uop is in the FP Scheduler
		FP_SCHEDULER_CYCLES__MASK__AMD64_FAM10H_FP_SCHEDULER_CYCLES__BOTTOM_EXECUTE_CYCLES = 0x1, // Number of cycles a bottom-execute uop is in the FP scheduler
		FP_SCHEDULER_CYCLES__MASK__AMD64_FAM10H_FP_SCHEDULER_CYCLES__BOTTOM_SERIALIZING_CYCLES = 0x2, // Number of cycles a bottom-serializing uop is in the FP scheduler
		FP_SCHEDULER_CYCLES__MASK__AMD64_FAM10H_FP_SCHEDULER_CYCLES__ALL = 0x3, // All sub-events selected
		SEGMENT_REGISTER_LOADS = 0x20, // Segment Register Loads
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__ES = 0x1, // ES
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__CS = 0x2, // CS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__SS = 0x4, // SS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__DS = 0x8, // DS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__FS = 0x10, // FS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__GS = 0x20, // GS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__HS = 0x40, // HS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM10H_SEGMENT_REGISTER_LOADS__ALL = 0x7f, // All sub-events selected
		PIPELINE_RESTART_DUE_TO_SELF_MODIFYING_CODE = 0x21, // Pipeline Restart Due to Self-Modifying Code
		PIPELINE_RESTART_DUE_TO_PROBE_HIT = 0x22, // Pipeline Restart Due to Probe Hit
		LS_BUFFER_2_FULL_CYCLES = 0x23, // LS Buffer 2 Full
		LOCKED_OPS = 0x24, // Locked Operations
		LOCKED_OPS__MASK__AMD64_FAM10H_LOCKED_OPS__EXECUTED = 0x1, // The number of locked instructions executed
		LOCKED_OPS__MASK__AMD64_FAM10H_LOCKED_OPS__CYCLES_SPECULATIVE_PHASE = 0x2, // The number of cycles spent in speculative phase
		LOCKED_OPS__MASK__AMD64_FAM10H_LOCKED_OPS__CYCLES_NON_SPECULATIVE_PHASE = 0x4, // The number of cycles spent in non-speculative phase (including cache miss penalty)
		LOCKED_OPS__MASK__AMD64_FAM10H_LOCKED_OPS__CYCLES_WAITING = 0x8, // The number of cycles waiting for a cache hit (cache miss penalty).
		LOCKED_OPS__MASK__AMD64_FAM10H_LOCKED_OPS__ALL = 0xf, // All sub-events selected
		RETIRED_CLFLUSH_INSTRUCTIONS = 0x26, // Retired CLFLUSH Instructions
		RETIRED_CPUID_INSTRUCTIONS = 0x27, // Retired CPUID Instructions
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS = 0x2a, // Cancelled Store to Load Forward Operations
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM10H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__ADDRESS_MISMATCHES = 0x1, // Address mismatches (starting byte not the same).
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM10H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__STORE_IS_SMALLER_THAN_LOAD = 0x2, // Store is smaller than load.
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM10H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MISALIGNED = 0x4, // Misaligned.
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM10H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__ALL = 0x7, // All sub-events selected
		SMIS_RECEIVED = 0x2b, // SMIs Received
		DATA_CACHE_ACCESSES = 0x40, // Data Cache Accesses
		DATA_CACHE_MISSES = 0x41, // Data Cache Misses
		DATA_CACHE_REFILLS = 0x42, // Data Cache Refills from L2 or Northbridge
		DATA_CACHE_REFILLS__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS__SYSTEM = 0x1, // Refill from the Northbridge
		DATA_CACHE_REFILLS__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS__L2_SHARED = 0x2, // Shared-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS__L2_EXCLUSIVE = 0x4, // Exclusive-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS__L2_OWNED = 0x8, // Owned-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS__L2_MODIFIED = 0x10, // Modified-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS__ALL = 0x1f, // All sub-events selected
		DATA_CACHE_REFILLS_FROM_SYSTEM = 0x43, // Data Cache Refills from the Northbridge
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS_FROM_SYSTEM__INVALID = 0x1, // Invalid
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS_FROM_SYSTEM__SHARED = 0x2, // Shared
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS_FROM_SYSTEM__EXCLUSIVE = 0x4, // Exclusive
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS_FROM_SYSTEM__OWNED = 0x8, // Owned
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS_FROM_SYSTEM__MODIFIED = 0x10, // Modified
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM10H_DATA_CACHE_REFILLS_FROM_SYSTEM__ALL = 0x1f, // All sub-events selected
		DATA_CACHE_LINES_EVICTED = 0x44, // Data Cache Lines Evicted
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__INVALID = 0x1, // Invalid
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__SHARED = 0x2, // Shared
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__EXCLUSIVE = 0x4, // Exclusive
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__OWNED = 0x8, // Owned
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__MODIFIED = 0x10, // Modified
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__BY_PREFETCHNTA = 0x20, // Cache line evicted was brought into the cache with by a PrefetchNTA instruction.
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__NOT_BY_PREFETCHNTA = 0x40, // Cache line evicted was not brought into the cache with by a PrefetchNTA instruction.
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM10H_DATA_CACHE_LINES_EVICTED__ALL = 0x7f, // All sub-events selected
		L1_DTLB_MISS_AND_L2_DTLB_HIT = 0x45, // L1 DTLB Miss and L2 DTLB Hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_MISS_AND_L2_DTLB_HIT__L2_4K_TLB_HIT = 0x1, // L2 4K TLB hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_MISS_AND_L2_DTLB_HIT__L2_2M_TLB_HIT = 0x2, // L2 2M TLB hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_MISS_AND_L2_DTLB_HIT__ALL = 0x3, // All sub-events selected
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_MISS_AND_L2_DTLB_HIT__L2_1G_TLB_HIT = 0x4, // L2 1G TLB hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_MISS_AND_L2_DTLB_HIT__ALL__REPEAT__1 = 0x7, // All sub-events selected
		L1_DTLB_AND_L2_DTLB_MISS = 0x46, // L1 DTLB and L2 DTLB Miss
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM10H_L1_DTLB_AND_L2_DTLB_MISS__4K_TLB_RELOAD = 0x1, // 4K TLB reload
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM10H_L1_DTLB_AND_L2_DTLB_MISS__2M_TLB_RELOAD = 0x2, // 2M TLB reload
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM10H_L1_DTLB_AND_L2_DTLB_MISS__1G_TLB_RELOAD = 0x4, // 1G TLB reload
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM10H_L1_DTLB_AND_L2_DTLB_MISS__ALL = 0x7, // All sub-events selected
		MISALIGNED_ACCESSES = 0x47, // Misaligned Accesses
		MICROARCHITECTURAL_LATE_CANCEL_OF_AN_ACCESS = 0x48, // Microarchitectural Late Cancel of an Access
		MICROARCHITECTURAL_EARLY_CANCEL_OF_AN_ACCESS = 0x49, // Microarchitectural Early Cancel of an Access
		SCRUBBER_SINGLE_BIT_ECC_ERRORS = 0x4a, // Single-bit ECC Errors Recorded by Scrubber
		SCRUBBER_SINGLE_BIT_ECC_ERRORS__MASK__AMD64_FAM10H_SCRUBBER_SINGLE_BIT_ECC_ERRORS__SCRUBBER_ERROR = 0x1, // Scrubber error
		SCRUBBER_SINGLE_BIT_ECC_ERRORS__MASK__AMD64_FAM10H_SCRUBBER_SINGLE_BIT_ECC_ERRORS__PIGGYBACK_ERROR = 0x2, // Piggyback scrubber errors
		SCRUBBER_SINGLE_BIT_ECC_ERRORS__MASK__AMD64_FAM10H_SCRUBBER_SINGLE_BIT_ECC_ERRORS__LOAD_PIPE_ERROR = 0x4, // Load pipe error
		SCRUBBER_SINGLE_BIT_ECC_ERRORS__MASK__AMD64_FAM10H_SCRUBBER_SINGLE_BIT_ECC_ERRORS__STORE_WRITE_PIPE_ERROR = 0x8, // Store write pipe error
		SCRUBBER_SINGLE_BIT_ECC_ERRORS__MASK__AMD64_FAM10H_SCRUBBER_SINGLE_BIT_ECC_ERRORS__ALL = 0xf, // All sub-events selected
		PREFETCH_INSTRUCTIONS_DISPATCHED = 0x4b, // Prefetch Instructions Dispatched
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM10H_PREFETCH_INSTRUCTIONS_DISPATCHED__LOAD = 0x1, // Load (Prefetch
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM10H_PREFETCH_INSTRUCTIONS_DISPATCHED__STORE = 0x2, // Store (PrefetchW)
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM10H_PREFETCH_INSTRUCTIONS_DISPATCHED__NTA = 0x4, // NTA (PrefetchNTA)
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM10H_PREFETCH_INSTRUCTIONS_DISPATCHED__ALL = 0x7, // All sub-events selected
		DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS = 0x4c, // DCACHE Misses by Locked Instructions
		DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__MASK__AMD64_FAM10H_DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__DATA_CACHE_MISSES_BY_LOCKED_INSTRUCTIONS = 0x2, // Data cache misses by locked instructions
		DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__MASK__AMD64_FAM10H_DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__ALL = 0x2, // All sub-events selected
		L1_DTLB_HIT = 0x4d, // L1 DTLB Hit
		L1_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_HIT__L1_4K_TLB_HIT = 0x1, // L1 4K TLB hit
		L1_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_HIT__L1_2M_TLB_HIT = 0x2, // L1 2M TLB hit
		L1_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_HIT__L1_1G_TLB_HIT = 0x4, // L1 1G TLB hit
		L1_DTLB_HIT__MASK__AMD64_FAM10H_L1_DTLB_HIT__ALL = 0x7, // All sub-events selected
		INEFFECTIVE_SW_PREFETCHES = 0x52, // Ineffective Software Prefetches
		INEFFECTIVE_SW_PREFETCHES__MASK__AMD64_FAM10H_INEFFECTIVE_SW_PREFETCHES__SW_PREFETCH_HIT_IN_L1 = 0x1, // Software prefetch hit in the L1.
		INEFFECTIVE_SW_PREFETCHES__MASK__AMD64_FAM10H_INEFFECTIVE_SW_PREFETCHES__SW_PREFETCH_HIT_IN_L2 = 0x8, // Software prefetch hit in L2.
		INEFFECTIVE_SW_PREFETCHES__MASK__AMD64_FAM10H_INEFFECTIVE_SW_PREFETCHES__ALL = 0x9, // All sub-events selected
		GLOBAL_TLB_FLUSHES = 0x54, // Global TLB Flushes
		MEMORY_REQUESTS = 0x65, // Memory Requests by Type
		MEMORY_REQUESTS__MASK__AMD64_FAM10H_MEMORY_REQUESTS__NON_CACHEABLE = 0x1, // Requests to non-cacheable (UC) memory
		MEMORY_REQUESTS__MASK__AMD64_FAM10H_MEMORY_REQUESTS__WRITE_COMBINING = 0x2, // Requests to write-combining (WC) memory or WC buffer flushes to WB memory
		MEMORY_REQUESTS__MASK__AMD64_FAM10H_MEMORY_REQUESTS__STREAMING_STORE = 0x80, // Streaming store (SS) requests
		MEMORY_REQUESTS__MASK__AMD64_FAM10H_MEMORY_REQUESTS__ALL = 0x83, // All sub-events selected
		DATA_PREFETCHES = 0x67, // Data Prefetcher
		DATA_PREFETCHES__MASK__AMD64_FAM10H_DATA_PREFETCHES__CANCELLED = 0x1, // Cancelled prefetches
		DATA_PREFETCHES__MASK__AMD64_FAM10H_DATA_PREFETCHES__ATTEMPTED = 0x2, // Prefetch attempts
		DATA_PREFETCHES__MASK__AMD64_FAM10H_DATA_PREFETCHES__ALL = 0x3, // All sub-events selected
		MAB_REQUESTS = 0x68, // Average L1 refill latency for Icache and Dcache misses (request count for cache refills)
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_0 = 0x0, // Buffer 0
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_1 = 0x1, // Buffer 1
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_2 = 0x2, // Buffer 2
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_3 = 0x3, // Buffer 3
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_4 = 0x4, // Buffer 4
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_5 = 0x5, // Buffer 5
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_6 = 0x6, // Buffer 6
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_7 = 0x7, // Buffer 7
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_8 = 0x8, // Buffer 8
		MAB_REQUESTS__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_9 = 0x9, // Buffer 9
		MAB_WAIT_CYCLES = 0x69, // Average L1 refill latency for Icache and Dcache misses (cycles that requests spent waiting for the refills)
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_0 = 0x0, // Buffer 0
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_1 = 0x1, // Buffer 1
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_2 = 0x2, // Buffer 2
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_3 = 0x3, // Buffer 3
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_4 = 0x4, // Buffer 4
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_5 = 0x5, // Buffer 5
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_6 = 0x6, // Buffer 6
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_7 = 0x7, // Buffer 7
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_8 = 0x8, // Buffer 8
		MAB_WAIT_CYCLES__MASK__AMD64_FAM10H_MAB_REQUESTS__BUFFER_9 = 0x9, // Buffer 9
		SYSTEM_READ_RESPONSES = 0x6c, // Northbridge Read Responses by Coherency State
		SYSTEM_READ_RESPONSES__MASK__AMD64_FAM10H_SYSTEM_READ_RESPONSES__EXCLUSIVE = 0x1, // Exclusive
		SYSTEM_READ_RESPONSES__MASK__AMD64_FAM10H_SYSTEM_READ_RESPONSES__MODIFIED = 0x2, // Modified
		SYSTEM_READ_RESPONSES__MASK__AMD64_FAM10H_SYSTEM_READ_RESPONSES__SHARED = 0x4, // Shared
		SYSTEM_READ_RESPONSES__MASK__AMD64_FAM10H_SYSTEM_READ_RESPONSES__OWNED = 0x8, // Owned
		SYSTEM_READ_RESPONSES__MASK__AMD64_FAM10H_SYSTEM_READ_RESPONSES__DATA_ERROR = 0x10, // Data Error
		SYSTEM_READ_RESPONSES__MASK__AMD64_FAM10H_SYSTEM_READ_RESPONSES__ALL = 0x1f, // All sub-events selected
		QUADWORDS_WRITTEN_TO_SYSTEM = 0x6d, // Octwords Written to System
		QUADWORDS_WRITTEN_TO_SYSTEM__MASK__AMD64_FAM10H_QUADWORDS_WRITTEN_TO_SYSTEM__QUADWORD_WRITE_TRANSFER = 0x1, // Octword write transfer
		QUADWORDS_WRITTEN_TO_SYSTEM__MASK__AMD64_FAM10H_QUADWORDS_WRITTEN_TO_SYSTEM__ALL = 0x1, // All sub-events selected
		CPU_CLK_UNHALTED = 0x76, // CPU Clocks not Halted
		REQUESTS_TO_L2 = 0x7d, // Requests to L2 Cache
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__INSTRUCTIONS = 0x1, // IC fill
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__DATA = 0x2, // DC fill
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__TLB_WALK = 0x4, // TLB fill (page table walks)
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__SNOOP = 0x8, // Tag snoop request
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__CANCELLED = 0x10, // Cancelled request
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__HW_PREFETCH_FROM_DC = 0x20, // Hardware prefetch from DC
		REQUESTS_TO_L2__MASK__AMD64_FAM10H_REQUESTS_TO_L2__ALL = 0x3f, // All sub-events selected
		L2_CACHE_MISS = 0x7e, // L2 Cache Misses
		L2_CACHE_MISS__MASK__AMD64_FAM10H_L2_CACHE_MISS__INSTRUCTIONS = 0x1, // IC fill
		L2_CACHE_MISS__MASK__AMD64_FAM10H_L2_CACHE_MISS__DATA = 0x2, // DC fill (includes possible replays
		L2_CACHE_MISS__MASK__AMD64_FAM10H_L2_CACHE_MISS__TLB_WALK = 0x4, // TLB page table walk
		L2_CACHE_MISS__MASK__AMD64_FAM10H_L2_CACHE_MISS__HW_PREFETCH_FROM_DC = 0x8, // Hardware prefetch from DC
		L2_CACHE_MISS__MASK__AMD64_FAM10H_L2_CACHE_MISS__ALL = 0xf, // All sub-events selected
		L2_FILL_WRITEBACK = 0x7f, // L2 Fill/Writeback
		L2_FILL_WRITEBACK__MASK__AMD64_FAM10H_L2_FILL_WRITEBACK__L2_FILLS = 0x1, // L2 fills (victims from L1 caches
		L2_FILL_WRITEBACK__MASK__AMD64_FAM10H_L2_FILL_WRITEBACK__L2_WRITEBACKS = 0x2, // L2 Writebacks to system.
		L2_FILL_WRITEBACK__MASK__AMD64_FAM10H_L2_FILL_WRITEBACK__ALL = 0x3, // All sub-events selected
		INSTRUCTION_CACHE_FETCHES = 0x80, // Instruction Cache Fetches
		INSTRUCTION_CACHE_MISSES = 0x81, // Instruction Cache Misses
		INSTRUCTION_CACHE_REFILLS_FROM_L2 = 0x82, // Instruction Cache Refills from L2
		INSTRUCTION_CACHE_REFILLS_FROM_SYSTEM = 0x83, // Instruction Cache Refills from System
		L1_ITLB_MISS_AND_L2_ITLB_HIT = 0x84, // L1 ITLB Miss and L2 ITLB Hit
		L1_ITLB_MISS_AND_L2_ITLB_MISS = 0x85, // L1 ITLB Miss and L2 ITLB Miss
		L1_ITLB_MISS_AND_L2_ITLB_MISS__MASK__AMD64_FAM10H_L1_ITLB_MISS_AND_L2_ITLB_MISS__4K_PAGE_FETCHES = 0x1, // Instruction fetches to a 4K page.
		L1_ITLB_MISS_AND_L2_ITLB_MISS__MASK__AMD64_FAM10H_L1_ITLB_MISS_AND_L2_ITLB_MISS__2M_PAGE_FETCHES = 0x2, // Instruction fetches to a 2M page.
		L1_ITLB_MISS_AND_L2_ITLB_MISS__MASK__AMD64_FAM10H_L1_ITLB_MISS_AND_L2_ITLB_MISS__ALL = 0x3, // All sub-events selected
		PIPELINE_RESTART_DUE_TO_INSTRUCTION_STREAM_PROBE = 0x86, // Pipeline Restart Due to Instruction Stream Probe
		INSTRUCTION_FETCH_STALL = 0x87, // Instruction Fetch Stall
		RETURN_STACK_HITS = 0x88, // Return Stack Hits
		RETURN_STACK_OVERFLOWS = 0x89, // Return Stack Overflows
		INSTRUCTION_CACHE_VICTIMS = 0x8b, // Instruction Cache Victims
		INSTRUCTION_CACHE_LINES_INVALIDATED = 0x8c, // Instruction Cache Lines Invalidated
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM10H_INSTRUCTION_CACHE_LINES_INVALIDATED__INVALIDATING_PROBE_NO_IN_FLIGHT = 0x1, // Invalidating probe that did not hit any in-flight instructions.
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM10H_INSTRUCTION_CACHE_LINES_INVALIDATED__INVALIDATING_PROBE_ONE_OR_MORE_IN_FLIGHT = 0x2, // Invalidating probe that hit one or more in-flight instructions.
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM10H_INSTRUCTION_CACHE_LINES_INVALIDATED__ALL = 0x3, // All sub-events selected
		ITLB_RELOADS = 0x99, // ITLB Reloads
		ITLB_RELOADS_ABORTED = 0x9a, // ITLB Reloads Aborted
		RETIRED_INSTRUCTIONS = 0xc0, // Retired Instructions
		RETIRED_UOPS = 0xc1, // Retired uops
		RETIRED_BRANCH_INSTRUCTIONS = 0xc2, // Retired Branch Instructions
		RETIRED_MISPREDICTED_BRANCH_INSTRUCTIONS = 0xc3, // Retired Mispredicted Branch Instructions
		RETIRED_TAKEN_BRANCH_INSTRUCTIONS = 0xc4, // Retired Taken Branch Instructions
		RETIRED_TAKEN_BRANCH_INSTRUCTIONS_MISPREDICTED = 0xc5, // Retired Taken Branch Instructions Mispredicted
		RETIRED_FAR_CONTROL_TRANSFERS = 0xc6, // Retired Far Control Transfers
		RETIRED_BRANCH_RESYNCS = 0xc7, // Retired Branch Resyncs
		RETIRED_NEAR_RETURNS = 0xc8, // Retired Near Returns
		RETIRED_NEAR_RETURNS_MISPREDICTED = 0xc9, // Retired Near Returns Mispredicted
		RETIRED_INDIRECT_BRANCHES_MISPREDICTED = 0xca, // Retired Indirect Branches Mispredicted
		RETIRED_MMX_AND_FP_INSTRUCTIONS = 0xcb, // Retired MMX/FP Instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_MMX_AND_FP_INSTRUCTIONS__X87 = 0x1, // X87 instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_MMX_AND_FP_INSTRUCTIONS__MMX_AND_3DNOW = 0x2, // MMX and 3DNow! instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_MMX_AND_FP_INSTRUCTIONS__PACKED_SSE_AND_SSE2 = 0x4, // SSE instructions (SSE
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_MMX_AND_FP_INSTRUCTIONS__ALL = 0x7, // All sub-events selected
		RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS = 0xcc, // Retired Fastpath Double Op Instructions
		RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__POSITION_0 = 0x1, // With low op in position 0
		RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__POSITION_1 = 0x2, // With low op in position 1
		RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__POSITION_2 = 0x4, // With low op in position 2
		RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__MASK__AMD64_FAM10H_RETIRED_FASTPATH_DOUBLE_OP_INSTRUCTIONS__ALL = 0x7, // All sub-events selected
		INTERRUPTS_MASKED_CYCLES = 0xcd, // Interrupts-Masked Cycles
		INTERRUPTS_MASKED_CYCLES_WITH_INTERRUPT_PENDING = 0xce, // Interrupts-Masked Cycles with Interrupt Pending
		INTERRUPTS_TAKEN = 0xcf, // Interrupts Taken
		DECODER_EMPTY = 0xd0, // Decoder Empty
		DISPATCH_STALLS = 0xd1, // Dispatch Stalls
		DISPATCH_STALL_FOR_BRANCH_ABORT = 0xd2, // Dispatch Stall for Branch Abort to Retire
		DISPATCH_STALL_FOR_SERIALIZATION = 0xd3, // Dispatch Stall for Serialization
		DISPATCH_STALL_FOR_SEGMENT_LOAD = 0xd4, // Dispatch Stall for Segment Load
		DISPATCH_STALL_FOR_REORDER_BUFFER_FULL = 0xd5, // Dispatch Stall for Reorder Buffer Full
		DISPATCH_STALL_FOR_RESERVATION_STATION_FULL = 0xd6, // Dispatch Stall for Reservation Station Full
		DISPATCH_STALL_FOR_FPU_FULL = 0xd7, // Dispatch Stall for FPU Full
		DISPATCH_STALL_FOR_LS_FULL = 0xd8, // Dispatch Stall for LS Full
		DISPATCH_STALL_WAITING_FOR_ALL_QUIET = 0xd9, // Dispatch Stall Waiting for All Quiet
		DISPATCH_STALL_FOR_FAR_TRANSFER_OR_RSYNC = 0xda, // Dispatch Stall for Far Transfer or Resync to Retire
		FPU_EXCEPTIONS = 0xdb, // FPU Exceptions
		FPU_EXCEPTIONS__MASK__AMD64_FAM10H_FPU_EXCEPTIONS__X87_RECLASS_MICROFAULTS = 0x1, // X87 reclass microfaults
		FPU_EXCEPTIONS__MASK__AMD64_FAM10H_FPU_EXCEPTIONS__SSE_RETYPE_MICROFAULTS = 0x2, // SSE retype microfaults
		FPU_EXCEPTIONS__MASK__AMD64_FAM10H_FPU_EXCEPTIONS__SSE_RECLASS_MICROFAULTS = 0x4, // SSE reclass microfaults
		FPU_EXCEPTIONS__MASK__AMD64_FAM10H_FPU_EXCEPTIONS__SSE_AND_X87_MICROTRAPS = 0x8, // SSE and x87 microtraps
		FPU_EXCEPTIONS__MASK__AMD64_FAM10H_FPU_EXCEPTIONS__ALL = 0xf, // All sub-events selected
		DR0_BREAKPOINT_MATCHES = 0xdc, // DR0 Breakpoint Matches
		DR1_BREAKPOINT_MATCHES = 0xdd, // DR1 Breakpoint Matches
		DR2_BREAKPOINT_MATCHES = 0xde, // DR2 Breakpoint Matches
		DR3_BREAKPOINT_MATCHES = 0xdf, // DR3 Breakpoint Matches
		DRAM_ACCESSES_PAGE = 0xe0, // DRAM Accesses
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__HIT = 0x1, // DCT0 Page hit
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__MISS = 0x2, // DCT0 Page Miss
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__CONFLICT = 0x4, // DCT0 Page Conflict
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__DCT1_PAGE_HIT = 0x8, // DCT1 Page hit
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__DCT1_PAGE_MISS = 0x10, // DCT1 Page Miss
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__DCT1_PAGE_CONFLICT = 0x20, // DCT1 Page Conflict
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM10H_DRAM_ACCESSES_PAGE__ALL = 0x3f, // All sub-events selected
		MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS = 0xe1, // DRAM Controller Page Table Overflows
		MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS__DCT0_PAGE_TABLE_OVERFLOW = 0x1, // DCT0 Page Table Overflow
		MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS__DCT1_PAGE_TABLE_OVERFLOW = 0x2, // DCT1 Page Table Overflow
		MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_PAGE_TABLE_OVERFLOWS__ALL = 0x3, // All sub-events selected
		MEMORY_CONTROLLER_SLOT_MISSES = 0xe2, // Memory Controller DRAM Command Slots Missed
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_SLOT_MISSES__DCT0_COMMAND_SLOTS_MISSED = 0x1, // DCT0 Command Slots Missed
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_SLOT_MISSES__DCT1_COMMAND_SLOTS_MISSED = 0x2, // DCT1 Command Slots Missed
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_SLOT_MISSES__ALL = 0x3, // All sub-events selected
		MEMORY_CONTROLLER_TURNAROUNDS = 0xe3, // Memory Controller Turnarounds
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__CHIP_SELECT = 0x1, // DCT0 DIMM (chip select) turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__READ_TO_WRITE = 0x2, // DCT0 Read to write turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__WRITE_TO_READ = 0x4, // DCT0 Write to read turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__DCT1_DIMM = 0x8, // DCT1 DIMM (chip select) turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__DCT1_READ_TO_WRITE_TURNAROUND = 0x10, // DCT1 Read to write turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__DCT1_WRITE_TO_READ_TURNAROUND = 0x20, // DCT1 Write to read turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_TURNAROUNDS__ALL = 0x3f, // All sub-events selected
		MEMORY_CONTROLLER_BYPASS = 0xe4, // Memory Controller Bypass Counter Saturation
		MEMORY_CONTROLLER_BYPASS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_BYPASS__HIGH_PRIORITY = 0x1, // Memory controller high priority bypass
		MEMORY_CONTROLLER_BYPASS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_BYPASS__LOW_PRIORITY = 0x2, // Memory controller medium priority bypass
		MEMORY_CONTROLLER_BYPASS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_BYPASS__DRAM_INTERFACE = 0x4, // DCT0 DCQ bypass
		MEMORY_CONTROLLER_BYPASS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_BYPASS__DRAM_QUEUE = 0x8, // DCT1 DCQ bypass
		MEMORY_CONTROLLER_BYPASS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_BYPASS__ALL = 0xf, // All sub-events selected
		THERMAL_STATUS_AND_ECC_ERRORS = 0xe8, // Thermal Status
		THERMAL_STATUS_AND_ECC_ERRORS__MASK__AMD64_FAM10H_THERMAL_STATUS_AND_ECC_ERRORS__CLKS_DIE_TEMP_TOO_HIGH = 0x4, // Number of times the HTC trip point is crossed
		THERMAL_STATUS_AND_ECC_ERRORS__MASK__AMD64_FAM10H_THERMAL_STATUS_AND_ECC_ERRORS__CLKS_TEMP_THRESHOLD_EXCEEDED = 0x8, // Number of clocks when STC trip point active
		THERMAL_STATUS_AND_ECC_ERRORS__MASK__AMD64_FAM10H_THERMAL_STATUS_AND_ECC_ERRORS__STC_TRIP_POINTS_CROSSED = 0x10, // Number of times the STC trip point is crossed
		THERMAL_STATUS_AND_ECC_ERRORS__MASK__AMD64_FAM10H_THERMAL_STATUS_AND_ECC_ERRORS__CLOCKS_HTC_P_STATE_INACTIVE = 0x20, // Number of clocks HTC P-state is inactive.
		THERMAL_STATUS_AND_ECC_ERRORS__MASK__AMD64_FAM10H_THERMAL_STATUS_AND_ECC_ERRORS__CLOCKS_HTC_P_STATE_ACTIVE = 0x40, // Number of clocks HTC P-state is active
		THERMAL_STATUS_AND_ECC_ERRORS__MASK__AMD64_FAM10H_THERMAL_STATUS_AND_ECC_ERRORS__ALL = 0x7c, // All sub-events selected
		CPU_IO_REQUESTS_TO_MEMORY_IO = 0xe9, // CPU/IO Requests to Memory/IO
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__I_O_TO_I_O = 0x1, // IO to IO
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__I_O_TO_MEM = 0x2, // IO to Mem
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__CPU_TO_I_O = 0x4, // CPU to IO
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__CPU_TO_MEM = 0x8, // CPU to Mem
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__TO_REMOTE_NODE = 0x10, // To remote node
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__TO_LOCAL_NODE = 0x20, // To local node
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__FROM_REMOTE_NODE = 0x40, // From remote node
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__FROM_LOCAL_NODE = 0x80, // From local node
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM10H_CPU_IO_REQUESTS_TO_MEMORY_IO__ALL = 0xff, // All sub-events selected
		CACHE_BLOCK = 0xea, // Cache Block Commands
		CACHE_BLOCK__MASK__AMD64_FAM10H_CACHE_BLOCK__VICTIM_WRITEBACK = 0x1, // Victim Block (Writeback)
		CACHE_BLOCK__MASK__AMD64_FAM10H_CACHE_BLOCK__DCACHE_LOAD_MISS = 0x4, // Read Block (Dcache load miss refill)
		CACHE_BLOCK__MASK__AMD64_FAM10H_CACHE_BLOCK__SHARED_ICACHE_REFILL = 0x8, // Read Block Shared (Icache refill)
		CACHE_BLOCK__MASK__AMD64_FAM10H_CACHE_BLOCK__READ_BLOCK_MODIFIED = 0x10, // Read Block Modified (Dcache store miss refill)
		CACHE_BLOCK__MASK__AMD64_FAM10H_CACHE_BLOCK__READ_TO_DIRTY = 0x20, // Change-to-Dirty (first store to clean block already in cache)
		CACHE_BLOCK__MASK__AMD64_FAM10H_CACHE_BLOCK__ALL = 0x3d, // All sub-events selected
		SIZED_COMMANDS = 0xeb, // Sized Commands
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__NON_POSTED_WRITE_BYTE = 0x1, // Non-Posted SzWr Byte (1-32 bytes) Legacy or mapped IO
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__NON_POSTED_WRITE_DWORD = 0x2, // Non-Posted SzWr DW (1-16 dwords) Legacy or mapped IO
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__POSTED_WRITE_BYTE = 0x4, // Posted SzWr Byte (1-32 bytes) Subcache-line DMA writes
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__POSTED_WRITE_DWORD = 0x8, // Posted SzWr DW (1-16 dwords) Block-oriented DMA writes
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__READ_BYTE_4_BYTES = 0x10, // SzRd Byte (4 bytes) Legacy or mapped IO
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__READ_DWORD_1_16_DWORDS = 0x20, // SzRd DW (1-16 dwords) Block-oriented DMA reads
		SIZED_COMMANDS__MASK__AMD64_FAM10H_SIZED_COMMANDS__ALL = 0x3f, // All sub-events selected
		PROBE = 0xec, // Probe Responses and Upstream Requests
		PROBE__MASK__AMD64_FAM10H_PROBE__MISS = 0x1, // Probe miss
		PROBE__MASK__AMD64_FAM10H_PROBE__HIT_CLEAN = 0x2, // Probe hit clean
		PROBE__MASK__AMD64_FAM10H_PROBE__HIT_DIRTY_NO_MEMORY_CANCEL = 0x4, // Probe hit dirty without memory cancel (probed by Sized Write or Change2Dirty)
		PROBE__MASK__AMD64_FAM10H_PROBE__HIT_DIRTY_WITH_MEMORY_CANCEL = 0x8, // Probe hit dirty with memory cancel (probed by DMA read or cache refill request)
		PROBE__MASK__AMD64_FAM10H_PROBE__UPSTREAM_DISPLAY_REFRESH_READS = 0x10, // Upstream display refresh/ISOC reads
		PROBE__MASK__AMD64_FAM10H_PROBE__UPSTREAM_NON_DISPLAY_REFRESH_READS = 0x20, // Upstream non-display refresh reads
		PROBE__MASK__AMD64_FAM10H_PROBE__UPSTREAM_WRITES = 0x40, // Upstream ISOC writes
		PROBE__MASK__AMD64_FAM10H_PROBE__UPSTREAM_NON_ISOC_WRITES = 0x80, // Upstream non-ISOC writes
		PROBE__MASK__AMD64_FAM10H_PROBE__ALL = 0xff, // All sub-events selected
		GART = 0xee, // GART Events
		GART__MASK__AMD64_FAM10H_GART__APERTURE_HIT_FROM_CPU = 0x1, // GART aperture hit on access from CPU
		GART__MASK__AMD64_FAM10H_GART__APERTURE_HIT_FROM_IO = 0x2, // GART aperture hit on access from IO
		GART__MASK__AMD64_FAM10H_GART__MISS = 0x4, // GART miss
		GART__MASK__AMD64_FAM10H_GART__REQUEST_HIT_TABLE_WALK = 0x8, // GART/DEV Request hit table walk in progress
		GART__MASK__AMD64_FAM10H_GART__DEV_HIT = 0x10, // DEV hit
		GART__MASK__AMD64_FAM10H_GART__DEV_MISS = 0x20, // DEV miss
		GART__MASK__AMD64_FAM10H_GART__DEV_ERROR = 0x40, // DEV error
		GART__MASK__AMD64_FAM10H_GART__MULTIPLE_TABLE_WALK = 0x80, // GART/DEV multiple table walk in progress
		GART__MASK__AMD64_FAM10H_GART__ALL = 0xff, // All sub-events selected
		MEMORY_CONTROLLER_REQUESTS = 0x1f0, // Memory Controller Requests
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__WRITE_REQUESTS = 0x1, // Write requests sent to the DCT
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__READ_REQUESTS = 0x2, // Read requests (including prefetch requests) sent to the DCT
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__PREFETCH_REQUESTS = 0x4, // Prefetch requests sent to the DCT
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__32_BYTES_WRITES = 0x8, // 32 Bytes Sized Writes
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__64_BYTES_WRITES = 0x10, // 64 Bytes Sized Writes
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__32_BYTES_READS = 0x20, // 32 Bytes Sized Reads
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__64_BYTES_READS = 0x40, // 64 Byte Sized Reads
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__READ_REQUESTS_WHILE_WRITES_REQUESTS = 0x80, // Read requests sent to the DCT while writes requests are pending in the DCT
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM10H_MEMORY_CONTROLLER_REQUESTS__ALL = 0xff, // All sub-events selected
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE = 0x1e0, // CPU to DRAM Requests to Target Node
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_0 = 0x1, // From Local node to Node 0
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_1 = 0x2, // From Local node to Node 1
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_2 = 0x4, // From Local node to Node 2
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_3 = 0x8, // From Local node to Node 3
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_4 = 0x10, // From Local node to Node 4
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_5 = 0x20, // From Local node to Node 5
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_6 = 0x40, // From Local node to Node 6
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_7 = 0x80, // From Local node to Node 7
		CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__ALL = 0xff, // All sub-events selected
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE = 0x1e1, // IO to DRAM Requests to Target Node
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_0 = 0x1, // From Local node to Node 0
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_1 = 0x2, // From Local node to Node 1
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_2 = 0x4, // From Local node to Node 2
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_3 = 0x8, // From Local node to Node 3
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_4 = 0x10, // From Local node to Node 4
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_5 = 0x20, // From Local node to Node 5
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_6 = 0x40, // From Local node to Node 6
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__LOCAL_TO_7 = 0x80, // From Local node to Node 7
		IO_TO_DRAM_REQUESTS_TO_TARGET_NODE__MASK__AMD64_FAM10H_CPU_TO_DRAM_REQUESTS_TO_TARGET_NODE__ALL = 0xff, // All sub-events selected
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3 = 0x1e2, // CPU Read Command Latency to Target Node 0-3
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__READ_BLOCK = 0x1, // Read block
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__READ_BLOCK_SHARED = 0x2, // Read block shared
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__READ_BLOCK_MODIFIED = 0x4, // Read block modified
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__CHANGE_TO_DIRTY = 0x8, // Change-to-Dirty
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_0 = 0x10, // From Local node to Node 0
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_1 = 0x20, // From Local node to Node 1
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_2 = 0x40, // From Local node to Node 2
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_3 = 0x80, // From Local node to Node 3
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__ALL = 0xff, // All sub-events selected
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3 = 0x1e3, // CPU Read Command Requests to Target Node 0-3
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__READ_BLOCK = 0x1, // Read block
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__READ_BLOCK_SHARED = 0x2, // Read block shared
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__READ_BLOCK_MODIFIED = 0x4, // Read block modified
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__CHANGE_TO_DIRTY = 0x8, // Change-to-Dirty
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_0 = 0x10, // From Local node to Node 0
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_1 = 0x20, // From Local node to Node 1
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_2 = 0x40, // From Local node to Node 2
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__LOCAL_TO_3 = 0x80, // From Local node to Node 3
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_0_3__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_0_3__ALL = 0xff, // All sub-events selected
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7 = 0x1e4, // CPU Read Command Latency to Target Node 4-7
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__READ_BLOCK = 0x1, // Read block
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__READ_BLOCK_SHARED = 0x2, // Read block shared
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__READ_BLOCK_MODIFIED = 0x4, // Read block modified
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__CHANGE_TO_DIRTY = 0x8, // Change-to-Dirty
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_4 = 0x10, // From Local node to Node 4
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_5 = 0x20, // From Local node to Node 5
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_6 = 0x40, // From Local node to Node 6
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_7 = 0x80, // From Local node to Node 7
		CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__ALL = 0xff, // All sub-events selected
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7 = 0x1e5, // CPU Read Command Requests to Target Node 4-7
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__READ_BLOCK = 0x1, // Read block
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__READ_BLOCK_SHARED = 0x2, // Read block shared
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__READ_BLOCK_MODIFIED = 0x4, // Read block modified
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__CHANGE_TO_DIRTY = 0x8, // Change-to-Dirty
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_4 = 0x10, // From Local node to Node 4
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_5 = 0x20, // From Local node to Node 5
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_6 = 0x40, // From Local node to Node 6
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__LOCAL_TO_7 = 0x80, // From Local node to Node 7
		CPU_READ_COMMAND_REQUESTS_TO_TARGET_NODE_4_7__MASK__AMD64_FAM10H_CPU_READ_COMMAND_LATENCY_TO_TARGET_NODE_4_7__ALL = 0xff, // All sub-events selected
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7 = 0x1e6, // CPU Command Latency to Target Node 0-3/4-7
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__READ_SIZED = 0x1, // Read Sized
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__WRITE_SIZED = 0x2, // Write Sized
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__VICTIM_BLOCK = 0x4, // Victim Block
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__NODE_GROUP_SELECT = 0x8, // Nodes 4-7.
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_0_4 = 0x10, // From Local node to Node 0/4
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_1_5 = 0x20, // From Local node to Node 1/5
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_2_6 = 0x40, // From Local node to Node 2/6
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_3_7 = 0x80, // From Local node to Node 3/7
		CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__ALL = 0xff, // All sub-events selected
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7 = 0x1e7, // CPU Requests to Target Node 0-3/4-7
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__READ_SIZED = 0x1, // Read Sized
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__WRITE_SIZED = 0x2, // Write Sized
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__VICTIM_BLOCK = 0x4, // Victim Block
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__NODE_GROUP_SELECT = 0x8, // Nodes 4-7.
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_0_4 = 0x10, // From Local node to Node 0/4
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_1_5 = 0x20, // From Local node to Node 1/5
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_2_6 = 0x40, // From Local node to Node 2/6
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__LOCAL_TO_3_7 = 0x80, // From Local node to Node 3/7
		CPU_REQUESTS_TO_TARGET_NODE_0_3_4_7__MASK__AMD64_FAM10H_CPU_COMMAND_LATENCY_TO_TARGET_NODE_0_3_4_7__ALL = 0xff, // All sub-events selected
		HYPERTRANSPORT_LINK0 = 0xf6, // HyperTransport Link 0 Transmit Bandwidth
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__COMMAND_DWORD_SENT = 0x1, // Command DWORD sent
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__DATA_DWORD_SENT = 0x2, // Data DWORD sent
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__BUFFER_RELEASE_DWORD_SENT = 0x4, // Buffer release DWORD sent
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__NOP_DWORD_SENT = 0x8, // Nop DW sent (idle)
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__ADDRESS_EXT_DWORD_SENT = 0x10, // Address extension DWORD sent
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__PER_PACKET_CRC_SENT = 0x20, // Per packet CRC sent
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__ALL = 0x3f, // All sub-events selected
		HYPERTRANSPORT_LINK0__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__SUBLINK_MASK = 0x80, // SubLink Mask
		HYPERTRANSPORT_LINK1 = 0xf7, // HyperTransport Link 1 Transmit Bandwidth
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__COMMAND_DWORD_SENT = 0x1, // Command DWORD sent
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__DATA_DWORD_SENT = 0x2, // Data DWORD sent
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__BUFFER_RELEASE_DWORD_SENT = 0x4, // Buffer release DWORD sent
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__NOP_DWORD_SENT = 0x8, // Nop DW sent (idle)
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__ADDRESS_EXT_DWORD_SENT = 0x10, // Address extension DWORD sent
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__PER_PACKET_CRC_SENT = 0x20, // Per packet CRC sent
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__ALL = 0x3f, // All sub-events selected
		HYPERTRANSPORT_LINK1__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__SUBLINK_MASK = 0x80, // SubLink Mask
		HYPERTRANSPORT_LINK2 = 0xf8, // HyperTransport Link 2 Transmit Bandwidth
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__COMMAND_DWORD_SENT = 0x1, // Command DWORD sent
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__DATA_DWORD_SENT = 0x2, // Data DWORD sent
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__BUFFER_RELEASE_DWORD_SENT = 0x4, // Buffer release DWORD sent
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__NOP_DWORD_SENT = 0x8, // Nop DW sent (idle)
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__ADDRESS_EXT_DWORD_SENT = 0x10, // Address extension DWORD sent
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__PER_PACKET_CRC_SENT = 0x20, // Per packet CRC sent
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__ALL = 0x3f, // All sub-events selected
		HYPERTRANSPORT_LINK2__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK0__SUBLINK_MASK = 0x80, // SubLink Mask
		HYPERTRANSPORT_LINK3 = 0x1f9, // HyperTransport Link 3 Transmit Bandwidth
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__COMMAND_DWORD_SENT = 0x1, // Command DWORD sent
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__DATA_DWORD_SENT = 0x2, // Data DWORD sent
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__BUFFER_RELEASE_DWORD_SENT = 0x4, // Buffer release DWORD sent
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__NOP_DWORD_SENT = 0x8, // Nop DW sent (idle)
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__ADDRESS_EXT_DWORD_SENT = 0x10, // Address DWORD sent
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__PER_PACKET_CRC_SENT = 0x20, // Per packet CRC sent
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__ALL = 0x3f, // All sub-events selected
		HYPERTRANSPORT_LINK3__MASK__AMD64_FAM10H_HYPERTRANSPORT_LINK3__SUBLINK_MASK = 0x80, // SubLink Mask
		READ_REQUEST_TO_L3_CACHE = 0x4e0, // Read Request to L3 Cache
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_READ_REQUEST_TO_L3_CACHE__READ_BLOCK_EXCLUSIVE = 0x1, // Read Block Exclusive (Data cache read)
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_READ_REQUEST_TO_L3_CACHE__READ_BLOCK_SHARED = 0x2, // Read Block Shared (Instruction cache read)
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_READ_REQUEST_TO_L3_CACHE__READ_BLOCK_MODIFY = 0x4, // Read Block Modify
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_READ_REQUEST_TO_L3_CACHE__ANY_READ = 0x7, // Any read modes (exclusive
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_READ_REQUEST_TO_L3_CACHE__ALL_CORES = 0xf0, // All sub-events selected
		L3_CACHE_MISSES = 0x4e1, // L3 Cache Misses
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_EXCLUSIVE = 0x1, // Read Block Exclusive (Data cache read)
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_SHARED = 0x2, // Read Block Shared (Instruction cache read)
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_MODIFY = 0x4, // Read Block Modify
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ANY_READ = 0x7, // Any read modes (exclusive
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ALL_CORES = 0xf0, // All cores
		L3_FILLS_CAUSED_BY_L2_EVICTIONS = 0x4e2, // L3 Fills caused by L2 Evictions
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__SHARED = 0x1, // Shared
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__EXCLUSIVE = 0x2, // Exclusive
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__OWNED = 0x4, // Owned
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__MODIFIED = 0x8, // Modified
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__ANY_STATE = 0xf, // Any line state (shared
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__ALL_CORES = 0xf0, // All cores
		L3_EVICTIONS = 0x4e3, // L3 Evictions
		L3_EVICTIONS__MASK__AMD64_FAM10H_L3_EVICTIONS__SHARED = 0x1, // Shared
		L3_EVICTIONS__MASK__AMD64_FAM10H_L3_EVICTIONS__EXCLUSIVE = 0x2, // Exclusive
		L3_EVICTIONS__MASK__AMD64_FAM10H_L3_EVICTIONS__OWNED = 0x4, // Owned
		L3_EVICTIONS__MASK__AMD64_FAM10H_L3_EVICTIONS__MODIFIED = 0x8, // Modified
		L3_EVICTIONS__MASK__AMD64_FAM10H_L3_EVICTIONS__ALL = 0xf, // All sub-events selected
		PAGE_SIZE_MISMATCHES = 0x165, // Page Size Mismatches
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM10H_PAGE_SIZE_MISMATCHES__GUEST_LARGER = 0x1, // Guest page size is larger than the host page size.
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM10H_PAGE_SIZE_MISMATCHES__MTRR_MISMATCH = 0x2, // MTRR mismatch.
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM10H_PAGE_SIZE_MISMATCHES__HOST_LARGER = 0x4, // Host page size is larger than the guest page size.
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM10H_PAGE_SIZE_MISMATCHES__ALL = 0x7, // All sub-events selected
		RETIRED_X87_OPS = 0x1c0, // Retired x87 Floating Point Operations
		RETIRED_X87_OPS__MASK__AMD64_FAM10H_RETIRED_X87_OPS__ADD_SUB_OPS = 0x1, // Add/subtract ops
		RETIRED_X87_OPS__MASK__AMD64_FAM10H_RETIRED_X87_OPS__MUL_OPS = 0x2, // Multiply ops
		RETIRED_X87_OPS__MASK__AMD64_FAM10H_RETIRED_X87_OPS__DIV_OPS = 0x4, // Divide ops
		RETIRED_X87_OPS__MASK__AMD64_FAM10H_RETIRED_X87_OPS__ALL = 0x7, // All sub-events selected
		IBS_OPS_TAGGED = 0x1cf, // IBS Ops Tagged
		LFENCE_INST_RETIRED = 0x1d3, // LFENCE Instructions Retired
		SFENCE_INST_RETIRED = 0x1d4, // SFENCE Instructions Retired
		MFENCE_INST_RETIRED = 0x1d5, // MFENCE Instructions Retired
		READ_REQUEST_TO_L3_CACHE__REPEAT__1 = 0x4e0, // Read Request to L3 Cache
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_EXCLUSIVE = 0x1, // Read Block Exclusive (Data cache read)
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_SHARED = 0x2, // Read Block Shared (Instruction cache read)
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_MODIFY = 0x4, // Read Block Modify
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ANY_READ = 0x7, // Any read modes (exclusive
		READ_REQUEST_TO_L3_CACHE__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ALL_CORES = 0xf0, // All cores
		L3_CACHE_MISSES__REPEAT__1 = 0x4e1, // L3 Cache Misses
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_EXCLUSIVE__REPEAT__1 = 0x1, // Read Block Exclusive (Data cache read)
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_SHARED__REPEAT__1 = 0x2, // Read Block Shared (Instruction cache read)
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_MODIFY__REPEAT__1 = 0x4, // Read Block Modify
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ANY_READ__REPEAT__1 = 0x7, // Any read modes (exclusive
		L3_CACHE_MISSES__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ALL_CORES__REPEAT__1 = 0xf0, // All cores
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__REPEAT__1 = 0x4e2, // L3 Fills caused by L2 Evictions
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__SHARED__REPEAT__1 = 0x1, // Shared
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__EXCLUSIVE__REPEAT__1 = 0x2, // Exclusive
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__OWNED__REPEAT__1 = 0x4, // Owned
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__MODIFIED__REPEAT__1 = 0x8, // Modified
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__ANY_STATE__REPEAT__1 = 0xf, // Any line state (shared
		L3_FILLS_CAUSED_BY_L2_EVICTIONS__MASK__AMD64_FAM10H_L3_FILLS_CAUSED_BY_L2_EVICTIONS__ALL_CORES__REPEAT__1 = 0xf0, // All cores
		NON_CANCELLED_L3_READ_REQUESTS = 0x4ed, // Non-cancelled L3 Read Requests
		NON_CANCELLED_L3_READ_REQUESTS__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_EXCLUSIVE = 0x1, // Read Block Exclusive (Data cache read)
		NON_CANCELLED_L3_READ_REQUESTS__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_SHARED = 0x2, // Read Block Shared (Instruction cache read)
		NON_CANCELLED_L3_READ_REQUESTS__MASK__AMD64_FAM10H_L3_CACHE_MISSES__READ_BLOCK_MODIFY = 0x4, // Read Block Modify
		NON_CANCELLED_L3_READ_REQUESTS__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ANY_READ = 0x7, // Any read modes (exclusive
		NON_CANCELLED_L3_READ_REQUESTS__MASK__AMD64_FAM10H_L3_CACHE_MISSES__ALL_CORES = 0xf0, // All cores
		
	};
};

namespace fam10h = optkit::amd64::fam10h;