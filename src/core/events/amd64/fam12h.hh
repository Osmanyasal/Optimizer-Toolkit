#include <cstdint>
namespace optkit::amd64{
	enum class fam12h : uint64_t {
		DISPATCHED_FPU = 0x0, // Dispatched FPU Operations
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__OPS_ADD = 0x1, // Add pipe ops excluding load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__OPS_MULTIPLY = 0x2, // Multiply pipe ops excluding load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__OPS_STORE = 0x4, // Store pipe ops excluding load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__OPS_ADD_PIPE_LOAD_OPS = 0x8, // Add pipe load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__OPS_MULTIPLY_PIPE_LOAD_OPS = 0x10, // Multiply pipe load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__OPS_STORE_PIPE_LOAD_OPS = 0x20, // Store pipe load ops and SSE move ops
		DISPATCHED_FPU__MASK__AMD64_FAM12H_DISPATCHED_FPU__ALL = 0x3f, // All sub-events selected
		CYCLES_NO_FPU_OPS_RETIRED = 0x1, // Cycles in which the FPU is Empty
		DISPATCHED_FPU_OPS_FAST_FLAG = 0x2, // Dispatched Fast Flag FPU Operations
		RETIRED_SSE_OPERATIONS = 0x3, // Retired SSE Operations
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__SINGLE_ADD_SUB_OPS = 0x1, // Single precision add/subtract ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__SINGLE_MUL_OPS = 0x2, // Single precision multiply ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__SINGLE_DIV_OPS = 0x4, // Single precision divide/square root ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__DOUBLE_ADD_SUB_OPS = 0x8, // Double precision add/subtract ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__DOUBLE_MUL_OPS = 0x10, // Double precision multiply ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__DOUBLE_DIV_OPS = 0x20, // Double precision divide/square root ops
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__OP_TYPE = 0x40, // FLOPS
		RETIRED_SSE_OPERATIONS__MASK__AMD64_FAM12H_RETIRED_SSE_OPERATIONS__ALL = 0x7f, // All sub-events selected
		RETIRED_MOVE_OPS = 0x4, // Retired Move Ops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM12H_RETIRED_MOVE_OPS__LOW_QW_MOVE_UOPS = 0x1, // Merging low quadword move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM12H_RETIRED_MOVE_OPS__HIGH_QW_MOVE_UOPS = 0x2, // Merging high quadword move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM12H_RETIRED_MOVE_OPS__ALL_OTHER_MERGING_MOVE_UOPS = 0x4, // All other merging move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM12H_RETIRED_MOVE_OPS__ALL_OTHER_MOVE_UOPS = 0x8, // All other move uops
		RETIRED_MOVE_OPS__MASK__AMD64_FAM12H_RETIRED_MOVE_OPS__ALL = 0xf, // All sub-events selected
		RETIRED_SERIALIZING_OPS = 0x5, // Retired Serializing Ops
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM12H_RETIRED_SERIALIZING_OPS__SSE_BOTTOM_EXECUTING_UOPS = 0x1, // SSE bottom-executing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM12H_RETIRED_SERIALIZING_OPS__SSE_BOTTOM_SERIALIZING_UOPS = 0x2, // SSE bottom-serializing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM12H_RETIRED_SERIALIZING_OPS__X87_BOTTOM_EXECUTING_UOPS = 0x4, // X87 bottom-executing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM12H_RETIRED_SERIALIZING_OPS__X87_BOTTOM_SERIALIZING_UOPS = 0x8, // X87 bottom-serializing uops retired
		RETIRED_SERIALIZING_OPS__MASK__AMD64_FAM12H_RETIRED_SERIALIZING_OPS__ALL = 0xf, // All sub-events selected
		FP_SCHEDULER_CYCLES = 0x6, // Number of Cycles that a Serializing uop is in the FP Scheduler
		FP_SCHEDULER_CYCLES__MASK__AMD64_FAM12H_FP_SCHEDULER_CYCLES__BOTTOM_EXECUTE_CYCLES = 0x1, // Number of cycles a bottom-execute uop is in the FP scheduler
		FP_SCHEDULER_CYCLES__MASK__AMD64_FAM12H_FP_SCHEDULER_CYCLES__BOTTOM_SERIALIZING_CYCLES = 0x2, // Number of cycles a bottom-serializing uop is in the FP scheduler
		FP_SCHEDULER_CYCLES__MASK__AMD64_FAM12H_FP_SCHEDULER_CYCLES__ALL = 0x3, // All sub-events selected
		SEGMENT_REGISTER_LOADS = 0x20, // Segment Register Loads
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__ES = 0x1, // ES
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__CS = 0x2, // CS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__SS = 0x4, // SS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__DS = 0x8, // DS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__FS = 0x10, // FS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__GS = 0x20, // GS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__HS = 0x40, // HS
		SEGMENT_REGISTER_LOADS__MASK__AMD64_FAM12H_SEGMENT_REGISTER_LOADS__ALL = 0x7f, // All sub-events selected
		PIPELINE_RESTART_DUE_TO_SELF_MODIFYING_CODE = 0x21, // Pipeline Restart Due to Self-Modifying Code
		PIPELINE_RESTART_DUE_TO_PROBE_HIT = 0x22, // Pipeline Restart Due to Probe Hit
		LS_BUFFER_2_FULL_CYCLES = 0x23, // LS Buffer 2 Full
		LOCKED_OPS = 0x24, // Locked Operations
		LOCKED_OPS__MASK__AMD64_FAM12H_LOCKED_OPS__EXECUTED = 0x1, // The number of locked instructions executed
		LOCKED_OPS__MASK__AMD64_FAM12H_LOCKED_OPS__CYCLES_SPECULATIVE_PHASE = 0x2, // The number of cycles spent in speculative phase
		LOCKED_OPS__MASK__AMD64_FAM12H_LOCKED_OPS__CYCLES_NON_SPECULATIVE_PHASE = 0x4, // The number of cycles spent in non-speculative phase (including cache miss penalty)
		LOCKED_OPS__MASK__AMD64_FAM12H_LOCKED_OPS__CYCLES_WAITING = 0x8, // The number of cycles waiting for a cache hit (cache miss penalty).
		LOCKED_OPS__MASK__AMD64_FAM12H_LOCKED_OPS__ALL = 0xf, // All sub-events selected
		RETIRED_CLFLUSH_INSTRUCTIONS = 0x26, // Retired CLFLUSH Instructions
		RETIRED_CPUID_INSTRUCTIONS = 0x27, // Retired CPUID Instructions
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS = 0x2a, // Cancelled Store to Load Forward Operations
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM12H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__ADDRESS_MISMATCHES = 0x1, // Address mismatches (starting byte not the same).
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM12H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__STORE_IS_SMALLER_THAN_LOAD = 0x2, // Store is smaller than load.
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM12H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MISALIGNED = 0x4, // Misaligned.
		CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__MASK__AMD64_FAM12H_CANCELLED_STORE_TO_LOAD_FORWARD_OPERATIONS__ALL = 0x7, // All sub-events selected
		SMIS_RECEIVED = 0x2b, // SMIs Received
		DATA_CACHE_ACCESSES = 0x40, // Data Cache Accesses
		DATA_CACHE_MISSES = 0x41, // Data Cache Misses
		DATA_CACHE_REFILLS = 0x42, // Data Cache Refills from L2 or Northbridge
		DATA_CACHE_REFILLS__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS__SYSTEM = 0x1, // Refill from the Northbridge
		DATA_CACHE_REFILLS__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS__L2_SHARED = 0x2, // Shared-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS__L2_EXCLUSIVE = 0x4, // Exclusive-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS__L2_OWNED = 0x8, // Owned-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS__L2_MODIFIED = 0x10, // Modified-state line from L2
		DATA_CACHE_REFILLS__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS__ALL = 0x1f, // All sub-events selected
		DATA_CACHE_REFILLS_FROM_SYSTEM = 0x43, // Data Cache Refills from the Northbridge
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS_FROM_NORTHBRIDGE__INVALID = 0x1, // Invalid
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS_FROM_NORTHBRIDGE__SHARED = 0x2, // Shared
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS_FROM_NORTHBRIDGE__EXCLUSIVE = 0x4, // Exclusive
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS_FROM_NORTHBRIDGE__OWNED = 0x8, // Owned
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS_FROM_NORTHBRIDGE__MODIFIED = 0x10, // Modified
		DATA_CACHE_REFILLS_FROM_SYSTEM__MASK__AMD64_FAM12H_DATA_CACHE_REFILLS_FROM_NORTHBRIDGE__ALL = 0x1f, // All sub-events selected
		DATA_CACHE_LINES_EVICTED = 0x44, // Data Cache Lines Evicted
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__INVALID = 0x1, // Invalid
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__SHARED = 0x2, // Shared
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__EXCLUSIVE = 0x4, // Exclusive
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__OWNED = 0x8, // Owned
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__MODIFIED = 0x10, // Modified
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__BY_PREFETCHNTA = 0x20, // Cache line evicted was brought into the cache with by a PrefetchNTA instruction.
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__NOT_BY_PREFETCHNTA = 0x40, // Cache line evicted was not brought into the cache with by a PrefetchNTA instruction.
		DATA_CACHE_LINES_EVICTED__MASK__AMD64_FAM12H_DATA_CACHE_LINES_EVICTED__ALL = 0x7f, // All sub-events selected
		L1_DTLB_MISS_AND_L2_DTLB_HIT = 0x45, // L1 DTLB Miss and L2 DTLB Hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_MISS_AND_L2_DTLB_HIT__L2_4K_TLB_HIT = 0x1, // L2 4K TLB hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_MISS_AND_L2_DTLB_HIT__L2_2M_TLB_HIT = 0x2, // L2 2M TLB hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_MISS_AND_L2_DTLB_HIT__L2_1G_TLB_HIT = 0x4, // L2 1G TLB hit
		L1_DTLB_MISS_AND_L2_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_MISS_AND_L2_DTLB_HIT__ALL = 0x7, // All sub-events selected
		L1_DTLB_AND_L2_DTLB_MISS = 0x46, // L1 DTLB and L2 DTLB Miss
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM12H_L1_DTLB_AND_L2_DTLB_MISS__4K_TLB_RELOAD = 0x1, // 4K TLB reload
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM12H_L1_DTLB_AND_L2_DTLB_MISS__2M_TLB_RELOAD = 0x2, // 2M TLB reload
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM12H_L1_DTLB_AND_L2_DTLB_MISS__1G_TLB_RELOAD = 0x4, // 1G TLB reload
		L1_DTLB_AND_L2_DTLB_MISS__MASK__AMD64_FAM12H_L1_DTLB_AND_L2_DTLB_MISS__ALL = 0x7, // All sub-events selected
		MISALIGNED_ACCESSES = 0x47, // Misaligned Accesses
		MICROARCHITECTURAL_LATE_CANCEL_OF_AN_ACCESS = 0x48, // Microarchitectural Late Cancel of an Access
		MICROARCHITECTURAL_EARLY_CANCEL_OF_AN_ACCESS = 0x49, // Microarchitectural Early Cancel of an Access
		PREFETCH_INSTRUCTIONS_DISPATCHED = 0x4b, // Prefetch Instructions Dispatched
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM12H_PREFETCH_INSTRUCTIONS_DISPATCHED__LOAD = 0x1, // Load (Prefetch
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM12H_PREFETCH_INSTRUCTIONS_DISPATCHED__STORE = 0x2, // Store (PrefetchW)
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM12H_PREFETCH_INSTRUCTIONS_DISPATCHED__NTA = 0x4, // NTA (PrefetchNTA)
		PREFETCH_INSTRUCTIONS_DISPATCHED__MASK__AMD64_FAM12H_PREFETCH_INSTRUCTIONS_DISPATCHED__ALL = 0x7, // All sub-events selected
		DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS = 0x4c, // DCACHE Misses by Locked Instructions
		DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__MASK__AMD64_FAM12H_DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__DATA_CACHE_MISSES_BY_LOCKED_INSTRUCTIONS = 0x2, // Data cache misses by locked instructions
		DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__MASK__AMD64_FAM12H_DCACHE_MISSES_BY_LOCKED_INSTRUCTIONS__ALL = 0x2, // All sub-events selected
		L1_DTLB_HIT = 0x4d, // L1 DTLB Hit
		L1_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_HIT__L1_4K_TLB_HIT = 0x1, // L1 4K TLB hit
		L1_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_HIT__L1_2M_TLB_HIT = 0x2, // L1 2M TLB hit
		L1_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_HIT__L1_1G_TLB_HIT = 0x4, // L1 1G TLB hit
		L1_DTLB_HIT__MASK__AMD64_FAM12H_L1_DTLB_HIT__ALL = 0x7, // All sub-events selected
		INEFFECTIVE_SW_PREFETCHES = 0x52, // Ineffective Software Prefetches
		INEFFECTIVE_SW_PREFETCHES__MASK__AMD64_FAM12H_INEFFECTIVE_SW_PREFETCHES__SW_PREFETCH_HIT_IN_L1 = 0x1, // Software prefetch hit in the L1.
		INEFFECTIVE_SW_PREFETCHES__MASK__AMD64_FAM12H_INEFFECTIVE_SW_PREFETCHES__SW_PREFETCH_HIT_IN_L2 = 0x8, // Software prefetch hit in L2.
		INEFFECTIVE_SW_PREFETCHES__MASK__AMD64_FAM12H_INEFFECTIVE_SW_PREFETCHES__ALL = 0x9, // All sub-events selected
		GLOBAL_TLB_FLUSHES = 0x54, // Global TLB Flushes
		MEMORY_REQUESTS = 0x65, // Memory Requests by Type
		MEMORY_REQUESTS__MASK__AMD64_FAM12H_MEMORY_REQUESTS__NON_CACHEABLE = 0x1, // Requests to non-cacheable (UC) memory
		MEMORY_REQUESTS__MASK__AMD64_FAM12H_MEMORY_REQUESTS__WRITE_COMBINING = 0x2, // Requests to write-combining (WC) memory or WC buffer flushes to WB memory
		MEMORY_REQUESTS__MASK__AMD64_FAM12H_MEMORY_REQUESTS__CACHE_DISABLED = 0x4, // Requests to cache-disabled (CD) memory
		MEMORY_REQUESTS__MASK__AMD64_FAM12H_MEMORY_REQUESTS__STREAMING_STORE = 0x80, // Streaming store (SS) requests
		MEMORY_REQUESTS__MASK__AMD64_FAM12H_MEMORY_REQUESTS__ALL = 0x87, // All sub-events selected
		DATA_PREFETCHES = 0x67, // Data Prefetcher
		DATA_PREFETCHES__MASK__AMD64_FAM12H_DATA_PREFETCHES__CANCELLED = 0x1, // Cancelled prefetches
		DATA_PREFETCHES__MASK__AMD64_FAM12H_DATA_PREFETCHES__ATTEMPTED = 0x2, // Prefetch attempts
		DATA_PREFETCHES__MASK__AMD64_FAM12H_DATA_PREFETCHES__ALL = 0x3, // All sub-events selected
		NORTHBRIDGE_READ_RESPONSES = 0x6c, // Northbridge Read Responses by Coherency State
		NORTHBRIDGE_READ_RESPONSES__MASK__AMD64_FAM12H_NORTHBRIDGE_READ_RESPONSES__EXCLUSIVE = 0x1, // Exclusive
		NORTHBRIDGE_READ_RESPONSES__MASK__AMD64_FAM12H_NORTHBRIDGE_READ_RESPONSES__MODIFIED = 0x2, // Modified
		NORTHBRIDGE_READ_RESPONSES__MASK__AMD64_FAM12H_NORTHBRIDGE_READ_RESPONSES__SHARED = 0x4, // Shared
		NORTHBRIDGE_READ_RESPONSES__MASK__AMD64_FAM12H_NORTHBRIDGE_READ_RESPONSES__OWNED = 0x8, // Owned
		NORTHBRIDGE_READ_RESPONSES__MASK__AMD64_FAM12H_NORTHBRIDGE_READ_RESPONSES__DATA_ERROR = 0x10, // Data Error
		NORTHBRIDGE_READ_RESPONSES__MASK__AMD64_FAM12H_NORTHBRIDGE_READ_RESPONSES__ALL = 0x1f, // All sub-events selected
		OCTWORDS_WRITTEN_TO_SYSTEM = 0x6d, // Octwords Written to System
		OCTWORDS_WRITTEN_TO_SYSTEM__MASK__AMD64_FAM12H_OCTWORDS_WRITTEN_TO_SYSTEM__OCTWORD_WRITE_TRANSFER = 0x1, // Octword write transfer
		OCTWORDS_WRITTEN_TO_SYSTEM__MASK__AMD64_FAM12H_OCTWORDS_WRITTEN_TO_SYSTEM__ALL = 0x1, // All sub-events selected
		CPU_CLK_UNHALTED = 0x76, // CPU Clocks not Halted
		REQUESTS_TO_L2 = 0x7d, // Requests to L2 Cache
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__INSTRUCTIONS = 0x1, // IC fill
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__DATA = 0x2, // DC fill
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__TLB_WALK = 0x4, // TLB fill (page table walks)
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__SNOOP = 0x8, // Tag snoop request
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__CANCELLED = 0x10, // Cancelled request
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__HW_PREFETCH_FROM_DC = 0x20, // Hardware prefetch from DC
		REQUESTS_TO_L2__MASK__AMD64_FAM12H_REQUESTS_TO_L2__ALL = 0x3f, // All sub-events selected
		L2_CACHE_MISS = 0x7e, // L2 Cache Misses
		L2_CACHE_MISS__MASK__AMD64_FAM12H_L2_CACHE_MISS__INSTRUCTIONS = 0x1, // IC fill
		L2_CACHE_MISS__MASK__AMD64_FAM12H_L2_CACHE_MISS__DATA = 0x2, // DC fill (includes possible replays
		L2_CACHE_MISS__MASK__AMD64_FAM12H_L2_CACHE_MISS__TLB_WALK = 0x4, // TLB page table walk
		L2_CACHE_MISS__MASK__AMD64_FAM12H_L2_CACHE_MISS__HW_PREFETCH_FROM_DC = 0x8, // Hardware prefetch from DC
		L2_CACHE_MISS__MASK__AMD64_FAM12H_L2_CACHE_MISS__ALL = 0xf, // All sub-events selected
		L2_FILL_WRITEBACK = 0x7f, // L2 Fill/Writeback
		L2_FILL_WRITEBACK__MASK__AMD64_FAM12H_L2_FILL_WRITEBACK__L2_FILLS = 0x1, // L2 fills (victims from L1 caches
		L2_FILL_WRITEBACK__MASK__AMD64_FAM12H_L2_FILL_WRITEBACK__L2_WRITEBACKS = 0x2, // L2 Writebacks to system.
		L2_FILL_WRITEBACK__MASK__AMD64_FAM12H_L2_FILL_WRITEBACK__ALL = 0x3, // All sub-events selected
		PAGE_SIZE_MISMATCHES = 0x165, // Page Size Mismatches
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM12H_PAGE_SIZE_MISMATCHES__GUEST_LARGER = 0x1, // Guest page size is larger than the host page size.
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM12H_PAGE_SIZE_MISMATCHES__MTRR_MISMATCH = 0x2, // MTRR mismatch.
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM12H_PAGE_SIZE_MISMATCHES__HOST_LARGER = 0x4, // Host page size is larger than the guest page size.
		PAGE_SIZE_MISMATCHES__MASK__AMD64_FAM12H_PAGE_SIZE_MISMATCHES__ALL = 0x7, // All sub-events selected
		INSTRUCTION_CACHE_FETCHES = 0x80, // Instruction Cache Fetches
		INSTRUCTION_CACHE_MISSES = 0x81, // Instruction Cache Misses
		INSTRUCTION_CACHE_REFILLS_FROM_L2 = 0x82, // Instruction Cache Refills from L2
		INSTRUCTION_CACHE_REFILLS_FROM_SYSTEM = 0x83, // Instruction Cache Refills from System
		L1_ITLB_MISS_AND_L2_ITLB_HIT = 0x84, // L1 ITLB Miss and L2 ITLB Hit
		L1_ITLB_MISS_AND_L2_ITLB_MISS = 0x85, // L1 ITLB Miss and L2 ITLB Miss
		L1_ITLB_MISS_AND_L2_ITLB_MISS__MASK__AMD64_FAM12H_L1_ITLB_MISS_AND_L2_ITLB_MISS__4K_PAGE_FETCHES = 0x1, // Instruction fetches to a 4K page.
		L1_ITLB_MISS_AND_L2_ITLB_MISS__MASK__AMD64_FAM12H_L1_ITLB_MISS_AND_L2_ITLB_MISS__2M_PAGE_FETCHES = 0x2, // Instruction fetches to a 2M page.
		L1_ITLB_MISS_AND_L2_ITLB_MISS__MASK__AMD64_FAM12H_L1_ITLB_MISS_AND_L2_ITLB_MISS__ALL = 0x3, // All sub-events selected
		PIPELINE_RESTART_DUE_TO_INSTRUCTION_STREAM_PROBE = 0x86, // Pipeline Restart Due to Instruction Stream Probe
		INSTRUCTION_FETCH_STALL = 0x87, // Instruction Fetch Stall
		RETURN_STACK_HITS = 0x88, // Return Stack Hits
		RETURN_STACK_OVERFLOWS = 0x89, // Return Stack Overflows
		INSTRUCTION_CACHE_VICTIMS = 0x8b, // Instruction Cache Victims
		INSTRUCTION_CACHE_LINES_INVALIDATED = 0x8c, // Instruction Cache Lines Invalidated
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM12H_INSTRUCTION_CACHE_LINES_INVALIDATED__INVALIDATING_PROBE_NO_IN_FLIGHT = 0x1, // Invalidating probe that did not hit any in-flight instructions.
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM12H_INSTRUCTION_CACHE_LINES_INVALIDATED__INVALIDATING_PROBE_ONE_OR_MORE_IN_FLIGHT = 0x2, // Invalidating probe that hit one or more in-flight instructions.
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM12H_INSTRUCTION_CACHE_LINES_INVALIDATED__SMC_NO_INFLIGHT = 0x4, // SMC that did not hit any in-flight instructions.
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM12H_INSTRUCTION_CACHE_LINES_INVALIDATED__SMC_INFLIGHT = 0x8, // SMC that hit one or more in-flight instructions.
		INSTRUCTION_CACHE_LINES_INVALIDATED__MASK__AMD64_FAM12H_INSTRUCTION_CACHE_LINES_INVALIDATED__ALL = 0xf, // All sub-events selected
		ITLB_RELOADS = 0x99, // ITLB Reloads
		ITLB_RELOADS_ABORTED = 0x9a, // ITLB Reloads Aborted
		RETIRED_INSTRUCTIONS = 0xc0, // Retired Instructions
		RETIRED_UOPS = 0xc1, // Retired uops
		RETIRED_BRANCH_INSTRUCTIONS = 0xc2, // Retired Branch Instructions
		RETIRED_MISPREDICTED_BRANCH_INSTRUCTIONS = 0xc3, // Retired Mispredicted Branch Instructions
		RETIRED_TAKEN_BRANCH_INSTRUCTIONS = 0xc4, // Retired Taken Branch Instructions
		RETIRED_TAKEN_BRANCH_INSTRUCTIONS_MISPREDICTED = 0xc5, // Retired Taken Branch Instructions Mispredicted
		RETIRED_FAR_CONTROL_TRANSFERS = 0xc6, // Retired Far Control Transfers
		RETIRED_BRANCH_RESYNCS = 0xc7, // Retired Branch Resyncs
		RETIRED_NEAR_RETURNS = 0xc8, // Retired Near Returns
		RETIRED_NEAR_RETURNS_MISPREDICTED = 0xc9, // Retired Near Returns Mispredicted
		RETIRED_INDIRECT_BRANCHES_MISPREDICTED = 0xca, // Retired Indirect Branches Mispredicted
		RETIRED_MMX_AND_FP_INSTRUCTIONS = 0xcb, // Retired MMX/FP Instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM12H_RETIRED_MMX_AND_FP_INSTRUCTIONS__X87 = 0x1, // X87 instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM12H_RETIRED_MMX_AND_FP_INSTRUCTIONS__MMX_AND_3DNOW = 0x2, // MMX and 3DNow! instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM12H_RETIRED_MMX_AND_FP_INSTRUCTIONS__SSE_AND_SSE2 = 0x4, // SSE and SSE2 instructions
		RETIRED_MMX_AND_FP_INSTRUCTIONS__MASK__AMD64_FAM12H_RETIRED_MMX_AND_FP_INSTRUCTIONS__ALL = 0x7, // All sub-events selected
		INTERRUPTS_MASKED_CYCLES = 0xcd, // Interrupts-Masked Cycles
		INTERRUPTS_MASKED_CYCLES_WITH_INTERRUPT_PENDING = 0xce, // Interrupts-Masked Cycles with Interrupt Pending
		INTERRUPTS_TAKEN = 0xcf, // Interrupts Taken
		DECODER_EMPTY = 0xd0, // Decoder Empty
		DISPATCH_STALLS = 0xd1, // Dispatch Stalls
		DISPATCH_STALL_FOR_BRANCH_ABORT = 0xd2, // Dispatch Stall for Branch Abort to Retire
		DISPATCH_STALL_FOR_SERIALIZATION = 0xd3, // Dispatch Stall for Serialization
		DISPATCH_STALL_FOR_SEGMENT_LOAD = 0xd4, // Dispatch Stall for Segment Load
		DISPATCH_STALL_FOR_REORDER_BUFFER_FULL = 0xd5, // Dispatch Stall for Reorder Buffer Full
		DISPATCH_STALL_FOR_RESERVATION_STATION_FULL = 0xd6, // Dispatch Stall for Reservation Station Full
		DISPATCH_STALL_FOR_FPU_FULL = 0xd7, // Dispatch Stall for FPU Full
		DISPATCH_STALL_FOR_LS_FULL = 0xd8, // Dispatch Stall for LS Full
		DISPATCH_STALL_WAITING_FOR_ALL_QUIET = 0xd9, // Dispatch Stall Waiting for All Quiet
		DISPATCH_STALL_FOR_FAR_TRANSFER_OR_RSYNC = 0xda, // Dispatch Stall for Far Transfer or Resync to Retire
		FPU_EXCEPTIONS = 0xdb, // FPU Exceptions
		FPU_EXCEPTIONS__MASK__AMD64_FAM12H_FPU_EXCEPTIONS__X87_RECLASS_MICROFAULTS = 0x1, // X87 reclass microfaults
		FPU_EXCEPTIONS__MASK__AMD64_FAM12H_FPU_EXCEPTIONS__SSE_RETYPE_MICROFAULTS = 0x2, // SSE retype microfaults
		FPU_EXCEPTIONS__MASK__AMD64_FAM12H_FPU_EXCEPTIONS__SSE_RECLASS_MICROFAULTS = 0x4, // SSE reclass microfaults
		FPU_EXCEPTIONS__MASK__AMD64_FAM12H_FPU_EXCEPTIONS__SSE_AND_X87_MICROTRAPS = 0x8, // SSE and x87 microtraps
		FPU_EXCEPTIONS__MASK__AMD64_FAM12H_FPU_EXCEPTIONS__ALL = 0xf, // All sub-events selected
		DR0_BREAKPOINT_MATCHES = 0xdc, // DR0 Breakpoint Matches
		DR1_BREAKPOINT_MATCHES = 0xdd, // DR1 Breakpoint Matches
		DR2_BREAKPOINT_MATCHES = 0xde, // DR2 Breakpoint Matches
		DR3_BREAKPOINT_MATCHES = 0xdf, // DR3 Breakpoint Matches
		RETIRED_X87_OPS = 0x1c0, // Retired x87 Floating Point Operations
		RETIRED_X87_OPS__MASK__AMD64_FAM12H_RETIRED_X87_OPS__ADD_SUB_OPS = 0x1, // Add/subtract ops
		RETIRED_X87_OPS__MASK__AMD64_FAM12H_RETIRED_X87_OPS__MUL_OPS = 0x2, // Multiply ops
		RETIRED_X87_OPS__MASK__AMD64_FAM12H_RETIRED_X87_OPS__DIV_OPS = 0x4, // Divide ops
		RETIRED_X87_OPS__MASK__AMD64_FAM12H_RETIRED_X87_OPS__ALL = 0x7, // All sub-events selected
		LFENCE_INST_RETIRED = 0x1d3, // LFENCE Instructions Retired
		SFENCE_INST_RETIRED = 0x1d4, // SFENCE Instructions Retired
		MFENCE_INST_RETIRED = 0x1d5, // MFENCE Instructions Retired
		DRAM_ACCESSES_PAGE = 0xe0, // DRAM Accesses
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__DCT0_HIT = 0x1, // DCT0 Page hit
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__DCT0_MISS = 0x2, // DCT0 Page Miss
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__DCT0_CONFLICT = 0x4, // DCT0 Page Conflict
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__DCT1_PAGE_HIT = 0x8, // DCT1 Page hit
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__DCT1_PAGE_MISS = 0x10, // DCT1 Page Miss
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__DCT1_PAGE_CONFLICT = 0x20, // DCT1 Page Conflict
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__WRITE_REQUEST = 0x40, // Write request.
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__READ_REQUEST = 0x80, // Read request.
		DRAM_ACCESSES_PAGE__MASK__AMD64_FAM12H_DRAM_ACCESSES_PAGE__ALL = 0xff, // All sub-events selected
		MEMORY_CONTROLLER_0_PAGE = 0xe1, // DRAM Controller 0 Page Table Events
		MEMORY_CONTROLLER_0_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_OVERFLOW = 0x1, // Page Table Overflow
		MEMORY_CONTROLLER_0_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__STALE_TABLE_ENTRY_HITS = 0x2, // Number of stale table entry hits. (hit on a page closed too soon).
		MEMORY_CONTROLLER_0_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_IDLE_CYCLE_LIMIT_INCREMENTED = 0x4, // Page table idle cycle limit incremented.
		MEMORY_CONTROLLER_0_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_IDLE_CYCLE_LIMIT_DECREMENTED = 0x8, // Page table idle cycle limit decremented.
		MEMORY_CONTROLLER_0_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_CLOSED_INACTIVITY = 0x10, // Page table is closed due to row inactivity.
		MEMORY_CONTROLLER_0_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__ALL = 0x1f, // All sub-events selected
		MEMORY_CONTROLLER_SLOT_MISSES = 0xe2, // Memory Controller DRAM Command Slots Missed
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_SLOT_MISSES__DCT0_RBD = 0x10, // DCT0 RBD.
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_SLOT_MISSES__DCT1_RBD = 0x20, // DCT1 RBD.
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_SLOT_MISSES__DCT0_PREFETCH = 0x40, // DCT0 Prefetch.
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_SLOT_MISSES__DCT1_PREFETCH = 0x80, // DCT1 Prefetch.
		MEMORY_CONTROLLER_SLOT_MISSES__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_SLOT_MISSES__ALL = 0xf0, // All sub-events selected
		MEMORY_CONTROLLER_TURNAROUNDS = 0xe3, // Memory Controller Turnarounds
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_TURNAROUNDS__DCT0_READ_TO_WRITE = 0x1, // DCT0 read-to-write turnaround.
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_TURNAROUNDS__DCT0_WRITE_TO_READ = 0x2, // DCT0 write-to-read turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_TURNAROUNDS__DCT1_READ_TO_WRITE = 0x8, // DCT1 read-to-write turnaround.
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_TURNAROUNDS__DCT1_WRITE_TO_READ = 0x10, // DCT1 write-to-read turnaround
		MEMORY_CONTROLLER_TURNAROUNDS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_TURNAROUNDS__ALL = 0x1b, // All sub-events selected
		MEMORY_CONTROLLER_RBD_QUEUE = 0xe4, // Memory Controller RBD Queue Events
		MEMORY_CONTROLLER_RBD_QUEUE__MASK__AMD64_FAM12H_MEMORY_RBD_QUEUE__COUNTER_REACHED = 0x4, // D18F2x[1
		MEMORY_CONTROLLER_RBD_QUEUE__MASK__AMD64_FAM12H_MEMORY_RBD_QUEUE__BANK_CLOSED = 0x8, // Bank is closed due to bank conflict with an outstanding request in the RBD queue.
		MEMORY_CONTROLLER_RBD_QUEUE__MASK__AMD64_FAM12H_MEMORY_RBD_QUEUE__ALL = 0xc, // All sub-events selected
		MEMORY_CONTROLLER_1_PAGE = 0xe5, // DRAM Controller 1 Page Table Events
		MEMORY_CONTROLLER_1_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_OVERFLOW = 0x1, // Page Table Overflow
		MEMORY_CONTROLLER_1_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__STALE_TABLE_ENTRY_HITS = 0x2, // Number of stale table entry hits. (hit on a page closed too soon).
		MEMORY_CONTROLLER_1_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_IDLE_CYCLE_LIMIT_INCREMENTED = 0x4, // Page table idle cycle limit incremented.
		MEMORY_CONTROLLER_1_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_IDLE_CYCLE_LIMIT_DECREMENTED = 0x8, // Page table idle cycle limit decremented.
		MEMORY_CONTROLLER_1_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__PAGE_TABLE_CLOSED_INACTIVITY = 0x10, // Page table is closed due to row inactivity.
		MEMORY_CONTROLLER_1_PAGE__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_PAGE_TABLE_EVENTS__ALL = 0x1f, // All sub-events selected
		THERMAL_STATUS = 0xe8, // Thermal Status
		THERMAL_STATUS__MASK__AMD64_FAM12H_THERMAL_STATUS__MEMHOT_L_ASSERTIONS = 0x1, // MEMHOT_L assertions.
		THERMAL_STATUS__MASK__AMD64_FAM12H_THERMAL_STATUS__HTC_TRANSITIONS = 0x4, // Number of times the HTC transitions from inactive to active.
		THERMAL_STATUS__MASK__AMD64_FAM12H_THERMAL_STATUS__CLOCKS_HTC_P_STATE_INACTIVE = 0x20, // Number of clocks HTC P-state is inactive.
		THERMAL_STATUS__MASK__AMD64_FAM12H_THERMAL_STATUS__CLOCKS_HTC_P_STATE_ACTIVE = 0x40, // Number of clocks HTC P-state is active
		THERMAL_STATUS__MASK__AMD64_FAM12H_THERMAL_STATUS__PROCHOT_L_ASSERTIONS = 0x80, // PROCHOT_L asserted by an external source and the assertion causes a P-state change.
		THERMAL_STATUS__MASK__AMD64_FAM12H_THERMAL_STATUS__ALL = 0xe5, // All sub-events selected
		CPU_IO_REQUESTS_TO_MEMORY_IO = 0xe9, // CPU/IO Requests to Memory/IO
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM12H_CPU_IO_REQUESTS_TO_MEMORY_IO__I_O_TO_I_O = 0x1, // IO to IO
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM12H_CPU_IO_REQUESTS_TO_MEMORY_IO__I_O_TO_MEM = 0x2, // IO to Mem
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM12H_CPU_IO_REQUESTS_TO_MEMORY_IO__CPU_TO_I_O = 0x4, // CPU to IO
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM12H_CPU_IO_REQUESTS_TO_MEMORY_IO__CPU_TO_MEM = 0x8, // CPU to Mem
		CPU_IO_REQUESTS_TO_MEMORY_IO__MASK__AMD64_FAM12H_CPU_IO_REQUESTS_TO_MEMORY_IO__ALL = 0x0f, // All sub-events selected
		CACHE_BLOCK = 0xea, // Cache Block Commands
		CACHE_BLOCK__MASK__AMD64_FAM12H_CACHE_BLOCK__VICTIM_WRITEBACK = 0x1, // Victim Block (Writeback)
		CACHE_BLOCK__MASK__AMD64_FAM12H_CACHE_BLOCK__DCACHE_LOAD_MISS = 0x4, // Read Block (Dcache load miss refill)
		CACHE_BLOCK__MASK__AMD64_FAM12H_CACHE_BLOCK__SHARED_ICACHE_REFILL = 0x8, // Read Block Shared (Icache refill)
		CACHE_BLOCK__MASK__AMD64_FAM12H_CACHE_BLOCK__READ_BLOCK_MODIFIED = 0x10, // Read Block Modified (Dcache store miss refill)
		CACHE_BLOCK__MASK__AMD64_FAM12H_CACHE_BLOCK__READ_TO_DIRTY = 0x20, // Change-to-Dirty (first store to clean block already in cache)
		CACHE_BLOCK__MASK__AMD64_FAM12H_CACHE_BLOCK__ALL = 0x3d, // All sub-events selected
		SIZED_COMMANDS = 0xeb, // Sized Commands
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__NON_POSTED_WRITE_BYTE = 0x1, // Non-Posted SzWr Byte (1-32 bytes) Legacy or mapped IO
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__NON_POSTED_WRITE_DWORD = 0x2, // Non-Posted SzWr DW (1-16 dwords) Legacy or mapped IO
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__POSTED_WRITE_BYTE = 0x4, // Posted SzWr Byte (1-32 bytes) Subcache-line DMA writes
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__POSTED_WRITE_DWORD = 0x8, // Posted SzWr DW (1-16 dwords) Block-oriented DMA writes
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__READ_BYTE_4_BYTES = 0x10, // SzRd Byte (4 bytes) Legacy or mapped IO
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__READ_DWORD_1_16_DWORDS = 0x20, // SzRd DW (1-16 dwords) Block-oriented DMA reads
		SIZED_COMMANDS__MASK__AMD64_FAM12H_SIZED_COMMANDS__ALL = 0x3f, // All sub-events selected
		PROBE = 0xec, // Probe Responses and Upstream Requests
		PROBE__MASK__AMD64_FAM12H_PROBE__MISS = 0x1, // Probe miss
		PROBE__MASK__AMD64_FAM12H_PROBE__HIT_CLEAN = 0x2, // Probe hit clean
		PROBE__MASK__AMD64_FAM12H_PROBE__HIT_DIRTY_NO_MEMORY_CANCEL = 0x4, // Probe hit dirty without memory cancel (probed by Sized Write or Change2Dirty)
		PROBE__MASK__AMD64_FAM12H_PROBE__HIT_DIRTY_WITH_MEMORY_CANCEL = 0x8, // Probe hit dirty with memory cancel (probed by DMA read or cache refill request)
		PROBE__MASK__AMD64_FAM12H_PROBE__UPSTREAM_HIGH_PRIORITY_READS = 0x10, // Upstream high priority reads.
		PROBE__MASK__AMD64_FAM12H_PROBE__UPSTREAM_LOW_PRIORITY_READS = 0x20, // Upstream low priority reads.
		PROBE__MASK__AMD64_FAM12H_PROBE__UPSTREAM_LOW_PRIORITY_WRITES = 0x80, // Upstream low priority writes.
		PROBE__MASK__AMD64_FAM12H_PROBE__ALL = 0xbf, // All sub-events selected
		DEV = 0xee, // DEV Events
		DEV__MASK__AMD64_FAM12H_DEV__DEV_HIT = 0x10, // DEV hit
		DEV__MASK__AMD64_FAM12H_DEV__DEV_MISS = 0x20, // DEV miss
		DEV__MASK__AMD64_FAM12H_DEV__DEV_ERROR = 0x40, // DEV error
		DEV__MASK__AMD64_FAM12H_DEV__ALL = 0x70, // All sub-events selected
		MEMORY_CONTROLLER_REQUESTS = 0x1f0, // Memory Controller Requests
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_REQUESTS__32_BYTES_WRITES = 0x8, // 32 Bytes Sized Writes
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_REQUESTS__64_BYTES_WRITES = 0x10, // 64 Bytes Sized Writes
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_REQUESTS__32_BYTES_READS = 0x20, // 32 Bytes Sized Reads
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_REQUESTS__64_BYTES_READS = 0x40, // 64 Byte Sized Reads
		MEMORY_CONTROLLER_REQUESTS__MASK__AMD64_FAM12H_MEMORY_CONTROLLER_REQUESTS__ALL = 0x78, // All sub-events selected
		SIDEBAND_SIGNALS = 0x1e9, // Sideband Signals and Special Cycles
		SIDEBAND_SIGNALS__MASK__AMD64_FAM12H_SIDEBAND_SIGNALS__STOPGRANT = 0x2, // STOPGRANT
		SIDEBAND_SIGNALS__MASK__AMD64_FAM12H_SIDEBAND_SIGNALS__SHUTDOWN = 0x4, // SHUTDOWN
		SIDEBAND_SIGNALS__MASK__AMD64_FAM12H_SIDEBAND_SIGNALS__WBINVD = 0x8, // WBINVD
		SIDEBAND_SIGNALS__MASK__AMD64_FAM12H_SIDEBAND_SIGNALS__INVD = 0x10, // INVD
		SIDEBAND_SIGNALS__MASK__AMD64_FAM12H_SIDEBAND_SIGNALS__ALL = 0x1e, // All sub-events selected
		INTERRUPT_EVENTS = 0x1ea, // Interrupt Events
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__FIXED_AND_LPA = 0x1, // Fixed and LPA
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__LPA = 0x2, // LPA
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__SMI = 0x4, // SMI
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__NMI = 0x8, // NMI
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__INIT = 0x10, // INIT
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__STARTUP = 0x20, // STARTUP
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__INT = 0x40, // INT
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__EOI = 0x80, // EOI
		INTERRUPT_EVENTS__MASK__AMD64_FAM12H_INTERRUPT_EVENTS__ALL = 0xff, // All sub-events selected
		
	};
};