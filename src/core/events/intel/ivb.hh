#include <cstdint>
#include <intel_priv.hh>
namespace optkit::intel::ivb{
	enum ivb : uint64_t {
		ARITH = 0x14, // Counts arithmetic multiply operations
		ARITH__MASK__IVB_ARITH__FPU_DIV_ACTIVE = 0x100, // Cycles that the divider is active
		ARITH__MASK__IVB_ARITH__FPU_DIV = 0x400 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles the divider is activated
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS__MASK__IVB_BACLEARS__ANY = 0x1f00, // Counts the number of times the front end is re-steered
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__NONTAKEN_COND = 0x4100, // All macro conditional non-taken branch instructions
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__TAKEN_COND = 0x8100, // All macro conditional taken branch instructions
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__TAKEN_DIRECT_JUMP = 0x8200, // All macro unconditional taken branch instructions
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken indirect branches that are not calls nor returns
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__TAKEN_NEAR_RETURN = 0x8800, // All taken indirect branches that have a return mnemonic
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__TAKEN_DIRECT_NEAR_CALL = 0x9000, // All taken non-indirect calls
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken indirect calls
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ALL_BRANCHES = 0xff00, // All near executed branches instructions (not necessarily retired)
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ALL_COND = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ANY_COND = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All indirect branches that are not calls nor returns
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ANY_DIRECT_NEAR_CALL = 0xd000, // All non-indirect calls
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ANY_DIRECT_JUMP = 0xc200, // All direct jumps
		BR_INST_EXEC__MASK__IVB_BR_INST_EXEC__ANY_INDIRECT_NEAR_RET = 0xc800, // All indirect near returns
		BR_INST_RETIRED = 0xc4, // Retired branch instructions
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__ALL_BRANCHES = 0x0000, // All taken and not taken macro branches including far branches (Precise Event)
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__COND = 0x100, // All taken and not taken macro conditional branch instructions (Precise Event)
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__FAR_BRANCH = 0x4000, // Number of far branch instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__NEAR_CALL = 0x200, // All macro direct and indirect near calls
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__NEAR_RETURN = 0x800, // Number of near ret instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__NEAR_TAKEN = 0x2000, // Number of near branch taken instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__IVB_BR_INST_RETIRED__NOT_TAKEN = 0x1000, // All not taken macro branch instructions retired (Precise Event)
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__NONTAKEN_COND = 0x4100, // All non-taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__TAKEN_COND = 0x8100, // All taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__TAKEN_NEAR_RETURN = 0x8800, // All taken mispredicted indirect branches that have a return mnemonic
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__TAKEN_RETURN_NEAR = 0x8800, // All taken mispredicted indirect branches that have a return mnemonic
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken mispredicted indirect calls
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__ANY_COND = 0xc100, // All mispredicted macro conditional branch instructions
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC__MASK__IVB_BR_MISP_EXEC__ALL_BRANCHES = 0xff00, // All mispredicted branch instructions
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches
		BR_MISP_RETIRED__MASK__IVB_BR_MISP_RETIRED__ALL_BRANCHES = 0x0000, // All mispredicted macro branches (Precise Event)
		BR_MISP_RETIRED__MASK__IVB_BR_MISP_RETIRED__COND = 0x100, // All mispredicted macro conditional branch instructions (Precise Event)
		BR_MISP_RETIRED__MASK__IVB_BR_MISP_RETIRED__CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions (Precise Event)
		BR_MISP_RETIRED__MASK__IVB_BR_MISP_RETIRED__NEAR_TAKEN = 0x2000, // Number of branch instructions retired that were mispredicted and taken (Precise Event)
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES__MASK__IVB_LOCK_CYCLES__SPLIT_LOCK_UC_LOCK_DURATION = 0x100, // Cycles in which the L1D and L2 are locked
		LOCK_CYCLES__MASK__IVB_LOCK_CYCLES__CACHE_LOCK_DURATION = 0x200, // Cycles in which the L1D is locked
		CPL_CYCLES = 0x5c, // Unhalted core cycles at a specific ring level
		CPL_CYCLES__MASK__IVB_CPL_CYCLES__RING0 = 0x100, // Unhalted core cycles the thread was in ring 0
		CPL_CYCLES__MASK__IVB_CPL_CYCLES__RING0_TRANS = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Transitions from rings 1
		CPL_CYCLES__MASK__IVB_CPL_CYCLES__RING123 = 0x200, // Unhalted core cycles the thread was in rings 1
		CPU_CLK_UNHALTED = 0x3c, // Cycles when processor is not in halted state
		CPU_CLK_UNHALTED__MASK__IVB_CPU_CLK_UNHALTED__REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_UNHALTED__MASK__IVB_CPU_CLK_UNHALTED__REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_UNHALTED__MASK__IVB_CPU_CLK_UNHALTED__REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses (100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_UNHALTED__MASK__IVB_CPU_CLK_UNHALTED__THREAD_P = 0x0, // Cycles when thread is not halted
		CPU_CLK_UNHALTED__MASK__IVB_CPU_CLK_UNHALTED__ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES__MASK__IVB_DSB2MITE_SWITCHES__COUNT = 0x0100, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES__MASK__IVB_DSB2MITE_SWITCHES__PENALTY_CYCLES = 0x0200, // Number of DSB to MITE switch true penalty cycles
		DSB_FILL = 0xac, // DSB fills
		DSB_FILL__MASK__IVB_DSB_FILL__EXCEED_DSB_LINES = 0x800, // DSB Fill encountered > 3 DSB lines
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__MISS_CAUSES_A_WALK = 0x8100, // Demand load miss in all TLB levels which causes a page walk of any page size
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__WALK_COMPLETED = 0x8200, // Demand load miss in all TLB levels which causes a page walk that completes for any page size
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__WALK_DURATION = 0x8400, // Cycles PMH is busy with a walk due to demand loads
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__DEMAND_LD_MISS_CAUSES_A_WALK = 0x8100, // Demand load miss in all TLB levels which causes a page walk of any page size
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__DEMAND_LD_WALK_COMPLETED = 0x8200, // Demand load miss in all TLB levels which causes a page walk that completes for any page size
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__DEMAND_LD_WALK_DURATION = 0x8400, // Cycles PMH is busy with a walk due to demand loads
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__STLB_HIT = 0x45f, // Number of load operations that missed L1TLB but hit L2TLB
		DTLB_LOAD_MISSES__MASK__IVB_DTLB_LOAD_MISSES__LARGE_WALK_COMPLETED = 0x8800, // Number of large page walks completed for demand loads
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES__MASK__IVB_DTLB_STORE_MISSES__MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES__MASK__IVB_DTLB_STORE_MISSES__CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES__MASK__IVB_DTLB_STORE_MISSES__STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		DTLB_STORE_MISSES__MASK__IVB_DTLB_STORE_MISSES__WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES__MASK__IVB_DTLB_STORE_MISSES__WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		FP_ASSIST = 0xca, // X87 Floating point assists
		FP_ASSIST__MASK__IVB_FP_ASSIST__ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SSE or FP assists
		FP_ASSIST__MASK__IVB_FP_ASSIST__SIMD_INPUT = 0x1000, // Number of SIMD FP assists due to input values
		FP_ASSIST__MASK__IVB_FP_ASSIST__SIMD_OUTPUT = 0x800, // Number of SIMD FP assists due to output values
		FP_ASSIST__MASK__IVB_FP_ASSIST__X87_INPUT = 0x400, // Number of X87 assists due to input value
		FP_ASSIST__MASK__IVB_FP_ASSIST__X87_OUTPUT = 0x200, // Number of X87 assists due to output value
		ICACHE = 0x80, // Instruction Cache accesses
		ICACHE__MASK__IVB_ICACHE__MISSES = 0x200, // Number of Instruction Cache
		ICACHE__MASK__IVB_ICACHE__IFETCH_STALL = 0x400, // Number of cycles wher a code-fetch stalled due to L1 instruction cache miss or iTLB miss
		ICACHE__MASK__IVB_ICACHE__HIT = 0x100, // Number of Instruction Cache
		IDQ = 0x79, // IDQ operations
		IDQ__MASK__IVB_IDQ__EMPTY = 0x200, // Cycles IDQ is empty
		IDQ__MASK__IVB_IDQ__MITE_UOPS = 0x400, // Number of uops delivered to IDQ from MITE path
		IDQ__MASK__IVB_IDQ__DSB_UOPS = 0x800, // Number of uops delivered to IDQ from DSB path
		IDQ__MASK__IVB_IDQ__MS_DSB_UOPS = 0x1000, // Number of uops delivered to IDQ when MS busy by DSB
		IDQ__MASK__IVB_IDQ__MS_MITE_UOPS = 0x2000, // Number of uops delivered to IDQ when MS busy by MITE
		IDQ__MASK__IVB_IDQ__MS_UOPS = 0x3000, // Number of uops were delivered to IDQ from MS by either DSB or MITE
		IDQ__MASK__IVB_IDQ__MITE_UOPS_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from MITE (MITE active)
		IDQ__MASK__IVB_IDQ__DSB_UOPS_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from DSB (DSB active)
		IDQ__MASK__IVB_IDQ__MS_DSB_UOPS_CYCLES = 0x1000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by DSB
		IDQ__MASK__IVB_IDQ__MS_MITE_UOPS_CYCLES = 0x2000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by MITE
		IDQ__MASK__IVB_IDQ__MS_UOPS_CYCLES = 0x3000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ from MS by either BSD or MITE
		IDQ__MASK__IVB_IDQ__MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ__MASK__IVB_IDQ__ALL_DSB_UOPS = 0x1800, // Number of uops delivered from either DSB paths
		IDQ__MASK__IVB_IDQ__ALL_DSB_CYCLES = 0x1800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles MITE/MS delivered anything
		IDQ__MASK__IVB_IDQ__ALL_DSB_CYCLES_4_UOPS = 0x1800 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles MITE/MS delivered 4 uops
		IDQ__MASK__IVB_IDQ__ALL_MITE_UOPS = 0x2400, // Number of uops delivered from either MITE paths
		IDQ__MASK__IVB_IDQ__ALL_MITE_CYCLES = 0x2400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles DSB/MS delivered anything
		IDQ__MASK__IVB_IDQ__ALL_MITE_CYCLES_4_UOPS = 0x2400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles MITE is  delivering 4 uops
		IDQ__MASK__IVB_IDQ__ANY_UOPS = 0x3c00, // Number of uops delivered to IDQ from any path
		IDQ__MASK__IVB_IDQ__MS_DSB_UOPS_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of DSB MS going active
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED__MASK__IVB_IDQ_UOPS_NOT_DELIVERED__CORE = 0x100, // Number of non-delivered uops to RAT (use cmask to qualify further)
		IDQ_UOPS_NOT_DELIVERED__MASK__IVB_IDQ_UOPS_NOT_DELIVERED__CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles per thread when 4 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__IVB_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_1_UOP_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles per thread when 3 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__IVB_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_2_UOP_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles with less than 2 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__IVB_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_3_UOP_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with less than 3 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__IVB_IDQ_UOPS_NOT_DELIVERED__CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles Front-End (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling FE
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL__MASK__IVB_ILD_STALL__LCP = 0x100, // Stall caused by changing prefix length of the instruction
		ILD_STALL__MASK__IVB_ILD_STALL__IQ_FULL = 0x400, // Stall cycles due to IQ full
		INST_RETIRED = 0xc0, // Instructions retired
		INST_RETIRED__MASK__IVB_INST_RETIRED__ANY_P = 0x0, // Number of instructions retired
		INST_RETIRED__MASK__IVB_INST_RETIRED__ALL = 0x100, // Precise instruction retired event to reduce effect of PEBS shadow IP distribution (Precise Event)
		INST_RETIRED__MASK__IVB_INST_RETIRED__PREC_DIST = 0x100, // Precise instruction retired event to reduce effect of PEBS shadow IP distribution (Precise Event)
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		ITLB = 0xae, // Instruction TLB
		ITLB__MASK__IVB_ITLB__ITLB_FLUSH = 0x100, // Number of ITLB flushes
		ITLB__MASK__IVB_ITLB__FLUSH = 0x100, // Number of ITLB flushes
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES__MASK__IVB_ITLB_MISSES__MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES__MASK__IVB_ITLB_MISSES__CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES__MASK__IVB_ITLB_MISSES__STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		ITLB_MISSES__MASK__IVB_ITLB_MISSES__WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		ITLB_MISSES__MASK__IVB_ITLB_MISSES__WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		ITLB_MISSES__MASK__IVB_ITLB_MISSES__LARGE_PAGE_WALK_COMPLETED = 0x8000, // Number of completed page walks in ITLB due to STLB load misses for large pages
		L1D = 0x51, // L1D cache
		L1D__MASK__IVB_L1D__REPLACEMENT = 0x100, // Number of cache lines brought into the L1D cache
		MOVE_ELIMINATION = 0x58, // Move Elimination
		MOVE_ELIMINATION__MASK__IVB_MOVE_ELIMINATION__INT_NOT_ELIMINATED = 0x100, // Number of integer Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION__MASK__IVB_MOVE_ELIMINATION__SIMD_NOT_ELIMINATED = 0x800, // Number of SIMD Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION__MASK__IVB_MOVE_ELIMINATION__INT_ELIMINATED = 0x400, // Number of integer Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION__MASK__IVB_MOVE_ELIMINATION__SIMD_ELIMINATED = 0x200, // Number of SIMD Move Elimination candidate uops that were eliminated
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS__MASK__IVB_L1D_PEND_MISS__OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS__MASK__IVB_L1D_PEND_MISS__EDGE = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS__MASK__IVB_L1D_PEND_MISS__PENDING = 0x100, // Number of L1D load misses outstanding every cycle
		L1D_PEND_MISS__MASK__IVB_L1D_PEND_MISS__PENDING_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load misses outstanding
		L1D_PEND_MISS__MASK__IVB_L1D_PEND_MISS__PENDING_CYCLES_ANY = 0x100 | (0x1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles with L1D load misses outstanding from any thread on the physical core
		L1D_PEND_MISS__MASK__IVB_L1D_PEND_MISS__FB_FULL = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles a demand request was blocked due to Fill Buffer (FB) unavailability
		L2_L1D_WB_RQSTS = 0x28, // Writeback requests from L1D to L2
		L2_L1D_WB_RQSTS__MASK__IVB_L2_L1D_WB_RQSTS__HIT_E = 0x400, // Non rejected writebacks from L1D to L2 cache lines in E state
		L2_L1D_WB_RQSTS__MASK__IVB_L2_L1D_WB_RQSTS__HIT_M = 0x800, // Non rejected writebacks from L1D to L2 cache lines in M state
		L2_L1D_WB_RQSTS__MASK__IVB_L2_L1D_WB_RQSTS__MISS = 0x100, // Not rejected writebacks that missed LLC
		L2_L1D_WB_RQSTS__MASK__IVB_L2_L1D_WB_RQSTS__ALL = 0xf00, // Not rejected writebacks from L1D to L2 cache lines in any state
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN__MASK__IVB_L2_LINES_IN__ANY = 0x700, // L2 cache lines filling (counting does not cover rejects)
		L2_LINES_IN__MASK__IVB_L2_LINES_IN__ALL = 0x700, // L2 cache lines filling (counting does not cover rejects)
		L2_LINES_IN__MASK__IVB_L2_LINES_IN__E = 0x400, // L2 cache lines in E state (counting does not cover rejects)
		L2_LINES_IN__MASK__IVB_L2_LINES_IN__I = 0x100, // L2 cache lines in I state (counting does not cover rejects)
		L2_LINES_IN__MASK__IVB_L2_LINES_IN__S = 0x200, // L2 cache lines in S state (counting does not cover rejects)
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__DEMAND_CLEAN = 0x100, // L2 clean line evicted by a demand
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__DEMAND_DIRTY = 0x200, // L2 dirty line evicted by a demand
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__PREFETCH_CLEAN = 0x400, // L2 clean line evicted by a prefetch
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__PF_CLEAN = 0x400, // L2 clean line evicted by a prefetch
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__PREFETCH_DIRTY = 0x800, // L2 dirty line evicted by an MLC Prefetch
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__PF_DIRTY = 0x800, // L2 dirty line evicted by an MLC Prefetch
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__DIRTY_ANY = 0xa00, // Any L2 dirty line evicted (does not cover rejects)
		L2_LINES_OUT__MASK__IVB_L2_LINES_OUT__DIRTY_ALL = 0xa00, // Any L2 dirty line evicted (does not cover rejects)
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS__MASK__IVB_L2_RQSTS__ALL_CODE_RD = 0x3000, // Any code request to L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__CODE_RD_HIT = 0x1000, // L2 cache hits when fetching instructions
		L2_RQSTS__MASK__IVB_L2_RQSTS__CODE_RD_MISS = 0x2000, // L2 cache misses when fetching instructions
		L2_RQSTS__MASK__IVB_L2_RQSTS__ALL_DEMAND_DATA_RD = 0x300, // Demand  data read requests to L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__DEMAND_DATA_RD_HIT = 0x100, // Demand data read requests that hit L2
		L2_RQSTS__MASK__IVB_L2_RQSTS__ALL_PF = 0xc000, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__PF_HIT = 0x4000, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__PF_MISS = 0x8000, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__ALL_RFO = 0xc00, // Any RFO requests to L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__RFO_HIT = 0x400, // Store RFO requests that hit L2 cache
		L2_RQSTS__MASK__IVB_L2_RQSTS__RFO_MISS = 0x800, // RFO requests that miss L2 cache
		L2_STORE_LOCK_RQSTS = 0x27, // L2 store lock requests
		L2_STORE_LOCK_RQSTS__MASK__IVB_L2_STORE_LOCK_RQSTS__MISS = 0x100, // RFOs that miss cache (I state)
		L2_STORE_LOCK_RQSTS__MASK__IVB_L2_STORE_LOCK_RQSTS__HIT_M = 0x800, // RFOs that hit cache lines in M state
		L2_STORE_LOCK_RQSTS__MASK__IVB_L2_STORE_LOCK_RQSTS__ALL = 0xf00, // RFOs that access cache lines in any state
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS__MASK__IVB_L2_TRANS__ALL = 0x8000, // Transactions accessing the L2 pipe
		L2_TRANS__MASK__IVB_L2_TRANS__CODE_RD = 0x400, // L2 cache accesses when fetching instructions
		L2_TRANS__MASK__IVB_L2_TRANS__L1D_WB = 0x1000, // L1D writebacks that access the L2 cache
		L2_TRANS__MASK__IVB_L2_TRANS__DMND_DATA_RD = 0x100, // Demand Data Read requests that access the L2 cache
		L2_TRANS__MASK__IVB_L2_TRANS__L2_FILL = 0x2000, // L2 fill requests that access the L2 cache
		L2_TRANS__MASK__IVB_L2_TRANS__L2_WB = 0x4000, // L2 writebacks that access the L2 cache
		L2_TRANS__MASK__IVB_L2_TRANS__ALL_PREFETCH = 0x800, // L2 or L3 HW prefetches that access the L2 cache (including rejects)
		L2_TRANS__MASK__IVB_L2_TRANS__ALL_PF = 0x800, // L2 or L3 HW prefetches that access the L2 cache (including rejects)
		L2_TRANS__MASK__IVB_L2_TRANS__RFO = 0x200, // RFO requests that access the L2 cache
		LAST_LEVEL_CACHE_MISSES = 0x412e, // This is an alias for L3_LAT_CACHE:MISS
		LLC_MISSES = 0x412e, // Alias for LAST_LEVEL_CACHE_MISSES
		LAST_LEVEL_CACHE_REFERENCES = 0x4f2e, // This is an alias for L3_LAT_CACHE:REFERENCE
		LLC_REFERENCES = 0x4f2e, // Alias for LAST_LEVEL_CACHE_REFERENCES
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS__MASK__IVB_LD_BLOCKS__STORE_FORWARD = 0x200, // Loads blocked by overlapping with store buffer that cannot be forwarded
		LD_BLOCKS__MASK__IVB_LD_BLOCKS__NO_SR = 0x800, // Number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL__MASK__IVB_LD_BLOCKS_PARTIAL__ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LOAD_HIT_PRE = 0x4c, // Load dispatches that hit fill buffer
		LOAD_HIT_PRE__MASK__IVB_LOAD_HIT_PRE__HW_PF = 0x200, // Non sw-prefetch load dispatches that hit the fill buffer allocated for HW prefetch
		LOAD_HIT_PRE__MASK__IVB_LOAD_HIT_PRE__SW_PF = 0x100, // Non sw-prefetch load dispatches that hit the fill buffer allocated for SW prefetch
		L3_LAT_CACHE = 0x2e, // Core-originated cacheable demand requests to L3
		L3_LAT_CACHE__MASK__IVB_L3_LAT_CACHE__MISS = 0x4100, // Core-originated cacheable demand requests missed L3
		L3_LAT_CACHE__MASK__IVB_L3_LAT_CACHE__REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to L3
		LONGEST_LAT_CACHE = 0x2e, // Core-originated cacheable demand requests to L3
		LONGEST_LAT_CACHE__MASK__IVB_L3_LAT_CACHE__MISS = 0x4100, // Core-originated cacheable demand requests missed L3
		LONGEST_LAT_CACHE__MASK__IVB_L3_LAT_CACHE__REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to L3
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS__MASK__IVB_MACHINE_CLEARS__MASKMOV = 0x2000, // The number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0
		MACHINE_CLEARS__MASK__IVB_MACHINE_CLEARS__MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS__MASK__IVB_MACHINE_CLEARS__SMC = 0x400, // Self-Modifying Code detected
		MACHINE_CLEARS__MASK__IVB_MACHINE_CLEARS__COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of machine clears (nukes) of any type
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired (deprecated use MEM_LOAD_UOPS_LLC_HIT_RETIRED)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // Load uops retired that missed the LLC
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_MISS_RETIRED__LOCAL_DRAM = 0x100, // Number of retired load uops that missed L3 but were service by local RAM. Does not count hardware prefetches (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_MISS_RETIRED__REMOTE_DRAM = 0xc00, // Number of retired load uops that missed L3 but were service by remote RAM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_MISS_RETIRED__REMOTE_HITM = 0x1000, // Number of retired load uops whose data sources was remote HITM (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_LLC_MISS_RETIRED__REMOTE_FWD = 0x2000, // Load uops that miss in the L3 whose data source was forwarded from a remote cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Memory loads uops retired
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L1_MISS = 0x800, // Load miss in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L2_MISS = 0x1000, // Load misses in mid-level (L2) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L3_MISS = 0x2000, // Load miss in last-level (L3) cache (Precise Event)
		MEM_LOAD_RETIRED = 0xd1, // Memory loads uops retired (deprecated use MEM_LOAD_UOPS_RETIRED)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L1_MISS = 0x800, // Load miss in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L2_MISS = 0x1000, // Load misses in mid-level (L2) cache (Precise Event)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_RETIRED__MASK__IVB_MEM_LOAD_UOPS_RETIRED__L3_MISS = 0x2000, // Load miss in last-level (L3) cache (Precise Event)
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired
		MEM_TRANS_RETIRED__MASK__IVB_MEM_TRANS_RETIRED__LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED__MASK__IVB_MEM_TRANS_RETIRED__PRECISE_STORE = 0x200, // Capture where stores occur
		MEM_UOPS_RETIRED = 0xd0, // Memory uops retired
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MEM_UOP_RETIRED = 0xd0, // Memory uops retired (deprecated use MEM_UOPS_RETIRED)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__IVB_MEM_UOPS_RETIRED__STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF__MASK__IVB_MISALIGN_MEM_REF__LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF__MASK__IVB_MISALIGN_MEM_REF__STORES = 0x200, // Speculative cache-line split Store-address uops dispatched to L1D
		OFFCORE_REQUESTS = 0xb0, // Offcore requests
		OFFCORE_REQUESTS__MASK__IVB_OFFCORE_REQUESTS__ALL_DATA_RD = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS__MASK__IVB_OFFCORE_REQUESTS__ALL_DATA_READ = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS__MASK__IVB_OFFCORE_REQUESTS__DEMAND_CODE_RD = 0x200, // Offcore code read requests
		OFFCORE_REQUESTS__MASK__IVB_OFFCORE_REQUESTS__DEMAND_DATA_RD = 0x100, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS__MASK__IVB_OFFCORE_REQUESTS__DEMAND_RFO = 0x400, // Offcore Demand RFOs
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__IVB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ
		OTHER_ASSISTS = 0xc1, // Count hardware assists
		OTHER_ASSISTS__MASK__IVB_OTHER_ASSISTS__AVX_TO_SSE = 0x1000, // Number of transitions from AVX-256 to legacy SSE when penalty applicable
		OTHER_ASSISTS__MASK__IVB_OTHER_ASSISTS__SSE_TO_AVX = 0x2000, // Number of transitions from legacy SSE to AVX-256 when penalty applicable
		OTHER_ASSISTS__MASK__IVB_OTHER_ASSISTS__AVX_STORE = 0x0800, // Number of assists associated with 256-bit AVX stores
		OTHER_ASSISTS__MASK__IVB_OTHER_ASSISTS__WB = 0x8000, // Number of times the microcode assist is invoked by hardware upon uop writeback
		RESOURCE_STALLS = 0xa2, // Resource related stall cycles
		RESOURCE_STALLS__MASK__IVB_RESOURCE_STALLS__ANY = 0x100, // Cycles stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__IVB_RESOURCE_STALLS__RS = 0x400, // Cycles stalled due to no eligible RS entry available
		RESOURCE_STALLS__MASK__IVB_RESOURCE_STALLS__SB = 0x800, // Cycles stalled due to no store buffers available (not including draining from sync)
		RESOURCE_STALLS__MASK__IVB_RESOURCE_STALLS__ROB = 0x1000, // Cycles stalled due to re-order buffer full
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss loads
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__CYCLES_LDM_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending memory loads
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__CYCLES_L1D_PENDING = 0x0800 | (0x8 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__CYCLES_NO_EXECUTE = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles of dispatch stalls
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__STALLS_L2_PENDING = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls due to L2 pending loads
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__STALLS_L1D_PENDING = 0x0c00 | (0xc << INTEL_X86_CMASK_BIT), // Execution stalls due to L1D pending loads
		CYCLE_ACTIVITY__MASK__IVB_CYCLE_ACTIVITY__STALLS_LDM_PENDING = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls due to memory loads
		ROB_MISC_EVENTS = 0xcc, // Reorder buffer events
		ROB_MISC_EVENTS__MASK__IVB_ROB_MISC_EVENTS__LBR_INSERTS = 0x2000, // Count each time an new LBR record is saved by HW
		RS_EVENTS = 0x5e, // Reservation station events
		RS_EVENTS__MASK__IVB_RS_EVENTS__EMPTY_CYCLES = 0x100, // Cycles the RS is empty for this thread
		RS_EVENTS__MASK__IVB_RS_EVENTS__EMPTY_END = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Counts number of time the Reservation Station (RS) goes from empty to non-empty
		DTLB_LOAD_ACCESS = 0x5f, // TLB access
		DTLB_LOAD_ACCESS__MASK__IVB_TLB_ACCESS__STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		DTLB_LOAD_ACCESS__MASK__IVB_TLB_ACCESS__LOAD_STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		TLB_ACCESS = 0x5f, // TLB access
		TLB_ACCESS__MASK__IVB_TLB_ACCESS__STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		TLB_ACCESS__MASK__IVB_TLB_ACCESS__LOAD_STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH__MASK__IVB_TLB_FLUSH__DTLB_THREAD = 0x100, // Number of DTLB flushes of thread-specific entries
		TLB_FLUSH__MASK__IVB_TLB_FLUSH__STLB_ANY = 0x2000, // Number of STLB flushes
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CORE = 0x200, // Counts total number of uops executed from any thread per cycle
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__THREAD = 0x100, // Counts total number of uops executed per thread each cycle
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CYCLES_GE_1_UOP_EXEC = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CYCLES_GE_2_UOPS_EXEC = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CYCLES_GE_3_UOPS_EXEC = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CYCLES_GE_4_UOPS_EXEC = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CORE_CYCLES_GE_1 = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed from any thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CORE_CYCLES_GE_2 = 0x200 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed from any thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CORE_CYCLES_GE_3 = 0x200 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed from any thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CORE_CYCLES_GE_4 = 0x200 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed from any thread
		UOPS_EXECUTED__MASK__IVB_UOPS_EXECUTED__CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV, // Cycles where no uop is executed on any thread
		UOPS_DISPATCHED_PORT = 0xa1, // Uops dispatch to specific ports
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_0 = 0x100, // Cycles in which a uop is dispatched on port 0
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_1 = 0x200, // Cycles in which a uop is dispatched on port 1
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_2 = 0xc00, // Cycles in which a uop is dispatched on port 2
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_3 = 0x3000, // Cycles in which a uop is dispatched on port 3
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_4 = 0x4000, // Cycles in which a uop is dispatched on port 4
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_5 = 0x8000, // Cycles in which a uop is dispatched on port 5
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 0 for any thread
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 1 for any thread
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_2_CORE = 0xc00 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 2 for any thread
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_3_CORE = 0x3000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 3 for any thread
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_4_CORE = 0x4000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 4 for any thread
		UOPS_DISPATCHED_PORT__MASK__IVB_UOPS_DISPATCHED_PORT__PORT_5_CORE = 0x8000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 5 for any thread
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED__MASK__IVB_UOPS_ISSUED__ANY = 0x100, // Number of uops issued by the RAT to the Reservation Station (RS)
		UOPS_ISSUED__MASK__IVB_UOPS_ISSUED__CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued on this core (by any thread)
		UOPS_ISSUED__MASK__IVB_UOPS_ISSUED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued by this thread
		UOPS_ISSUED__MASK__IVB_UOPS_ISSUED__FLAGS_MERGE = 0x1000, // Number of flags-merge uops allocated. Such uops adds delay
		UOPS_ISSUED__MASK__IVB_UOPS_ISSUED__SLOW_LEA = 0x2000, // Number of slow LEA or similar uops allocated
		UOPS_ISSUED__MASK__IVB_UOPS_ISSUED__SINGLE_MUL = 0x4000, // Number of multiply packed/scalar single precision uops allocated
		UOPS_RETIRED = 0xc2, // Uops retired
		UOPS_RETIRED__MASK__IVB_UOPS_RETIRED__ALL = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED__MASK__IVB_UOPS_RETIRED__ANY = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED__MASK__IVB_UOPS_RETIRED__RETIRE_SLOTS = 0x200, // Number of retirement slots used (Precise Event)
		UOPS_RETIRED__MASK__IVB_UOPS_RETIRED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no executable uop retired (Precise Event)
		UOPS_RETIRED__MASK__IVB_UOPS_RETIRED__TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Total cycles using precise uop retired event (Precise Event)
		FP_COMP_OPS_EXE = 0x10, // Counts number of floating point events
		FP_COMP_OPS_EXE__MASK__IVB_FP_COMP_OPS_EXE__X87 = 0x100, // Number of X87 uops executed
		FP_COMP_OPS_EXE__MASK__IVB_FP_COMP_OPS_EXE__SSE_FP_PACKED_DOUBLE = 0x1000, // Number of SSE or AVX-128 double precision FP packed uops executed
		FP_COMP_OPS_EXE__MASK__IVB_FP_COMP_OPS_EXE__SSE_FP_SCALAR_SINGLE = 0x2000, // Number of SSE or AVX-128 single precision FP scalar uops executed
		FP_COMP_OPS_EXE__MASK__IVB_FP_COMP_OPS_EXE__SSE_PACKED_SINGLE = 0x4000, // Number of SSE or AVX-128 single precision FP packed uops executed
		FP_COMP_OPS_EXE__MASK__IVB_FP_COMP_OPS_EXE__SSE_SCALAR_DOUBLE = 0x8000, // Number of SSE or AVX-128 double precision FP scalar uops executed
		SIMD_FP_256 = 0x11, // Counts 256-bit packed floating point instructions
		SIMD_FP_256__MASK__IVB_SIMD_FP_256__PACKED_SINGLE = 0x100, // Counts 256-bit packed single-precision
		SIMD_FP_256__MASK__IVB_SIMD_FP_256__PACKED_DOUBLE = 0x200, // Counts 256-bit packed double-precision
		LSD = 0xa8, // Loop stream detector
		LSD__MASK__IVB_LSD__UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD__MASK__IVB_LSD__ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with uops delivered by the LSD but which did not come from decoder
		LSD__MASK__IVB_LSD__CYCLES_4_UOPS = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles with 4 uops delivered by the LSD but which did not come from decoder
		EPT = 0x4f, // Extended page table
		EPT__MASK__IVB_EPT__WALK_CYCLES = 0x1000, // Cycles for an extended page table walk
		PAGE_WALKS = 0xbe, // page walker
		PAGE_WALKS__MASK__IVB_PAGE_WALKS__LLC_MISS = 0x100, // Number of page walks with a LLC miss
		INT_MISC = 0xd, // Miscellaneous interruptions
		INT_MISC__MASK__IVB_INT_MISC__RECOVERY_CYCLES = 0x300 | (1 << INTEL_X86_CMASK_BIT), // Cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception
		INT_MISC__MASK__IVB_INT_MISC__RECOVERY_CYCLES_ANY = 0x300 | (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)
		INT_MISC__MASK__IVB_INT_MISC__RECOVERY_STALLS_COUNT = 0x300 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of occurrences waiting for Machine Clears
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore reqest buffer
		OFFCORE_REQUESTS_BUFFER__MASK__IVB_OFFCORE_REQUESTS_BUFFER__SQ_FULL = 0x0100, // Number of cycles the offcore requests buffer is full
		SQ_MISC = 0xf4, // SuperQueue miscellaneous
		SQ_MISC__MASK__IVB_SQ_MISC__SPLIT_LOCK = 0x1000, // Number of split locks in the super queue (SQ)
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0__MASK__IVB_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1__MASK__IVB_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};

namespace ivb = optkit::intel::ivb;