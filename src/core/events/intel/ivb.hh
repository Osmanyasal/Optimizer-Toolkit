#include <cstdint>

namespace optkit::intel{
	enum class ivb : uint64_t {
		ARITH = 0x14, // Counts arithmetic multiply operations
		ARITH_MASK_FPU_DIV_ACTIVE = 0x100, // Cycles that the divider is active
		ARITH_MASK_FPU_DIV = 0x400 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles the divider is activated
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS_MASK_ANY = 0x1f00, // Counts the number of times the front end is re-steered
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC_MASK_NONTAKEN_COND = 0x4100, // All macro conditional non-taken branch instructions
		BR_INST_EXEC_MASK_TAKEN_COND = 0x8100, // All macro conditional taken branch instructions
		BR_INST_EXEC_MASK_TAKEN_DIRECT_JUMP = 0x8200, // All macro unconditional taken branch instructions
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken indirect branches that are not calls nor returns
		BR_INST_EXEC_MASK_TAKEN_NEAR_RETURN = 0x8800, // All taken indirect branches that have a return mnemonic
		BR_INST_EXEC_MASK_TAKEN_DIRECT_NEAR_CALL = 0x9000, // All taken non-indirect calls
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken indirect calls
		BR_INST_EXEC_MASK_ALL_BRANCHES = 0xff00, // All near executed branches instructions (not necessarily retired)
		BR_INST_EXEC_MASK_ALL_COND = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC_MASK_ANY_COND = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC_MASK_ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All indirect branches that are not calls nor returns
		BR_INST_EXEC_MASK_ANY_DIRECT_NEAR_CALL = 0xd000, // All non-indirect calls
		BR_INST_EXEC_MASK_ANY_DIRECT_JUMP = 0xc200, // All direct jumps
		BR_INST_EXEC_MASK_ANY_INDIRECT_NEAR_RET = 0xc800, // All indirect near returns
		BR_INST_RETIRED = 0xc4, // Retired branch instructions
		BR_INST_RETIRED_MASK_ALL_BRANCHES = 0x0000, // All taken and not taken macro branches including far branches (Precise Event)
		BR_INST_RETIRED_MASK_COND = 0x100, // All taken and not taken macro conditional branch instructions (Precise Event)
		BR_INST_RETIRED_MASK_FAR_BRANCH = 0x4000, // Number of far branch instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_NEAR_CALL = 0x200, // All macro direct and indirect near calls
		BR_INST_RETIRED_MASK_NEAR_RETURN = 0x800, // Number of near ret instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_NEAR_TAKEN = 0x2000, // Number of near branch taken instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_NOT_TAKEN = 0x1000, // All not taken macro branch instructions retired (Precise Event)
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC_MASK_NONTAKEN_COND = 0x4100, // All non-taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC_MASK_TAKEN_COND = 0x8100, // All taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC_MASK_TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC_MASK_TAKEN_NEAR_RETURN = 0x8800, // All taken mispredicted indirect branches that have a return mnemonic
		BR_MISP_EXEC_MASK_TAKEN_RETURN_NEAR = 0x8800, // All taken mispredicted indirect branches that have a return mnemonic
		BR_MISP_EXEC_MASK_TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken mispredicted indirect calls
		BR_MISP_EXEC_MASK_ANY_COND = 0xc100, // All mispredicted macro conditional branch instructions
		BR_MISP_EXEC_MASK_ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC_MASK_ALL_BRANCHES = 0xff00, // All mispredicted branch instructions
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches
		BR_MISP_RETIRED_MASK_ALL_BRANCHES = 0x0000, // All mispredicted macro branches (Precise Event)
		BR_MISP_RETIRED_MASK_COND = 0x100, // All mispredicted macro conditional branch instructions (Precise Event)
		BR_MISP_RETIRED_MASK_CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions (Precise Event)
		BR_MISP_RETIRED_MASK_NEAR_TAKEN = 0x2000, // Number of branch instructions retired that were mispredicted and taken (Precise Event)
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES_MASK_SPLIT_LOCK_UC_LOCK_DURATION = 0x100, // Cycles in which the L1D and L2 are locked
		LOCK_CYCLES_MASK_CACHE_LOCK_DURATION = 0x200, // Cycles in which the L1D is locked
		CPL_CYCLES = 0x5c, // Unhalted core cycles at a specific ring level
		CPL_CYCLES_MASK_RING0 = 0x100, // Unhalted core cycles the thread was in ring 0
		CPL_CYCLES_MASK_RING0_TRANS = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Transitions from rings 1
		CPL_CYCLES_MASK_RING123 = 0x200, // Unhalted core cycles the thread was in rings 1
		CPU_CLK_UNHALTED = 0x3c, // Cycles when processor is not in halted state
		CPU_CLK_UNHALTED_MASK_REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_UNHALTED_MASK_REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_UNHALTED_MASK_REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses (100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_UNHALTED_MASK_THREAD_P = 0x0, // Cycles when thread is not halted
		CPU_CLK_UNHALTED_MASK_ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES_MASK_COUNT = 0x0100, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES_MASK_PENALTY_CYCLES = 0x0200, // Number of DSB to MITE switch true penalty cycles
		DSB_FILL = 0xac, // DSB fills
		DSB_FILL_MASK_EXCEED_DSB_LINES = 0x800, // DSB Fill encountered > 3 DSB lines
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES_MASK_MISS_CAUSES_A_WALK = 0x8100, // Demand load miss in all TLB levels which causes a page walk of any page size
		DTLB_LOAD_MISSES_MASK_WALK_COMPLETED = 0x8200, // Demand load miss in all TLB levels which causes a page walk that completes for any page size
		DTLB_LOAD_MISSES_MASK_WALK_DURATION = 0x8400, // Cycles PMH is busy with a walk due to demand loads
		DTLB_LOAD_MISSES_MASK_DEMAND_LD_MISS_CAUSES_A_WALK = 0x8100, // Demand load miss in all TLB levels which causes a page walk of any page size
		DTLB_LOAD_MISSES_MASK_DEMAND_LD_WALK_COMPLETED = 0x8200, // Demand load miss in all TLB levels which causes a page walk that completes for any page size
		DTLB_LOAD_MISSES_MASK_DEMAND_LD_WALK_DURATION = 0x8400, // Cycles PMH is busy with a walk due to demand loads
		DTLB_LOAD_MISSES_MASK_STLB_HIT = 0x45f, // Number of load operations that missed L1TLB but hit L2TLB
		DTLB_LOAD_MISSES_MASK_LARGE_WALK_COMPLETED = 0x8800, // Number of large page walks completed for demand loads
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES_MASK_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES_MASK_STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		DTLB_STORE_MISSES_MASK_WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES_MASK_WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		FP_ASSIST = 0xca, // X87 Floating point assists
		FP_ASSIST_MASK_ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SSE or FP assists
		FP_ASSIST_MASK_SIMD_INPUT = 0x1000, // Number of SIMD FP assists due to input values
		FP_ASSIST_MASK_SIMD_OUTPUT = 0x800, // Number of SIMD FP assists due to output values
		FP_ASSIST_MASK_X87_INPUT = 0x400, // Number of X87 assists due to input value
		FP_ASSIST_MASK_X87_OUTPUT = 0x200, // Number of X87 assists due to output value
		ICACHE = 0x80, // Instruction Cache accesses
		ICACHE_MASK_MISSES = 0x200, // Number of Instruction Cache
		ICACHE_MASK_IFETCH_STALL = 0x400, // Number of cycles wher a code-fetch stalled due to L1 instruction cache miss or iTLB miss
		ICACHE_MASK_HIT = 0x100, // Number of Instruction Cache
		IDQ = 0x79, // IDQ operations
		IDQ_MASK_EMPTY = 0x200, // Cycles IDQ is empty
		IDQ_MASK_MITE_UOPS = 0x400, // Number of uops delivered to IDQ from MITE path
		IDQ_MASK_DSB_UOPS = 0x800, // Number of uops delivered to IDQ from DSB path
		IDQ_MASK_MS_DSB_UOPS = 0x1000, // Number of uops delivered to IDQ when MS busy by DSB
		IDQ_MASK_MS_MITE_UOPS = 0x2000, // Number of uops delivered to IDQ when MS busy by MITE
		IDQ_MASK_MS_UOPS = 0x3000, // Number of uops were delivered to IDQ from MS by either DSB or MITE
		IDQ_MASK_MITE_UOPS_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from MITE (MITE active)
		IDQ_MASK_DSB_UOPS_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from DSB (DSB active)
		IDQ_MASK_MS_DSB_UOPS_CYCLES = 0x1000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by DSB
		IDQ_MASK_MS_MITE_UOPS_CYCLES = 0x2000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by MITE
		IDQ_MASK_MS_UOPS_CYCLES = 0x3000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ from MS by either BSD or MITE
		IDQ_MASK_MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ_MASK_ALL_DSB_UOPS = 0x1800, // Number of uops delivered from either DSB paths
		IDQ_MASK_ALL_DSB_CYCLES = 0x1800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles MITE/MS delivered anything
		IDQ_MASK_ALL_DSB_CYCLES_4_UOPS = 0x1800 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles MITE/MS delivered 4 uops
		IDQ_MASK_ALL_MITE_UOPS = 0x2400, // Number of uops delivered from either MITE paths
		IDQ_MASK_ALL_MITE_CYCLES = 0x2400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles DSB/MS delivered anything
		IDQ_MASK_ALL_MITE_CYCLES_4_UOPS = 0x2400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles MITE is  delivering 4 uops
		IDQ_MASK_ANY_UOPS = 0x3c00, // Number of uops delivered to IDQ from any path
		IDQ_MASK_MS_DSB_UOPS_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of DSB MS going active
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED_MASK_CORE = 0x100, // Number of non-delivered uops to RAT (use cmask to qualify further)
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles per thread when 4 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_1_UOP_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles per thread when 3 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_2_UOP_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles with less than 2 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_3_UOP_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with less than 3 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles Front-End (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling FE
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL_MASK_LCP = 0x100, // Stall caused by changing prefix length of the instruction
		ILD_STALL_MASK_IQ_FULL = 0x400, // Stall cycles due to IQ full
		INST_RETIRED = 0xc0, // Instructions retired
		INST_RETIRED_MASK_ANY_P = 0x0, // Number of instructions retired
		INST_RETIRED_MASK_ALL = 0x100, // Precise instruction retired event to reduce effect of PEBS shadow IP distribution (Precise Event)
		INST_RETIRED_MASK_PREC_DIST = 0x100, // Precise instruction retired event to reduce effect of PEBS shadow IP distribution (Precise Event)
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		ITLB = 0xae, // Instruction TLB
		ITLB_MASK_ITLB_FLUSH = 0x100, // Number of ITLB flushes
		ITLB_MASK_FLUSH = 0x100, // Number of ITLB flushes
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES_MASK_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES_MASK_STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		ITLB_MISSES_MASK_WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		ITLB_MISSES_MASK_WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		ITLB_MISSES_MASK_LARGE_PAGE_WALK_COMPLETED = 0x8000, // Number of completed page walks in ITLB due to STLB load misses for large pages
		L1D = 0x51, // L1D cache
		L1D_MASK_REPLACEMENT = 0x100, // Number of cache lines brought into the L1D cache
		MOVE_ELIMINATION = 0x58, // Move Elimination
		MOVE_ELIMINATION_MASK_INT_NOT_ELIMINATED = 0x100, // Number of integer Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION_MASK_SIMD_NOT_ELIMINATED = 0x800, // Number of SIMD Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION_MASK_INT_ELIMINATED = 0x400, // Number of integer Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION_MASK_SIMD_ELIMINATED = 0x200, // Number of SIMD Move Elimination candidate uops that were eliminated
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS_MASK_OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS_MASK_EDGE = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS_MASK_PENDING = 0x100, // Number of L1D load misses outstanding every cycle
		L1D_PEND_MISS_MASK_PENDING_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load misses outstanding
		L1D_PEND_MISS_MASK_PENDING_CYCLES_ANY = 0x100 | (0x1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles with L1D load misses outstanding from any thread on the physical core
		L1D_PEND_MISS_MASK_FB_FULL = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles a demand request was blocked due to Fill Buffer (FB) unavailability
		L2_L1D_WB_RQSTS = 0x28, // Writeback requests from L1D to L2
		L2_L1D_WB_RQSTS_MASK_HIT_E = 0x400, // Non rejected writebacks from L1D to L2 cache lines in E state
		L2_L1D_WB_RQSTS_MASK_HIT_M = 0x800, // Non rejected writebacks from L1D to L2 cache lines in M state
		L2_L1D_WB_RQSTS_MASK_MISS = 0x100, // Not rejected writebacks that missed LLC
		L2_L1D_WB_RQSTS_MASK_ALL = 0xf00, // Not rejected writebacks from L1D to L2 cache lines in any state
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN_MASK_ANY = 0x700, // L2 cache lines filling (counting does not cover rejects)
		L2_LINES_IN_MASK_ALL = 0x700, // L2 cache lines filling (counting does not cover rejects)
		L2_LINES_IN_MASK_E = 0x400, // L2 cache lines in E state (counting does not cover rejects)
		L2_LINES_IN_MASK_I = 0x100, // L2 cache lines in I state (counting does not cover rejects)
		L2_LINES_IN_MASK_S = 0x200, // L2 cache lines in S state (counting does not cover rejects)
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT_MASK_DEMAND_CLEAN = 0x100, // L2 clean line evicted by a demand
		L2_LINES_OUT_MASK_DEMAND_DIRTY = 0x200, // L2 dirty line evicted by a demand
		L2_LINES_OUT_MASK_PREFETCH_CLEAN = 0x400, // L2 clean line evicted by a prefetch
		L2_LINES_OUT_MASK_PF_CLEAN = 0x400, // L2 clean line evicted by a prefetch
		L2_LINES_OUT_MASK_PREFETCH_DIRTY = 0x800, // L2 dirty line evicted by an MLC Prefetch
		L2_LINES_OUT_MASK_PF_DIRTY = 0x800, // L2 dirty line evicted by an MLC Prefetch
		L2_LINES_OUT_MASK_DIRTY_ANY = 0xa00, // Any L2 dirty line evicted (does not cover rejects)
		L2_LINES_OUT_MASK_DIRTY_ALL = 0xa00, // Any L2 dirty line evicted (does not cover rejects)
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS_MASK_ALL_CODE_RD = 0x3000, // Any code request to L2 cache
		L2_RQSTS_MASK_CODE_RD_HIT = 0x1000, // L2 cache hits when fetching instructions
		L2_RQSTS_MASK_CODE_RD_MISS = 0x2000, // L2 cache misses when fetching instructions
		L2_RQSTS_MASK_ALL_DEMAND_DATA_RD = 0x300, // Demand  data read requests to L2 cache
		L2_RQSTS_MASK_DEMAND_DATA_RD_HIT = 0x100, // Demand data read requests that hit L2
		L2_RQSTS_MASK_ALL_PF = 0xc000, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS_MASK_PF_HIT = 0x4000, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS_MASK_PF_MISS = 0x8000, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS_MASK_ALL_RFO = 0xc00, // Any RFO requests to L2 cache
		L2_RQSTS_MASK_RFO_HIT = 0x400, // Store RFO requests that hit L2 cache
		L2_RQSTS_MASK_RFO_MISS = 0x800, // RFO requests that miss L2 cache
		L2_STORE_LOCK_RQSTS = 0x27, // L2 store lock requests
		L2_STORE_LOCK_RQSTS_MASK_MISS = 0x100, // RFOs that miss cache (I state)
		L2_STORE_LOCK_RQSTS_MASK_HIT_M = 0x800, // RFOs that hit cache lines in M state
		L2_STORE_LOCK_RQSTS_MASK_ALL = 0xf00, // RFOs that access cache lines in any state
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS_MASK_ALL = 0x8000, // Transactions accessing the L2 pipe
		L2_TRANS_MASK_CODE_RD = 0x400, // L2 cache accesses when fetching instructions
		L2_TRANS_MASK_L1D_WB = 0x1000, // L1D writebacks that access the L2 cache
		L2_TRANS_MASK_DMND_DATA_RD = 0x100, // Demand Data Read requests that access the L2 cache
		L2_TRANS_MASK_L2_FILL = 0x2000, // L2 fill requests that access the L2 cache
		L2_TRANS_MASK_L2_WB = 0x4000, // L2 writebacks that access the L2 cache
		L2_TRANS_MASK_ALL_PREFETCH = 0x800, // L2 or L3 HW prefetches that access the L2 cache (including rejects)
		L2_TRANS_MASK_ALL_PF = 0x800, // L2 or L3 HW prefetches that access the L2 cache (including rejects)
		L2_TRANS_MASK_RFO = 0x200, // RFO requests that access the L2 cache
		LAST_LEVEL_CACHE_MISSES = 0x412e, // This is an alias for L3_LAT_CACHE:MISS
		LLC_MISSES = 0x412e, // Alias for LAST_LEVEL_CACHE_MISSES
		LAST_LEVEL_CACHE_REFERENCES = 0x4f2e, // This is an alias for L3_LAT_CACHE:REFERENCE
		LLC_REFERENCES = 0x4f2e, // Alias for LAST_LEVEL_CACHE_REFERENCES
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS_MASK_STORE_FORWARD = 0x200, // Loads blocked by overlapping with store buffer that cannot be forwarded
		LD_BLOCKS_MASK_NO_SR = 0x800, // Number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL_MASK_ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LOAD_HIT_PRE = 0x4c, // Load dispatches that hit fill buffer
		LOAD_HIT_PRE_MASK_HW_PF = 0x200, // Non sw-prefetch load dispatches that hit the fill buffer allocated for HW prefetch
		LOAD_HIT_PRE_MASK_SW_PF = 0x100, // Non sw-prefetch load dispatches that hit the fill buffer allocated for SW prefetch
		L3_LAT_CACHE = 0x2e, // Core-originated cacheable demand requests to L3
		L3_LAT_CACHE_MASK_MISS = 0x4100, // Core-originated cacheable demand requests missed L3
		L3_LAT_CACHE_MASK_REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to L3
		LONGEST_LAT_CACHE = 0x2e, // Core-originated cacheable demand requests to L3
		LONGEST_LAT_CACHE_MASK_MISS = 0x4100, // Core-originated cacheable demand requests missed L3
		LONGEST_LAT_CACHE_MASK_REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to L3
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS_MASK_MASKMOV = 0x2000, // The number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0
		MACHINE_CLEARS_MASK_MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS_MASK_SMC = 0x400, // Self-Modifying Code detected
		MACHINE_CLEARS_MASK_COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of machine clears (nukes) of any type
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired (deprecated use MEM_LOAD_UOPS_LLC_HIT_RETIRED)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // Load uops retired that missed the LLC
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_LOCAL_DRAM = 0x100, // Number of retired load uops that missed L3 but were service by local RAM. Does not count hardware prefetches (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_DRAM = 0xc00, // Number of retired load uops that missed L3 but were service by remote RAM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_HITM = 0x1000, // Number of retired load uops whose data sources was remote HITM (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_FWD = 0x2000, // Load uops that miss in the L3 whose data source was forwarded from a remote cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Memory loads uops retired
		MEM_LOAD_UOPS_RETIRED_MASK_HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L1_MISS = 0x800, // Load miss in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L2_MISS = 0x1000, // Load misses in mid-level (L2) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L3_MISS = 0x2000, // Load miss in last-level (L3) cache (Precise Event)
		MEM_LOAD_RETIRED = 0xd1, // Memory loads uops retired (deprecated use MEM_LOAD_UOPS_RETIRED)
		MEM_LOAD_RETIRED_MASK_HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_RETIRED_MASK_L1_MISS = 0x800, // Load miss in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L2_MISS = 0x1000, // Load misses in mid-level (L2) cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_RETIRED_MASK_L3_MISS = 0x2000, // Load miss in last-level (L3) cache (Precise Event)
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired
		MEM_TRANS_RETIRED_MASK_LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED_MASK_PRECISE_STORE = 0x200, // Capture where stores occur
		MEM_UOPS_RETIRED = 0xd0, // Memory uops retired
		MEM_UOPS_RETIRED_MASK_ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED_MASK_LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED_MASK_SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED_MASK_SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED_MASK_STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MEM_UOP_RETIRED = 0xd0, // Memory uops retired (deprecated use MEM_UOPS_RETIRED)
		MEM_UOP_RETIRED_MASK_ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED_MASK_LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED_MASK_SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED_MASK_SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED_MASK_STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF_MASK_LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF_MASK_STORES = 0x200, // Speculative cache-line split Store-address uops dispatched to L1D
		OFFCORE_REQUESTS = 0xb0, // Offcore requests
		OFFCORE_REQUESTS_MASK_ALL_DATA_RD = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS_MASK_ALL_DATA_READ = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS_MASK_DEMAND_CODE_RD = 0x200, // Offcore code read requests
		OFFCORE_REQUESTS_MASK_DEMAND_DATA_RD = 0x100, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS_MASK_DEMAND_RFO = 0x400, // Offcore Demand RFOs
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING_MASK_ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING_MASK_ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ
		OTHER_ASSISTS = 0xc1, // Count hardware assists
		OTHER_ASSISTS_MASK_AVX_TO_SSE = 0x1000, // Number of transitions from AVX-256 to legacy SSE when penalty applicable
		OTHER_ASSISTS_MASK_SSE_TO_AVX = 0x2000, // Number of transitions from legacy SSE to AVX-256 when penalty applicable
		OTHER_ASSISTS_MASK_AVX_STORE = 0x0800, // Number of assists associated with 256-bit AVX stores
		OTHER_ASSISTS_MASK_WB = 0x8000, // Number of times the microcode assist is invoked by hardware upon uop writeback
		RESOURCE_STALLS = 0xa2, // Resource related stall cycles
		RESOURCE_STALLS_MASK_ANY = 0x100, // Cycles stalled due to Resource Related reason
		RESOURCE_STALLS_MASK_RS = 0x400, // Cycles stalled due to no eligible RS entry available
		RESOURCE_STALLS_MASK_SB = 0x800, // Cycles stalled due to no store buffers available (not including draining from sync)
		RESOURCE_STALLS_MASK_ROB = 0x1000, // Cycles stalled due to re-order buffer full
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY_MASK_CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss loads
		CYCLE_ACTIVITY_MASK_CYCLES_LDM_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending memory loads
		CYCLE_ACTIVITY_MASK_CYCLES_L1D_PENDING = 0x0800 | (0x8 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY_MASK_CYCLES_NO_EXECUTE = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles of dispatch stalls
		CYCLE_ACTIVITY_MASK_STALLS_L2_PENDING = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls due to L2 pending loads
		CYCLE_ACTIVITY_MASK_STALLS_L1D_PENDING = 0x0c00 | (0xc << INTEL_X86_CMASK_BIT), // Execution stalls due to L1D pending loads
		CYCLE_ACTIVITY_MASK_STALLS_LDM_PENDING = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls due to memory loads
		ROB_MISC_EVENTS = 0xcc, // Reorder buffer events
		ROB_MISC_EVENTS_MASK_LBR_INSERTS = 0x2000, // Count each time an new LBR record is saved by HW
		RS_EVENTS = 0x5e, // Reservation station events
		RS_EVENTS_MASK_EMPTY_CYCLES = 0x100, // Cycles the RS is empty for this thread
		RS_EVENTS_MASK_EMPTY_END = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Counts number of time the Reservation Station (RS) goes from empty to non-empty
		DTLB_LOAD_ACCESS = 0x5f, // TLB access
		DTLB_LOAD_ACCESS_MASK_STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		DTLB_LOAD_ACCESS_MASK_LOAD_STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		TLB_ACCESS = 0x5f, // TLB access
		TLB_ACCESS_MASK_STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		TLB_ACCESS_MASK_LOAD_STLB_HIT = 0x400, // Number of load operations that missed L1TLB but hit L2TLB
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH_MASK_DTLB_THREAD = 0x100, // Number of DTLB flushes of thread-specific entries
		TLB_FLUSH_MASK_STLB_ANY = 0x2000, // Number of STLB flushes
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED_MASK_CORE = 0x200, // Counts total number of uops executed from any thread per cycle
		UOPS_EXECUTED_MASK_THREAD = 0x100, // Counts total number of uops executed per thread each cycle
		UOPS_EXECUTED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed
		UOPS_EXECUTED_MASK_CYCLES_GE_1_UOP_EXEC = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_2_UOPS_EXEC = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_3_UOPS_EXEC = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_4_UOPS_EXEC = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_1 = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_2 = 0x200 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_3 = 0x200 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_4 = 0x200 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV, // Cycles where no uop is executed on any thread
		UOPS_DISPATCHED_PORT = 0xa1, // Uops dispatch to specific ports
		UOPS_DISPATCHED_PORT_MASK_PORT_0 = 0x100, // Cycles in which a uop is dispatched on port 0
		UOPS_DISPATCHED_PORT_MASK_PORT_1 = 0x200, // Cycles in which a uop is dispatched on port 1
		UOPS_DISPATCHED_PORT_MASK_PORT_2 = 0xc00, // Cycles in which a uop is dispatched on port 2
		UOPS_DISPATCHED_PORT_MASK_PORT_3 = 0x3000, // Cycles in which a uop is dispatched on port 3
		UOPS_DISPATCHED_PORT_MASK_PORT_4 = 0x4000, // Cycles in which a uop is dispatched on port 4
		UOPS_DISPATCHED_PORT_MASK_PORT_5 = 0x8000, // Cycles in which a uop is dispatched on port 5
		UOPS_DISPATCHED_PORT_MASK_PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 0 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 1 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_2_CORE = 0xc00 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 2 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_3_CORE = 0x3000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 3 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_4_CORE = 0x4000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 4 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_5_CORE = 0x8000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 5 for any thread
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED_MASK_ANY = 0x100, // Number of uops issued by the RAT to the Reservation Station (RS)
		UOPS_ISSUED_MASK_CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued on this core (by any thread)
		UOPS_ISSUED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued by this thread
		UOPS_ISSUED_MASK_FLAGS_MERGE = 0x1000, // Number of flags-merge uops allocated. Such uops adds delay
		UOPS_ISSUED_MASK_SLOW_LEA = 0x2000, // Number of slow LEA or similar uops allocated
		UOPS_ISSUED_MASK_SINGLE_MUL = 0x4000, // Number of multiply packed/scalar single precision uops allocated
		UOPS_RETIRED = 0xc2, // Uops retired
		UOPS_RETIRED_MASK_ALL = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED_MASK_ANY = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED_MASK_RETIRE_SLOTS = 0x200, // Number of retirement slots used (Precise Event)
		UOPS_RETIRED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no executable uop retired (Precise Event)
		UOPS_RETIRED_MASK_TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Total cycles using precise uop retired event (Precise Event)
		FP_COMP_OPS_EXE = 0x10, // Counts number of floating point events
		FP_COMP_OPS_EXE_MASK_X87 = 0x100, // Number of X87 uops executed
		FP_COMP_OPS_EXE_MASK_SSE_FP_PACKED_DOUBLE = 0x1000, // Number of SSE or AVX-128 double precision FP packed uops executed
		FP_COMP_OPS_EXE_MASK_SSE_FP_SCALAR_SINGLE = 0x2000, // Number of SSE or AVX-128 single precision FP scalar uops executed
		FP_COMP_OPS_EXE_MASK_SSE_PACKED_SINGLE = 0x4000, // Number of SSE or AVX-128 single precision FP packed uops executed
		FP_COMP_OPS_EXE_MASK_SSE_SCALAR_DOUBLE = 0x8000, // Number of SSE or AVX-128 double precision FP scalar uops executed
		SIMD_FP_256 = 0x11, // Counts 256-bit packed floating point instructions
		SIMD_FP_256_MASK_PACKED_SINGLE = 0x100, // Counts 256-bit packed single-precision
		SIMD_FP_256_MASK_PACKED_DOUBLE = 0x200, // Counts 256-bit packed double-precision
		LSD = 0xa8, // Loop stream detector
		LSD_MASK_UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD_MASK_ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with uops delivered by the LSD but which did not come from decoder
		LSD_MASK_CYCLES_4_UOPS = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles with 4 uops delivered by the LSD but which did not come from decoder
		EPT = 0x4f, // Extended page table
		EPT_MASK_WALK_CYCLES = 0x1000, // Cycles for an extended page table walk
		PAGE_WALKS = 0xbe, // page walker
		PAGE_WALKS_MASK_LLC_MISS = 0x100, // Number of page walks with a LLC miss
		INT_MISC = 0xd, // Miscellaneous interruptions
		INT_MISC_MASK_RECOVERY_CYCLES = 0x300 | (1 << INTEL_X86_CMASK_BIT), // Cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception
		INT_MISC_MASK_RECOVERY_CYCLES_ANY = 0x300 | (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)
		INT_MISC_MASK_RECOVERY_STALLS_COUNT = 0x300 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of occurrences waiting for Machine Clears
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore reqest buffer
		OFFCORE_REQUESTS_BUFFER_MASK_SQ_FULL = 0x0100, // Number of cycles the offcore requests buffer is full
		SQ_MISC = 0xf4, // SuperQueue miscellaneous
		SQ_MISC_MASK_SPLIT_LOCK = 0x1000, // Number of split locks in the super queue (SQ)
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0_MASK_DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0_MASK_DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0_MASK_DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0_MASK_WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_0_MASK_PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_0_MASK_PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_0_MASK_PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0_MASK_BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_0_MASK_STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_0_MASK_OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0_MASK_ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_0_MASK_ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0_MASK_ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_0_MASK_ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_0_MASK_ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0_MASK_NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0_MASK_LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0_MASK_LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0_MASK_LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0_MASK_LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0_MASK_SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0_MASK_SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0_MASK_SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0_MASK_SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0_MASK_SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0_MASK_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0_MASK_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0_MASK_SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1_MASK_DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1_MASK_DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1_MASK_DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1_MASK_WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_1_MASK_PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_1_MASK_PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_1_MASK_PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1_MASK_BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_1_MASK_STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_1_MASK_OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1_MASK_ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_1_MASK_ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1_MASK_ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_1_MASK_ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_1_MASK_ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1_MASK_NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1_MASK_LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1_MASK_LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1_MASK_LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1_MASK_LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1_MASK_SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1_MASK_SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1_MASK_SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1_MASK_SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1_MASK_SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1_MASK_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1_MASK_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1_MASK_SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};