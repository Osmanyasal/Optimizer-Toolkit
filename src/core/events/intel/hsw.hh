#include <cstdint>

namespace optkit_intel{
	enum class hsw : uint64_t {
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS_MASK_ANY = 0x1f00, // Counts the number of times the front end is re-steered
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC_MASK_NONTAKEN_CONDITIONAL = 0x4100, // All macro conditional nontaken branch instructions
		BR_INST_EXEC_MASK_NONTAKEN_COND = 0x4100, // All macro conditional nontaken branch instructions
		BR_INST_EXEC_MASK_TAKEN_CONDITIONAL = 0x8100, // Taken speculative and retired macro-conditional branches
		BR_INST_EXEC_MASK_TAKEN_COND = 0x8100, // Taken speculative and retired macro-conditional branches
		BR_INST_EXEC_MASK_TAKEN_DIRECT_JUMP = 0x8200, // Taken speculative and retired macro-conditional branch instructions excluding calls and indirects
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // Taken speculative and retired indirect branches excluding calls and returns
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_NEAR_RETURN = 0x8800, // Taken speculative and retired indirect branches with return mnemonic
		BR_INST_EXEC_MASK_TAKEN_DIRECT_NEAR_CALL = 0x9000, // Taken speculative and retired direct near calls
		BR_INST_EXEC_MASK_ALL_CONDITIONAL = 0xc100, // Speculative and retired macro-conditional branches
		BR_INST_EXEC_MASK_ALL_COND = 0xc100, // Speculative and retired macro-conditional branches
		BR_INST_EXEC_MASK_ANY_COND = 0xc100, // Speculative and retired macro-conditional branches
		BR_INST_EXEC_MASK_ALL_DIRECT_JMP = 0xc200, // Speculative and retired macro-unconditional branches excluding calls and indirects
		BR_INST_EXEC_MASK_ALL_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // Speculative and retired indirect branches excluding calls and returns
		BR_INST_EXEC_MASK_ALL_INDIRECT_NEAR_RETURN = 0xc800, // Speculative and retired indirect return branches
		BR_INST_EXEC_MASK_ALL_DIRECT_NEAR_CALL = 0xd000, // Speculative and retired direct near calls
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All indirect calls
		BR_INST_EXEC_MASK_ALL_BRANCHES = 0xff00, // All branch instructions executed
		BR_INST_RETIRED = 0xc4, // Branch instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_CONDITIONAL = 0x100, // Counts all taken and not taken macro conditional branch instructions
		BR_INST_RETIRED_MASK_COND = 0x100, // Counts all taken and not taken macro conditional branch instructions
		BR_INST_RETIRED_MASK_NEAR_CALL = 0x200, // Counts all macro direct and indirect near calls
		BR_INST_RETIRED_MASK_ALL_BRANCHES = 0x0, // Counts all taken and not taken macro branches including far branches (architectural event)
		BR_INST_RETIRED_MASK_NEAR_RETURN = 0x800, // Counts the number of near ret instructions retired
		BR_INST_RETIRED_MASK_NOT_TAKEN = 0x1000, // Counts all not taken macro branch instructions retired
		BR_INST_RETIRED_MASK_NEAR_TAKEN = 0x2000, // Counts the number of near branch taken instructions retired
		BR_INST_RETIRED_MASK_FAR_BRANCH = 0x4000, // Counts the number of far branch instructions retired
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC_MASK_NONTAKEN_CONDITIONAL = 0x4100, // Not taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_NONTAKEN_COND = 0x4100, // Not taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_TAKEN_CONDITIONAL = 0x8100, // Taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_TAKEN_COND = 0x8100, // Taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // Taken speculative and retired mispredicted indirect branches excluding calls and returns
		BR_MISP_EXEC_MASK_TAKEN_RETURN_NEAR = 0x8800, // Taken speculative and retired mispredicted indirect branches with return mnemonic
		BR_MISP_EXEC_MASK_ALL_CONDITIONAL = 0xc100, // Speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_ANY_COND = 0xc100, // Speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_ALL_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC_MASK_ALL_BRANCHES = 0xff00, // Speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC_MASK_TAKEN_INDIRECT_NEAR_CALL = 0xa000, // Taken speculative and retired mispredicted indirect calls
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches (Precise Event)
		BR_MISP_RETIRED_MASK_CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions
		BR_MISP_RETIRED_MASK_COND = 0x100, // All mispredicted macro conditional branch instructions
		BR_MISP_RETIRED_MASK_ALL_BRANCHES = 0x0, // All mispredicted macro branches (architectural event)
		BR_MISP_RETIRED_MASK_NEAR_TAKEN = 0x2000, // number of near branch instructions retired that were mispredicted and taken
		CPL_CYCLES = 0x5c, // Unhalted core cycles at a specific ring level
		CPL_CYCLES_MASK_RING0 = 0x100, // Unhalted core cycles when the thread is in ring 0
		CPL_CYCLES_MASK_RING123 = 0x200, // Unhalted core cycles when thread is in rings 1
		CPL_CYCLES_MASK_RING0_TRANS = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of intervals between processor halts while thread is in ring 0
		CPU_CLK_THREAD_UNHALTED = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		CPU_CLK_THREAD_UNHALTED_MASK_REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_THREAD_UNHALTED_MASK_REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses 100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_THREAD_UNHALTED_MASK_REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_THREAD_UNHALTED_MASK_THREAD_P = 0x000, // Cycles when thread is not halted
		CPU_CLK_THREAD_UNHALTED_MASK_ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		CPU_CLK_UNHALTED = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY_MASK_CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss loads (must use with HT off only)
		CYCLE_ACTIVITY_MASK_CYCLES_LDM_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending memory loads
		CYCLE_ACTIVITY_MASK_CYCLES_L1D_PENDING = 0x0800 | (0x8 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY_MASK_STALLS_L1D_PENDING = 0x0c00 | (0xc << INTEL_X86_CMASK_BIT), // Executions stalls due to pending L1D load cache misses
		CYCLE_ACTIVITY_MASK_STALLS_L2_PENDING = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls due to L2 pending loads (must use with HT off only)
		CYCLE_ACTIVITY_MASK_STALLS_LDM_PENDING = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls due to memory subsystem
		CYCLE_ACTIVITY_MASK_CYCLES_NO_EXECUTE = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles during which no instructions were executed in the execution stage of the pipeline
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		DTLB_LOAD_MISSES_MASK_WALK_COMPLETED_4K = 0x200, // Misses in all TLB levels causes a page walk that completes (4K)
		DTLB_LOAD_MISSES_MASK_WALK_COMPLETED_2M_4M = 0x400, // Misses in all TLB levels causes a page walk that completes (2M/4M)
		DTLB_LOAD_MISSES_MASK_WALK_COMPLETED = 0xe00, // Misses in all TLB levels causes a page walk of any page size that completes
		DTLB_LOAD_MISSES_MASK_WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		DTLB_LOAD_MISSES_MASK_STLB_HIT_4K = 0x2000, // Misses that miss the DTLB and hit the STLB (4K)
		DTLB_LOAD_MISSES_MASK_STLB_HIT_2M = 0x4000, // Misses that miss the DTLB and hit the STLB (2M)
		DTLB_LOAD_MISSES_MASK_STLB_HIT = 0x6000, // Number of cache load STLB hits. No page walk
		DTLB_LOAD_MISSES_MASK_PDE_CACHE_MISS = 0x8000, // DTLB misses with low part of linear-to-physical address translation missed
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		DTLB_STORE_MISSES_MASK_WALK_COMPLETED_4K = 0x200, // Misses in all TLB levels causes a page walk that completes (4K)
		DTLB_STORE_MISSES_MASK_WALK_COMPLETED_2M_4M = 0x400, // Misses in all TLB levels causes a page walk that completes (2M/4M)
		DTLB_STORE_MISSES_MASK_WALK_COMPLETED = 0xe00, // Misses in all TLB levels causes a page walk of any page size that completes
		DTLB_STORE_MISSES_MASK_WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		DTLB_STORE_MISSES_MASK_STLB_HIT_4K = 0x2000, // Misses that miss the DTLB and hit the STLB (4K)
		DTLB_STORE_MISSES_MASK_STLB_HIT_2M = 0x4000, // Misses that miss the DTLB and hit the STLB (2M)
		DTLB_STORE_MISSES_MASK_STLB_HIT = 0x6000, // Number of cache load STLB hits. No page walk
		DTLB_STORE_MISSES_MASK_PDE_CACHE_MISS = 0x8000, // DTLB misses with low part of linear-to-physical address translation missed
		FP_ASSIST = 0xca, // X87 floating-point assists
		FP_ASSIST_MASK_X87_OUTPUT = 0x200, // Number of X87 FP assists due to output values
		FP_ASSIST_MASK_X87_INPUT = 0x400, // Number of X87 FP assists due to input values
		FP_ASSIST_MASK_SIMD_OUTPUT = 0x800, // Number of SIMD FP assists due to output values
		FP_ASSIST_MASK_SIMD_INPUT = 0x1000, // Number of SIMD FP assists due to input values
		FP_ASSIST_MASK_ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SEE or FP assists
		FP_ASSIST_MASK_ALL = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input and output SSE or FP assist
		HLE_RETIRED = 0xc8, // HLE execution (Precise Event)
		HLE_RETIRED_MASK_START = 0x100, // Number of times an HLE execution started
		HLE_RETIRED_MASK_COMMIT = 0x200, // Number of times an HLE execution successfully committed
		HLE_RETIRED_MASK_ABORTED = 0x400, // Number of times an HLE execution aborted due to any reasons (multiple categories may count as one) (Precise Event)
		HLE_RETIRED_MASK_ABORTED_MISC1 = 0x800, // Number of times an HLE execution aborted due to various memory events
		HLE_RETIRED_MASK_ABORTED_MISC2 = 0x1000, // Number of times an HLE execution aborted due to uncommon conditions
		HLE_RETIRED_MASK_ABORTED_MISC3 = 0x2000, // Number of times an HLE execution aborted due to HLE-unfriendly instructions
		HLE_RETIRED_MASK_ABORTED_MISC4 = 0x4000, // Number of times an HLE execution aborted due to incompatible memory type
		HLE_RETIRED_MASK_ABORTED_MISC5 = 0x8000, // Number of times an HLE execution aborted due to none of the other 4 reasons (e.g.
		ICACHE = 0x80, // Instruction Cache
		ICACHE_MASK_MISSES = 0x200, // Number of Instruction Cache
		ICACHE_MASK_HIT = 0x100, // Number of Instruction Cache
		ICACHE_MASK_IFETCH_STALL = 0x400, // Number of cycles where a code-fetch stalled due to L1 instruction cache miss or an iTLB miss
		IDQ = 0x79, // IDQ operations
		IDQ_MASK_EMPTY = 0x200, // Cycles the Instruction Decode Queue (IDQ) is empty
		IDQ_MASK_MITE_UOPS = 0x400, // Number of uops delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ_MASK_DSB_UOPS = 0x800, // Number of uops delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path
		IDQ_MASK_MS_DSB_UOPS = 0x1000, // Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ_MASK_MS_MITE_UOPS = 0x2000, // Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ_MASK_MS_UOPS = 0x3000, // Number of Uops were delivered into Instruction Decode Queue (IDQ) from MS
		IDQ_MASK_MS_UOPS_CYCLES = 0x3000 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ_MASK_MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ_MASK_MITE_UOPS_CYCLES = 0x400 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ_MASK_DSB_UOPS_CYCLES = 0x800 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path
		IDQ_MASK_MS_DSB_UOPS_CYCLES = 0x1000 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ_MASK_MS_DSB_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequencer (MS) is busy
		IDQ_MASK_ALL_DSB_CYCLES_4_UOPS = 0x1800 | (4 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering 4 Uops
		IDQ_MASK_ALL_DSB_CYCLES_ANY_UOPS = 0x1800 | (1 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering any Uop
		IDQ_MASK_ALL_MITE_CYCLES_4_UOPS = 0x2400 | (4 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering 4 Uops
		IDQ_MASK_ALL_MITE_CYCLES_ANY_UOPS = 0x2400 | (1 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering any Uop
		IDQ_MASK_ALL_MITE_UOPS = 0x3c00, // Number of uops delivered to Instruction Decode Queue (IDQ) from any path
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED_MASK_CORE = 0x100, // Count number of non-delivered uops to Resource Allocation Table (RAT)
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles per thread when 4 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_1_UOP_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles per thread when 3 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_2_UOP_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles with less than 2 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_3_UOP_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with less than 3 uops delivered by the front end
		INST_RETIRED = 0xc0, // Number of instructions retired (Precise Event)
		INST_RETIRED_MASK_CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles Front-End (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling FE
		INST_RETIRED_MASK_ALL = 0x100, // Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Precise Event)
		INST_RETIRED_MASK_TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Number of cycles using always true condition
		INST_RETIRED_MASK_PREC_DIST = 0x100, // Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution
		INST_RETIRED_MASK_X87 = 0x200, // X87 FP operations retired with no exceptions. Also counts flows that have several X87 or flows that use X87 uops in the exception handling
		INT_MISC = 0xd, // Miscellaneous interruptions
		INT_MISC_MASK_RECOVERY_CYCLES = 0x300 | (1 << INTEL_X86_CMASK_BIT), // Cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception
		INT_MISC_MASK_RECOVERY_CYCLES_ANY = 0x300 | (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)
		INT_MISC_MASK_RECOVERY_STALLS_COUNT = 0x300 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of occurrences waiting for Machine Clears
		ITLB = 0xae, // Instruction TLB
		ITLB_MASK_ITLB_FLUSH = 0x100, // Flushing of the Instruction TLB (ITLB) pages independent of page size
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		ITLB_MISSES_MASK_WALK_COMPLETED_4K = 0x200, // Misses in all TLB levels causes a page walk that completes (4K)
		ITLB_MISSES_MASK_WALK_COMPLETED_2M_4M = 0x400, // Misses in all TLB levels causes a page walk that completes (2M/4M)
		ITLB_MISSES_MASK_WALK_COMPLETED = 0xe00, // Misses in all TLB levels causes a page walk of any page size that completes
		ITLB_MISSES_MASK_WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		ITLB_MISSES_MASK_STLB_HIT_4K = 0x2000, // Misses that miss the DTLB and hit the STLB (4K)
		ITLB_MISSES_MASK_STLB_HIT_2M = 0x4000, // Misses that miss the DTLB and hit the STLB (2M)
		ITLB_MISSES_MASK_STLB_HIT = 0x6000, // Number of cache load STLB hits. No page walk
		L1D = 0x51, // L1D cache
		L1D_MASK_REPLACEMENT = 0x100, // L1D Data line replacements
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS_MASK_PENDING = 0x100, // Cycles with L1D load misses outstanding
		L1D_PEND_MISS_MASK_PENDING_CYCLES = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load misses outstanding
		L1D_PEND_MISS_MASK_OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number L1D miss outstanding
		L1D_PEND_MISS_MASK_EDGE = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number L1D miss outstanding
		L1D_PEND_MISS_MASK_REQUEST_FB_FULL = 0x200, // Number of times a demand request was blocked due to Fill Buffer (FB) unavailability
		L1D_PEND_MISS_MASK_FB_FULL = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles a demand request was blocked due to Fill Buffer (FB) unavailability
		L2_DEMAND_RQSTS = 0x27, // Demand Data Read requests to L2
		L2_DEMAND_RQSTS_MASK_WB_HIT = 0x5000, // WB requests that hit L2 cache
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN_MASK_I = 0x100, // L2 cache lines in I state filling L2
		L2_LINES_IN_MASK_S = 0x200, // L2 cache lines in S state filling L2
		L2_LINES_IN_MASK_E = 0x400, // L2 cache lines in E state filling L2
		L2_LINES_IN_MASK_ALL = 0x700, // L2 cache lines filling L2
		L2_LINES_IN_MASK_ANY = 0x700, // L2 cache lines filling L2
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT_MASK_DEMAND_CLEAN = 0x500, // Number of clean L2 cachelines evicted by demand
		L2_LINES_OUT_MASK_DEMAND_DIRTY = 0x600, // Number of dirty L2 cachelines evicted by demand
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS_MASK_DEMAND_DATA_RD_MISS = 0x2100, // Demand Data Read requests that miss L2 cache
		L2_RQSTS_MASK_DEMAND_DATA_RD_HIT = 0x4100, // Demand Data Read requests that hit L2 cache
		L2_RQSTS_MASK_DEMAND_RFO_MISS = 0x2200, // RFO requests that miss L2 cache
		L2_RQSTS_MASK_RFO_MISS = 0x2200, // RFO requests that miss L2 cache
		L2_RQSTS_MASK_DEMAND_RFO_HIT = 0x4200, // RFO requests that hit L2 cache
		L2_RQSTS_MASK_RFO_HIT = 0x4200, // RFO requests that hit L2 cache
		L2_RQSTS_MASK_CODE_RD_MISS = 0x2400, // L2 cache misses when fetching instructions
		L2_RQSTS_MASK_ALL_DEMAND_MISS = 0x2700, // All demand requests that miss the L2 cache
		L2_RQSTS_MASK_CODE_RD_HIT = 0x4400, // L2 cache hits when fetching instructions
		L2_RQSTS_MASK_L2_PF_MISS = 0x3800, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS_MASK_PF_MISS = 0x3800, // Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that miss L2 cache
		L2_RQSTS_MASK_MISS = 0x3f00, // All requests that miss the L2 cache
		L2_RQSTS_MASK_L2_PF_HIT = 0xd800, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS_MASK_PF_HIT = 0xd800, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS_MASK_ALL_DEMAND_DATA_RD = 0xe100, // Any data read request to L2 cache
		L2_RQSTS_MASK_ALL_RFO = 0xe200, // Any data RFO request to L2 cache
		L2_RQSTS_MASK_ALL_CODE_RD = 0xe400, // Any code read request to L2 cache
		L2_RQSTS_MASK_ALL_DEMAND_REFERENCES = 0xe700, // All demand requests to L2 cache
		L2_RQSTS_MASK_ALL_PF = 0xf800, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS_MASK_REFERENCES = 0xff00, // All requests to L2 cache
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS_MASK_DEMAND_DATA_RD = 0x100, // Demand Data Read requests that access L2 cache
		L2_TRANS_MASK_RFO = 0x200, // RFO requests that access L2 cache
		L2_TRANS_MASK_CODE_RD = 0x400, // L2 cache accesses when fetching instructions
		L2_TRANS_MASK_ALL_PF = 0x800, // L2 or L3 HW prefetches that access L2 cache
		L2_TRANS_MASK_L1D_WB = 0x1000, // L1D writebacks that access L2 cache
		L2_TRANS_MASK_L2_FILL = 0x2000, // L2 fill requests that access L2 cache
		L2_TRANS_MASK_L2_WB = 0x4000, // L2 writebacks that access L2 cache
		L2_TRANS_MASK_ALL_REQUESTS = 0x8000, // Transactions accessing L2 pipe
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS_MASK_STORE_FORWARD = 0x200, // Counts the number of loads blocked by overlapping with store buffer entries that cannot be forwarded
		LD_BLOCKS_MASK_NO_SR = 0x800, // number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL_MASK_ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LOAD_HIT_PRE = 0x4c, // Load dispatches
		LOAD_HIT_PRE_MASK_SW_PF = 0x100, // Non software-prefetch load dispatches that hit FB allocated for software prefetch
		LOAD_HIT_PRE_MASK_HW_PF = 0x200, // Non software-prefetch load dispatches that hit FB allocated for hardware prefetch
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES_MASK_SPLIT_LOCK_UC_LOCK_DURATION = 0x100, // Cycles in which the L1D and L2 are locked
		LOCK_CYCLES_MASK_CACHE_LOCK_DURATION = 0x200, // cycles that the L1D is locked
		LONGEST_LAT_CACHE = 0x2e, // L3 cache
		LONGEST_LAT_CACHE_MASK_MISS = 0x4100, // Core-originated cacheable demand requests missed LLC - architectural event
		LONGEST_LAT_CACHE_MASK_REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to LLC - architectural event
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS_MASK_CYCLES = 0x100, // Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes
		MACHINE_CLEARS_MASK_MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS_MASK_SMC = 0x400, // Number of Self-modifying code (SMC) Machine Clears detected
		MACHINE_CLEARS_MASK_MASKMOV = 0x2000, // This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0
		MACHINE_CLEARS_MASK_COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of machine clears (nukes) of any type
		MEM_LOAD_UOPS_L3_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_UOPS_L3_HIT_RETIRED_MASK_XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_UOPS_L3_HIT_RETIRED_MASK_XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_UOPS_L3_HIT_RETIRED_MASK_XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_UOPS_L3_HIT_RETIRED_MASK_XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_UOPS_L3_MISS_RETIRED = 0xd3, // Load uops retired that missed the L3 (Precise Event)
		MEM_LOAD_UOPS_L3_MISS_RETIRED_MASK_LOCAL_DRAM = 0x100, // Retired load uops missing L3 cache but hitting local memory
		MEM_LOAD_UOPS_L3_MISS_RETIRED_MASK_REMOTE_DRAM = 0x400, // Number of retired load uops that missed L3 but were service by remote RAM
		MEM_LOAD_UOPS_L3_MISS_RETIRED_MASK_REMOTE_HITM = 0x1000, // Number of retired load uops whose data sources was remote HITM (Precise Event)
		MEM_LOAD_UOPS_L3_MISS_RETIRED_MASK_REMOTE_FWD = 0x2000, // Load uops that miss in the L3 whose data source was forwarded from a remote cache (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // Load uops retired that missed the L3 (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_LOCAL_DRAM = 0x100, // Retired load uops missing L3 cache but hitting local memory
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_DRAM = 0x400, // Number of retired load uops that missed L3 but were service by remote RAM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_HITM = 0x1000, // Number of retired load uops whose data sources was remote HITM (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_FWD = 0x2000, // Load uops that miss in the L3 whose data source was forwarded from a remote cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Retired load uops (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L1_HIT = 0x100, // Retired load uops with L1 cache hits as data source
		MEM_LOAD_UOPS_RETIRED_MASK_L2_HIT = 0x200, // Retired load uops with L2 cache hits as data source
		MEM_LOAD_UOPS_RETIRED_MASK_L3_HIT = 0x400, // Retired load uops with L3 cache hits as data source
		MEM_LOAD_UOPS_RETIRED_MASK_L1_MISS = 0x800, // Retired load uops which missed the L1D
		MEM_LOAD_UOPS_RETIRED_MASK_L2_MISS = 0x1000, // Retired load uops which missed the L2. Unknown data source excluded
		MEM_LOAD_UOPS_RETIRED_MASK_L3_MISS = 0x2000, // Retired load uops which missed the L3
		MEM_LOAD_UOPS_RETIRED_MASK_HIT_LFB = 0x4000, // Retired load uops which missed L1 but hit line fill buffer (LFB)
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired (Precise Event)
		MEM_TRANS_RETIRED_MASK_LOAD_LATENCY = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED_MASK_LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_UOPS_RETIRED = 0xd0, // Memory uops retired (Precise Event)
		MEM_UOPS_RETIRED_MASK_STLB_MISS_LOADS = 0x1100, // Load uops with true STLB miss retired to architected path
		MEM_UOPS_RETIRED_MASK_STLB_MISS_STORES = 0x1200, // Store uops with true STLB miss retired to architected path
		MEM_UOPS_RETIRED_MASK_LOCK_LOADS = 0x2100, // Load uops with locked access retired
		MEM_UOPS_RETIRED_MASK_SPLIT_LOADS = 0x4100, // Line-splitted load uops retired
		MEM_UOPS_RETIRED_MASK_SPLIT_STORES = 0x4200, // Line-splitted store uops retired
		MEM_UOPS_RETIRED_MASK_ALL_LOADS = 0x8100, // All load uops retired
		MEM_UOPS_RETIRED_MASK_ALL_STORES = 0x8200, // All store uops retired
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF_MASK_LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF_MASK_STORES = 0x200, // Speculative cache-line split store-address uops dispatched to L1D
		MOVE_ELIMINATION = 0x58, // Move Elimination
		MOVE_ELIMINATION_MASK_INT_ELIMINATED = 0x100, // Number of integer Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION_MASK_SIMD_ELIMINATED = 0x200, // Number of SIMD Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION_MASK_INT_NOT_ELIMINATED = 0x400, // Number of integer Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION_MASK_SIMD_NOT_ELIMINATED = 0x800, // Number of SIMD Move Elimination candidate uops that were not eliminated
		OFFCORE_REQUESTS = 0xb0, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS_MASK_DEMAND_DATA_RD = 0x100, // Demand data read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS_MASK_DEMAND_CODE_RD = 0x200, // Demand code read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS_MASK_DEMAND_RFO = 0x400, // Demand RFOs requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS_MASK_ALL_DATA_RD = 0x800, // Data read requests sent to uncore (use with HT off only)
		OTHER_ASSISTS = 0xc1, // Software assist
		OTHER_ASSISTS_MASK_AVX_TO_SSE = 0x800, // Number of transitions from AVX-256 to legacy SSE when penalty applicable
		OTHER_ASSISTS_MASK_SSE_TO_AVX = 0x1000, // Number of transitions from legacy SSE to AVX-256 when penalty applicable
		OTHER_ASSISTS_MASK_ANY_WB_ASSIST = 0x4000, // Number of times any microcode assist is invoked by HW upon uop writeback
		RESOURCE_STALLS = 0xa2, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS_MASK_ANY = 0x100, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS_MASK_ALL = 0x100, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS_MASK_RS = 0x400, // Stall cycles caused by absence of eligible entries in Reservation Station (RS)
		RESOURCE_STALLS_MASK_SB = 0x800, // Cycles Allocator is stalled due to Store Buffer full (not including draining from synch)
		RESOURCE_STALLS_MASK_ROB = 0x1000, // ROB full stall cycles
		ROB_MISC_EVENTS = 0xcc, // ROB miscellaneous events
		ROB_MISC_EVENTS_MASK_LBR_INSERTS = 0x2000, // Count each time an new Last Branch Record (LBR) is inserted
		RS_EVENTS = 0x5e, // Reservation Station
		RS_EVENTS_MASK_EMPTY_CYCLES = 0x100, // Cycles the Reservation Station (RS) is empty for this thread
		RS_EVENTS_MASK_EMPTY_END = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Counts number of time the Reservation Station (RS) goes from empty to non-empty
		RTM_RETIRED = 0xc9, // Restricted Transaction Memory execution (Precise Event)
		RTM_RETIRED_MASK_START = 0x100, // Number of times an RTM execution started
		RTM_RETIRED_MASK_COMMIT = 0x200, // Number of times an RTM execution successfully committed
		RTM_RETIRED_MASK_ABORTED = 0x400, // Number of times an RTM execution aborted due to any reasons (multiple categories may count as one) (Precise Event)
		RTM_RETIRED_MASK_ABORTED_MISC1 = 0x800, // Number of times an RTM execution aborted due to various memory events
		RTM_RETIRED_MASK_ABORTED_MISC2 = 0x1000, // Number of times an RTM execution aborted due to uncommon conditions
		RTM_RETIRED_MASK_ABORTED_MISC3 = 0x2000, // Number of times an RTM execution aborted due to RTM-unfriendly instructions
		RTM_RETIRED_MASK_ABORTED_MISC4 = 0x4000, // Number of times an RTM execution aborted due to incompatible memory type
		RTM_RETIRED_MASK_ABORTED_MISC5 = 0x8000, // Number of times an RTM execution aborted due to none of the other 4 reasons (e.g.
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH_MASK_DTLB_THREAD = 0x100, // Count number of DTLB flushes of thread-specific entries
		TLB_FLUSH_MASK_STLB_ANY = 0x2000, // Count number of any STLB flushes
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED_MASK_CORE = 0x200, // Number of uops executed from any thread
		UOPS_EXECUTED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed
		UOPS_EXECUTED_MASK_CYCLES_GE_1_UOP_EXEC = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_2_UOPS_EXEC = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_3_UOPS_EXEC = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_4_UOPS_EXEC = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_1 = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_2 = 0x200 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_3 = 0x200 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_4 = 0x200 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV, // Cycles where no uop is executed on any thread
		LSD = 0xa8, // Loop stream detector
		LSD_MASK_UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD_MASK_ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with uops delivered by the LSD but which did not come from decoder
		LSD_MASK_CYCLES_4_UOPS = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles with 4 uops delivered by the LSD but which did not come from decoder
		UOPS_EXECUTED_PORT = 0xa1, // Uops dispatched to specific ports
		UOPS_EXECUTED_PORT_MASK_PORT_0 = 0x100, // Cycles which a Uop is executed on port 0
		UOPS_EXECUTED_PORT_MASK_PORT_1 = 0x200, // Cycles which a Uop is executed on port 1
		UOPS_EXECUTED_PORT_MASK_PORT_2 = 0x400, // Cycles which a Uop is executed on port 2
		UOPS_EXECUTED_PORT_MASK_PORT_3 = 0x800, // Cycles which a Uop is executed on port 3
		UOPS_EXECUTED_PORT_MASK_PORT_4 = 0x1000, // Cycles which a Uop is executed on port 4
		UOPS_EXECUTED_PORT_MASK_PORT_5 = 0x2000, // Cycles which a Uop is executed on port 5
		UOPS_EXECUTED_PORT_MASK_PORT_6 = 0x4000, // Cycles which a Uop is executed on port 6
		UOPS_EXECUTED_PORT_MASK_PORT_7 = 0x8000, // Cycles which a Uop is executed on port 7
		UOPS_EXECUTED_PORT_MASK_PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_2_CORE = 0x400 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_3_CORE = 0x800 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_4_CORE = 0x1000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_5_CORE = 0x2000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_6_CORE = 0x4000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT_MASK_PORT_7_CORE = 0x8000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT = 0xa1, // Uops dispatched to specific ports
		UOPS_DISPATCHED_PORT_MASK_PORT_0 = 0x100, // Cycles which a Uop is executed on port 0
		UOPS_DISPATCHED_PORT_MASK_PORT_1 = 0x200, // Cycles which a Uop is executed on port 1
		UOPS_DISPATCHED_PORT_MASK_PORT_2 = 0x400, // Cycles which a Uop is executed on port 2
		UOPS_DISPATCHED_PORT_MASK_PORT_3 = 0x800, // Cycles which a Uop is executed on port 3
		UOPS_DISPATCHED_PORT_MASK_PORT_4 = 0x1000, // Cycles which a Uop is executed on port 4
		UOPS_DISPATCHED_PORT_MASK_PORT_5 = 0x2000, // Cycles which a Uop is executed on port 5
		UOPS_DISPATCHED_PORT_MASK_PORT_6 = 0x4000, // Cycles which a Uop is executed on port 6
		UOPS_DISPATCHED_PORT_MASK_PORT_7 = 0x8000, // Cycles which a Uop is executed on port 7
		UOPS_DISPATCHED_PORT_MASK_PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_2_CORE = 0x400 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_3_CORE = 0x800 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_4_CORE = 0x1000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_5_CORE = 0x2000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_6_CORE = 0x4000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT_MASK_PORT_7_CORE = 0x8000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED_MASK_ANY = 0x100, // Number of Uops issued by the Resource Allocation Table (RAT) to the Reservation Station (RS)
		UOPS_ISSUED_MASK_ALL = 0x100, // Number of Uops issued by the Resource Allocation Table (RAT) to the Reservation Station (RS)
		UOPS_ISSUED_MASK_FLAGS_MERGE = 0x1000, // Number of flags-merge uops being allocated. Such uops adds delay
		UOPS_ISSUED_MASK_SLOW_LEA = 0x2000, // Number of slow LEA or similar uops allocated. Such uop has 3 sources regardless if result of LEA instruction or not
		UOPS_ISSUED_MASK_SINGLE_MUL = 0x4000, // Number of Multiply packed/scalar single precision uops allocated
		UOPS_ISSUED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Counts the number of cycles no uops issued by this thread
		UOPS_ISSUED_MASK_CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Counts the number of cycles no uops issued on this core
		UOPS_RETIRED = 0xc2, // Uops retired (Precise Event)
		UOPS_RETIRED_MASK_ALL = 0x100, // All uops that actually retired
		UOPS_RETIRED_MASK_ANY = 0x100, // All uops that actually retired
		UOPS_RETIRED_MASK_RETIRE_SLOTS = 0x200, // number of retirement slots used non PEBS
		UOPS_RETIRED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles no executable uops retired (Precise Event)
		UOPS_RETIRED_MASK_TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Number of cycles using always true condition applied to PEBS uops retired event
		UOPS_RETIRED_MASK_CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles no executable uops retired on core (Precise Event)
		UOPS_RETIRED_MASK_STALL_OCCURRENCES = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE| (1 << INTEL_X86_CMASK_BIT), // Number of transitions from stalled to unstalled execution (Precise Event)
		TX_MEM = 0x54, // Transactional memory aborts
		TX_MEM_MASK_ABORT_CONFLICT = 0x100, // Number of times a transactional abort was signaled due to data conflict on a transactionally accessed address
		TX_MEM_MASK_ABORT_CAPACITY_WRITE = 0x200, // Number of times a transactional abort was signaled due to data capacity limitation for transactional writes
		TX_MEM_MASK_ABORT_HLE_STORE_TO_ELIDED_LOCK = 0x400, // Number of times a HLE transactional execution aborted due to a non xrelease prefixed instruction writing to an elided lock in the elision buffer
		TX_MEM_MASK_ABORT_HLE_ELISION_BUFFER_NOT_EMPTY = 0x800, // Number of times a HLE transactional execution aborted due to NoAllocatedElisionBuffer being non-zero
		TX_MEM_MASK_ABORT_HLE_ELISION_BUFFER_MISMATCH = 0x1000, // Number of times a HLE transaction execution aborted due to xrelease lock not satisfying the address and value requirements in the elision buffer
		TX_MEM_MASK_ABORT_HLE_ELISION_BUFFER_UNSUPPORTED_ALIGNMENT = 0x2000, // Number of times a HLE transaction execution aborted due to an unsupported read alignment from the elision buffer
		TX_MEM_MASK_ABORT_HLE_ELISION_BUFFER_FULL = 0x4000, // Number of times a HLE clock could not be elided due to ElisionBufferAvailable being zero
		TX_EXEC = 0x5d, // Transactional execution
		TX_EXEC_MASK_MISC1 = 0x100, // Number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution
		TX_EXEC_MASK_MISC2 = 0x200, // Number of times a class of instructions that may cause a transactional abort was executed inside a transactional region
		TX_EXEC_MASK_MISC3 = 0x400, // Number of times an instruction execution caused the supported nest count to be exceeded
		TX_EXEC_MASK_MISC4 = 0x800, // Number of times an instruction with HLE xbegin prefix was executed inside a RTM transactional region
		TX_EXEC_MASK_MISC5 = 0x1000, // Number of times an instruction with HLE xacquire prefix was executed inside a RTM transactional region
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING_MASK_ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ (use with HT off only)
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL_MASK_LCP = 0x100, // Stall caused by changing prefix length of the instruction
		ILD_STALL_MASK_IQ_FULL = 0x400, // Stall cycles due to IQ full
		PAGE_WALKER_LOADS = 0xbc, // Page walker loads
		PAGE_WALKER_LOADS_MASK_DTLB_L1 = 0x1100, // Number of DTLB page walker loads that hit in the L1D and line fill buffer
		PAGE_WALKER_LOADS_MASK_ITLB_L1 = 0x2100, // Number of ITLB page walker loads that hit in the L1I and line fill buffer
		PAGE_WALKER_LOADS_MASK_DTLB_L2 = 0x1200, // Number of DTLB page walker loads that hit in the L2
		PAGE_WALKER_LOADS_MASK_ITLB_L2 = 0x2200, // Number of ITLB page walker loads that hit in the L2
		PAGE_WALKER_LOADS_MASK_DTLB_L3 = 0x1400, // Number of DTLB page walker loads that hit in the L3
		PAGE_WALKER_LOADS_MASK_ITLB_L3 = 0x2400, // Number of ITLB page walker loads that hit in the L3
		PAGE_WALKER_LOADS_MASK_EPT_DTLB_L1 = 0x4100, // Number of extended page table walks from the DTLB that hit in the L1D and line fill buffer
		PAGE_WALKER_LOADS_MASK_EPT_ITLB_L1 = 0x8100, // Number of extended page table walks from the ITLB that hit in the L1D and line fill buffer
		PAGE_WALKER_LOADS_MASK_EPT_DTLB_L2 = 0x4200, // Number of extended page table walks from the DTLB that hit in the L2
		PAGE_WALKER_LOADS_MASK_EPT_ITLB_L2 = 0x8200, // Number of extended page table walks from the ITLB that hit in the L2
		PAGE_WALKER_LOADS_MASK_EPT_DTLB_L3 = 0x4400, // Number of extended page table walks from the DTLB that hit in the L3
		PAGE_WALKER_LOADS_MASK_EPT_ITLB_L3 = 0x8400, // Number of extended page table walks from the ITLB that hit in the L3
		PAGE_WALKER_LOADS_MASK_DTLB_MEMORY = 0x1800, // Number of DTLB page walker loads that hit memory
		PAGE_WALKER_LOADS_MASK_ITLB_MEMORY = 0x2800, // Number of ITLB page walker loads that hit memory
		PAGE_WALKER_LOADS_MASK_EPT_DTLB_MEMORY = 0x4800, // Number of extended page table walks from the DTLB that hit memory
		PAGE_WALKER_LOADS_MASK_EPT_ITLB_MEMORY = 0x8800, // Number of extended page table walks from the ITLB that hit memory
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES_MASK_PENALTY_CYCLES = 0x0200, // Number of DSB to MITE switch true penalty cycles
		EPT = 0x4f, // Extended page table
		EPT_MASK_WALK_CYCLES = 0x1000, // Cycles for an extended page table walk
		ARITH = 0x14, // Counts arithmetic multiply operations
		ARITH_MASK_DIVIDER_UOPS = 0x0200, // Number of uops executed by divider
		AVX = 0xc6, // Counts AVX instructions
		AVX_MASK_ALL = 0x0700, // Approximate counts of AVX and AVX2 256-bit instructions
		SQ_MISC = 0xf4, // SuperQueue miscellaneous
		SQ_MISC_MASK_SPLIT_LOCK = 0x1000, // Number of split locks in the super queue (SQ)
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore reqest buffer
		OFFCORE_REQUESTS_BUFFER_MASK_SQ_FULL = 0x0100, // Number of cycles the offcore requests buffer is full
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0_MASK_DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0_MASK_DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0_MASK_DMND_CODE_RD = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0_MASK_DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0_MASK_WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_0_MASK_PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_CODE_RD = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_L3_DATA_RD = 1ULL << (7 + 8), // Request: number of L2 prefetcher requests to L3 for loads
		OFFCORE_RESPONSE_0_MASK_PF_L3_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_0_MASK_PF_L3_CODE_RD = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0_MASK_PF_L3_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0_MASK_SPLIT_LOCK_UC_LOCK = 1ULL << (10 + 8), // Request: number of bus lock and split lock requests
		OFFCORE_RESPONSE_0_MASK_BUS_LOCKS = 1ULL << (10 + 8), // Request: number of bus lock and split lock requests
		OFFCORE_RESPONSE_0_MASK_STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_0_MASK_OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0_MASK_ANY_CODE_RD = 0x24400, // Request: combination of PF_CODE_RD | DMND_CODE_RD | PF_L3_CODE_RD
		OFFCORE_RESPONSE_0_MASK_ANY_IFETCH = 0x24000, // Request: combination of PF_CODE_RD | PF_L3_CODE_RD
		OFFCORE_RESPONSE_0_MASK_ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0_MASK_ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_L3_DATA_RD
		OFFCORE_RESPONSE_0_MASK_ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_L3_RFO
		OFFCORE_RESPONSE_0_MASK_ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0_MASK_NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0_MASK_L3_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0_MASK_L3_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0_MASK_L3_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0_MASK_L3_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0_MASK_L3_HIT = 0x7ULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0_MASK_L3_HIT = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0_MASK_L3_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS_REMOTE_HOP0 = 0x1ULL << (27+8), // Supplier: counts L3 misses to remote DRAM with 0 hop
		OFFCORE_RESPONSE_0_MASK_L3_MISS_REMOTE_HOP1 = 0x1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_0_MASK_L3_MISS_REMOTE_HOP2P = 0x1ULL << (29+8), // Supplier: counts L3 misses to remote DRAM with 2P hops
		OFFCORE_RESPONSE_0_MASK_L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS = 0x7ULL << (27+8) | 0x1ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS_REMOTE = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_0_MASK_L3_MISS_REMOTE_DRAM = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_0_MASK_SPL_HIT = 0x1ULL << (30+8), // Supplier: counts L3 supplier hit
		OFFCORE_RESPONSE_0_MASK_SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0_MASK_SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0_MASK_SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0_MASK_SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0_MASK_SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0_MASK_SNP_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0_MASK_SNP_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0_MASK_SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1_MASK_DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1_MASK_DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1_MASK_DMND_CODE_RD = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1_MASK_DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1_MASK_WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_1_MASK_PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_CODE_RD = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_L3_DATA_RD = 1ULL << (7 + 8), // Request: number of L2 prefetcher requests to L3 for loads
		OFFCORE_RESPONSE_1_MASK_PF_L3_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_1_MASK_PF_L3_CODE_RD = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1_MASK_PF_L3_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1_MASK_SPLIT_LOCK_UC_LOCK = 1ULL << (10 + 8), // Request: number of bus lock and split lock requests
		OFFCORE_RESPONSE_1_MASK_BUS_LOCKS = 1ULL << (10 + 8), // Request: number of bus lock and split lock requests
		OFFCORE_RESPONSE_1_MASK_STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_1_MASK_OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1_MASK_ANY_CODE_RD = 0x24400, // Request: combination of PF_CODE_RD | DMND_CODE_RD | PF_L3_CODE_RD
		OFFCORE_RESPONSE_1_MASK_ANY_IFETCH = 0x24000, // Request: combination of PF_CODE_RD | PF_L3_CODE_RD
		OFFCORE_RESPONSE_1_MASK_ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1_MASK_ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_L3_DATA_RD
		OFFCORE_RESPONSE_1_MASK_ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_L3_RFO
		OFFCORE_RESPONSE_1_MASK_ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1_MASK_NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1_MASK_L3_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1_MASK_L3_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1_MASK_L3_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1_MASK_L3_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1_MASK_L3_HIT = 0x7ULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1_MASK_L3_HIT = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1_MASK_L3_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS_REMOTE_HOP0 = 0x1ULL << (27+8), // Supplier: counts L3 misses to remote DRAM with 0 hop
		OFFCORE_RESPONSE_1_MASK_L3_MISS_REMOTE_HOP1 = 0x1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_1_MASK_L3_MISS_REMOTE_HOP2P = 0x1ULL << (29+8), // Supplier: counts L3 misses to remote DRAM with 2P hops
		OFFCORE_RESPONSE_1_MASK_L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS = 0x7ULL << (27+8) | 0x1ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS_REMOTE = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_1_MASK_L3_MISS_REMOTE_DRAM = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_1_MASK_SPL_HIT = 0x1ULL << (30+8), // Supplier: counts L3 supplier hit
		OFFCORE_RESPONSE_1_MASK_SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1_MASK_SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1_MASK_SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1_MASK_SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1_MASK_SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1_MASK_SNP_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1_MASK_SNP_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1_MASK_SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};