#include <cstdint>

namespace optkit_intel{
	enum class snb : uint64_t {
		AGU_BYPASS_CANCEL = 0xb6, // Number of executed load operations with all the following traits: 1. addressing of the format [base + offset]
		AGU_BYPASS_CANCEL_MASK_COUNT = 0x100, // This event counts executed load operations
		ARITH = 0x14, // Counts arithmetic multiply operations
		ARITH_MASK_FPU_DIV_ACTIVE = 0x100, // Cycles that the divider is active
		ARITH_MASK_FPU_DIV = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Number of cycles the divider is activated
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS_MASK_ANY = 0x1f00, // Counts the number of times the front end is re-steered
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC_MASK_NONTAKEN_COND = 0x4100, // All macro conditional non-taken branch instructions
		BR_INST_EXEC_MASK_TAKEN_COND = 0x8100, // All macro conditional taken branch instructions
		BR_INST_EXEC_MASK_TAKEN_DIRECT_JUMP = 0x8200, // All macro unconditional taken branch instructions
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken indirect branches that are not calls nor returns
		BR_INST_EXEC_MASK_TAKEN_RETURN_NEAR = 0x8800, // All taken indirect branches that have a return mnemonic
		BR_INST_EXEC_MASK_TAKEN_DIRECT_NEAR_CALL = 0x9000, // All taken non-indirect calls
		BR_INST_EXEC_MASK_TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken indirect calls
		BR_INST_EXEC_MASK_ALL_BRANCHES = 0xff00, // All near executed branches instructions (not necessarily retired)
		BR_INST_EXEC_MASK_ALL_CONDITIONAL = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC_MASK_ANY_COND = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC_MASK_ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All indirect branches that are not calls nor returns
		BR_INST_EXEC_MASK_ANY_DIRECT_NEAR_CALL = 0xd000, // All non-indirect calls
		BR_INST_EXEC_MASK_ALL_DIRECT_JMP = 0xc200, // Speculative and retired macro-unconditional branches excluding calls and indirects
		BR_INST_EXEC_MASK_ALL_INDIRECT_NEAR_RETURN = 0xc800, // Speculative and retired indirect return branches
		BR_INST_RETIRED = 0xc4, // Retired branch instructions
		BR_INST_RETIRED_MASK_ALL_BRANCHES = 0x400, // All taken and not taken macro branches including far branches (Precise Event)
		BR_INST_RETIRED_MASK_CONDITIONAL = 0x100, // All taken and not taken macro conditional branch instructions (Precise Event)
		BR_INST_RETIRED_MASK_FAR_BRANCH = 0x4000, // Number of far branch instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_NEAR_CALL = 0x200, // All macro direct and indirect near calls
		BR_INST_RETIRED_MASK_NEAR_RETURN = 0x800, // Number of near ret instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_NEAR_TAKEN = 0x2000, // Number of near branch taken instructions retired (Precise Event)
		BR_INST_RETIRED_MASK_NOT_TAKEN = 0x1000, // All not taken macro branch instructions retired (Precise Event)
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC_MASK_NONTAKEN_COND = 0x4100, // All non-taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC_MASK_TAKEN_COND = 0x8100, // All taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC_MASK_TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC_MASK_TAKEN_RETURN_NEAR = 0x8800, // All taken mispredicted indirect branches that have a return mnemonic
		BR_MISP_EXEC_MASK_TAKEN_DIRECT_NEAR_CALL = 0x9000, // All taken mispredicted non-indirect calls
		BR_MISP_EXEC_MASK_TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken mispredicted indirect calls
		BR_MISP_EXEC_MASK_ANY_COND = 0xc100, // All mispredicted macro conditional branch instructions
		BR_MISP_EXEC_MASK_ANY_DIRECT_NEAR_CALL = 0xd000, // All mispredicted non-indirect calls
		BR_MISP_EXEC_MASK_ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC_MASK_ALL_BRANCHES = 0xff00, // All mispredicted branch instructions
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches
		BR_MISP_RETIRED_MASK_ALL_BRANCHES = 0x400, // All mispredicted macro branches (Precise Event)
		BR_MISP_RETIRED_MASK_CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions (Precise Event)
		BR_MISP_RETIRED_MASK_NEAR_CALL = 0x200, // All macro direct and indirect near calls (Precise Event)
		BR_MISP_RETIRED_MASK_NOT_TAKEN = 0x1000, // Number of branch instructions retired that were mispredicted and not-taken (Precise Event)
		BR_MISP_RETIRED_MASK_TAKEN = 0x2000, // Number of branch instructions retired that were mispredicted and taken (Precise Event)
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES_MASK_SPLIT_LOCK_UC_LOCK_DURATION = 0x100, // Cycles in which the L1D and L2 are locked
		LOCK_CYCLES_MASK_CACHE_LOCK_DURATION = 0x200, // Cycles in which the L1D is locked
		CPL_CYCLES = 0x5c, // Unhalted core cycles at a specific ring level
		CPL_CYCLES_MASK_RING0 = 0x100, // Unhalted core cycles the thread was in ring 0
		CPL_CYCLES_MASK_RING0_TRANS = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Transitions from rings 1
		CPL_CYCLES_MASK_RING123 = 0x200, // Unhalted core cycles the thread was in rings 1
		CPU_CLK_UNHALTED = 0x3c, // Cycles when processor is not in halted state
		CPU_CLK_UNHALTED_MASK_REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_UNHALTED_MASK_REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_UNHALTED_MASK_REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses (100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_UNHALTED_MASK_THREAD_P = 0x0, // Cycles when thread is not halted
		CPU_CLK_UNHALTED_MASK_ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES_MASK_COUNT = 0x100, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES_MASK_PENALTY_CYCLES = 0x200, // Cycles SB to MITE switches caused delay
		DSB_FILL = 0xac, // DSB fills
		DSB_FILL_MASK_ALL_CANCEL = 0xa00, // Number of times a valid DSB fill has been cancelled for any reason
		DSB_FILL_MASK_EXCEED_DSB_LINES = 0x800, // DSB Fill encountered > 3 DSB lines
		DSB_FILL_MASK_OTHER_CANCEL = 0x200, // Number of times a valid DSB fill has been cancelled not because of exceeding way limit
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Demand load miss in all TLB levels which causes an page walk of any page size
		DTLB_LOAD_MISSES_MASK_CAUSES_A_WALK = 0x100, // Demand load miss in all TLB levels which causes an page walk of any page size
		DTLB_LOAD_MISSES_MASK_STLB_HIT = 0x1000, // Number of DTLB lookups for loads which missed first level DTLB but hit second level DTLB (STLB); No page walk.
		DTLB_LOAD_MISSES_MASK_WALK_COMPLETED = 0x200, // Demand load miss in all TLB levels which causes a page walk that completes for any page size
		DTLB_LOAD_MISSES_MASK_WALK_DURATION = 0x400, // Cycles PMH is busy with a walk
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES_MASK_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES_MASK_STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		DTLB_STORE_MISSES_MASK_WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES_MASK_WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		FP_ASSIST = 0xca, // X87 Floating point assists
		FP_ASSIST_MASK_ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SSE or FP assists
		FP_ASSIST_MASK_SIMD_INPUT = 0x1000, // Number of SIMD FP assists due to input values
		FP_ASSIST_MASK_SIMD_OUTPUT = 0x800, // Number of SIMD FP assists due to output values
		FP_ASSIST_MASK_X87_INPUT = 0x400, // Number of X87 assists due to input value
		FP_ASSIST_MASK_X87_OUTPUT = 0x200, // Number of X87 assists due to output value
		FP_ASSIST_MASK_ALL = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input and output SSE or FP assist
		FP_COMP_OPS_EXE = 0x10, // Counts number of floating point events
		FP_COMP_OPS_EXE_MASK_X87 = 0x100, // Number of X87 uops executed
		FP_COMP_OPS_EXE_MASK_SSE_FP_PACKED_DOUBLE = 0x1000, // Number of SSE double precision FP packed uops executed
		FP_COMP_OPS_EXE_MASK_SSE_FP_SCALAR_SINGLE = 0x2000, // Number of SSE single precision FP scalar uops executed
		FP_COMP_OPS_EXE_MASK_SSE_PACKED_SINGLE = 0x4000, // Number of SSE single precision FP packed uops executed
		FP_COMP_OPS_EXE_MASK_SSE_SCALAR_DOUBLE = 0x8000, // Number of SSE double precision FP scalar uops executed
		HW_PRE_REQ = 0x4e, // Hardware prefetch requests
		HW_PRE_REQ_MASK_L1D_MISS = 0x200, // Hardware prefetch requests that misses the L1D cache. A request is counted each time it accesses the cache and misses it
		ICACHE = 0x80, // Instruction Cache accesses
		ICACHE_MASK_MISSES = 0x200, // Number of Instruction Cache
		ICACHE_MASK_HIT = 0x100, // Number of Instruction Cache
		IDQ = 0x79, // IDQ operations
		IDQ_MASK_EMPTY = 0x200, // Cycles IDQ is empty
		IDQ_MASK_MITE_UOPS = 0x400, // Number of uops delivered to IDQ from MITE path
		IDQ_MASK_DSB_UOPS = 0x800, // Number of uops delivered to IDQ from DSB path
		IDQ_MASK_MS_DSB_UOPS = 0x1000, // Number of uops delivered to IDQ when MS busy by DSB
		IDQ_MASK_MS_MITE_UOPS = 0x2000, // Number of uops delivered to IDQ when MS busy by MITE
		IDQ_MASK_MS_UOPS = 0x3000, // Number of uops were delivered to IDQ from MS by either DSB or MITE
		IDQ_MASK_MITE_UOPS_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from MITE (MITE active)
		IDQ_MASK_MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ_MASK_DSB_UOPS_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from DSB (DSB active)
		IDQ_MASK_MS_DSB_UOPS_CYCLES = 0x1000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by DSB
		IDQ_MASK_MS_MITE_UOPS_CYCLES = 0x2000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by MITE
		IDQ_MASK_MS_UOPS_CYCLES = 0x3000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ from MS by either BSD or MITE
		IDQ_MASK_ALL_DSB_UOPS = 0x1800, // Number of uops deliver from either DSB paths
		IDQ_MASK_ALL_DSB_CYCLES = 0x1800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles MITE/MS deliver anything
		IDQ_MASK_ALL_DSB_CYCLES_4_UOPS = 0x1800 | (4 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering 4 Uops
		IDQ_MASK_ALL_MITE_UOPS = 0x2400, // Number of uops delivered from either MITE paths
		IDQ_MASK_ALL_MITE_CYCLES = 0x2400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles DSB/MS deliver anything
		IDQ_MASK_ALL_MITE_CYCLES_4_UOPS = 0x2400 | (4 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering 4 Uops
		IDQ_MASK_ANY_UOPS = 0x3c00, // Number of uops delivered to IDQ from any path
		IDQ_MASK_MS_DSB_UOPS_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of DSB MS going active
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED_MASK_CORE = 0x100, // Number of non-delivered uops to RAT (use cmask to qualify further)
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles per thread when 4 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_GE_1_UOP_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_INV, // Cycles per thread when 1 or more uops are delivered to the Resource Allocation Table (RAT) by the front end
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_1_UOP_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles per thread when 3 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_2_UOP_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles with less than 2 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_LE_3_UOP_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with less than 3 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED_MASK_CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles Front-End (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling FE
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL_MASK_LCP = 0x100, // Stall caused by changing prefix length of the instruction
		ILD_STALL_MASK_IQ_FULL = 0x400, // Stall cycles due to IQ full
		INSTS_WRITTEN_TO_IQ = 0x17, // Instructions written to IQ
		INSTS_WRITTEN_TO_IQ_MASK_INSTS = 0x100, // Number of instructions written to IQ every cycle
		INST_RETIRED = 0xc0, // Instructions retired
		INST_RETIRED_MASK_ANY_P = 0x0, // Number of instructions retired
		INST_RETIRED_MASK_PREC_DIST = 0x100, // Precise instruction retired event to reduce effect of PEBS shadow IP distribution (Precise Event)
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		INT_MISC = 0xd, // Miscellaneous internals
		INT_MISC_MASK_RAT_STALL_CYCLES = 0x4000, // Cycles RAT external stall is sent to IDQ for this thread
		INT_MISC_MASK_RECOVERY_CYCLES = 0x300 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles waiting to be recovered after Machine Clears due to all other cases except JEClear
		INT_MISC_MASK_RECOVERY_STALLS_COUNT = 0x300 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Number of times need to wait after Machine Clears due to all other cases except JEClear
		INT_MISC_MASK_RECOVERY_CYCLES_ANY = 0x300 | (0x1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles during which the allocator was stalled due to recovery from earlier clear event for any thread (e.g. misprediction or memory nuke)
		ITLB = 0xae, // Instruction TLB
		ITLB_MASK_ITLB_FLUSH = 0x100, // Number of ITLB flushes
		ITLB_MASK_FLUSH = 0x100, // Number of ITLB flushes
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES_MASK_MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES_MASK_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES_MASK_STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		ITLB_MISSES_MASK_WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		ITLB_MISSES_MASK_WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		L1D = 0x51, // L1D cache
		L1D_MASK_ALLOCATED_IN_M = 0x200, // Number of allocations of L1D cache lines in modified (M) state
		L1D_MASK_ALL_M_REPLACEMENT = 0x800, // Number of cache lines in M-state evicted of L1D due to snoop HITM or dirty line replacement
		L1D_MASK_M_EVICT = 0x400, // Number of modified lines evicted from L1D due to replacement
		L1D_MASK_REPLACEMENT = 0x100, // Number of cache lines brought into the L1D cache
		L1D_BLOCKS = 0xbf, // L1D is blocking
		L1D_BLOCKS_MASK_BANK_CONFLICT = 0x500, // Number of dispatched loads cancelled due to L1D bank conflicts with other load ports
		L1D_BLOCKS_MASK_BANK_CONFLICT_CYCLES = 0x500 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when dispatched loads are cancelled due to L1D bank conflicts with other load ports
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS_MASK_OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS_MASK_EDGE = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS_MASK_PENDING = 0x100, // Number of L1D load misses outstanding every cycle
		L1D_PEND_MISS_MASK_PENDING_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load misses outstanding
		L1D_PEND_MISS_MASK_PENDING_CYCLES_ANY = 0x100 | (0x1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles with L1D load misses outstanding from any thread
		L1D_PEND_MISS_MASK_FB_FULL = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles a demand request was blocked due to Fill Buffer (FB) unavailability
		L2_L1D_WB_RQSTS = 0x28, // Writeback requests from L1D to L2
		L2_L1D_WB_RQSTS_MASK_ALL = 0xf00, // Non rejected writebacks from L1D to L2 cache lines in E state
		L2_L1D_WB_RQSTS_MASK_HIT_E = 0x400, // Non rejected writebacks from L1D to L2 cache lines in E state
		L2_L1D_WB_RQSTS_MASK_HIT_M = 0x800, // Non rejected writebacks from L1D to L2 cache lines in M state
		L2_L1D_WB_RQSTS_MASK_HIT_S = 0x200, // Non rejected writebacks from L1D to L2 cache lines in S state
		L2_L1D_WB_RQSTS_MASK_MISS = 0x100, // Number of modified lines evicted from L1 and missing L2 (non-rejected WB from DCU)
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN_MASK_ANY = 0x700, // L2 cache lines filling (counting does not cover rejects)
		L2_LINES_IN_MASK_E = 0x400, // L2 cache lines in E state (counting does not cover rejects)
		L2_LINES_IN_MASK_I = 0x100, // L2 cache lines in I state (counting does not cover rejects)
		L2_LINES_IN_MASK_S = 0x200, // L2 cache lines in S state (counting does not cover rejects)
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT_MASK_DEMAND_CLEAN = 0x100, // L2 clean line evicted by a demand
		L2_LINES_OUT_MASK_DEMAND_DIRTY = 0x200, // L2 dirty line evicted by a demand
		L2_LINES_OUT_MASK_PREFETCH_CLEAN = 0x400, // L2 clean line evicted by a prefetch
		L2_LINES_OUT_MASK_PREFETCH_DIRTY = 0x800, // L2 dirty line evicted by an MLC Prefetch
		L2_LINES_OUT_MASK_DIRTY_ANY = 0xa00, // Any L2 dirty line evicted (does not cover rejects)
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS_MASK_ALL_CODE_RD = 0x3000, // Any ifetch request to L2 cache
		L2_RQSTS_MASK_CODE_RD_HIT = 0x1000, // L2 cache hits when fetching instructions
		L2_RQSTS_MASK_CODE_RD_MISS = 0x2000, // L2 cache misses when fetching instructions
		L2_RQSTS_MASK_ALL_DEMAND_DATA_RD = 0x300, // Demand  data read requests to L2 cache
		L2_RQSTS_MASK_ALL_DEMAND_RD_HIT = 0x100, // Demand data read requests that hit L2
		L2_RQSTS_MASK_ALL_PF = 0xc000, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS_MASK_PF_HIT = 0x4000, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS_MASK_PF_MISS = 0x8000, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS_MASK_RFO_ANY = 0xc00, // Any RFO requests to L2 cache
		L2_RQSTS_MASK_RFO_HITS = 0x400, // RFO requests that hit L2 cache
		L2_RQSTS_MASK_RFO_MISS = 0x800, // RFO requests that miss L2 cache
		L2_STORE_LOCK_RQSTS = 0x27, // L2 store lock requests
		L2_STORE_LOCK_RQSTS_MASK_HIT_E = 0x400, // RFOs that hit cache lines in E state
		L2_STORE_LOCK_RQSTS_MASK_MISS = 0x100, // RFOs that miss cache (I state)
		L2_STORE_LOCK_RQSTS_MASK_HIT_M = 0x800, // RFOs that hit cache lines in M state
		L2_STORE_LOCK_RQSTS_MASK_ALL = 0xf00, // RFOs that access cache lines in any state
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS_MASK_ALL = 0x8000, // Transactions accessing MLC pipe
		L2_TRANS_MASK_CODE_RD = 0x400, // L2 cache accesses when fetching instructions
		L2_TRANS_MASK_L1D_WB = 0x1000, // L1D writebacks that access L2 cache
		L2_TRANS_MASK_LOAD = 0x100, // Demand Data Read* requests that access L2 cache
		L2_TRANS_MASK_L2_FILL = 0x2000, // L2 fill requests that access L2 cache
		L2_TRANS_MASK_L2_WB = 0x4000, // L2 writebacks that access L2 cache
		L2_TRANS_MASK_ALL_PREFETCH = 0x800, // L2 or L3 HW prefetches that access L2 cache (including rejects)
		L2_TRANS_MASK_RFO = 0x200, // RFO requests that access L2 cache
		LAST_LEVEL_CACHE_MISSES = 0x412e, // This is an alias for L3_LAT_CACHE:MISS
		LLC_MISSES = 0x412e, // Alias for LAST_LEVEL_CACHE_MISSES
		LAST_LEVEL_CACHE_REFERENCES = 0x4f2e, // This is an alias for L3_LAT_CACHE:REFERENCE
		LLC_REFERENCES = 0x4f2e, // Alias for LAST_LEVEL_CACHE_REFERENCES
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS_MASK_DATA_UNKNOWN = 0x100, // Blocked loads due to store buffer blocks with unknown data
		LD_BLOCKS_MASK_STORE_FORWARD = 0x200, // Loads blocked by overlapping with store buffer that cannot be forwarded
		LD_BLOCKS_MASK_NO_SR = 0x800, // Number of split loads blocked due to resource not available
		LD_BLOCKS_MASK_ALL_BLOCK = 0x1000, // Number of cases where any load is blocked but has not DCU miss
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL_MASK_ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LD_BLOCKS_PARTIAL_MASK_ALL_STA_BLOCK = 0x800, // Number of times that load operations are temporarily blocked because of older stores
		LOAD_HIT_PRE = 0x4c, // Load dispatches that hit fill buffer
		LOAD_HIT_PRE_MASK_HW_PF = 0x200, // Non sw-prefetch load dispatches that hit the fill buffer allocated for HW prefetch
		LOAD_HIT_PRE_MASK_SW_PF = 0x100, // Non sw-prefetch load dispatches that hit the fill buffer allocated for SW prefetch
		L3_LAT_CACHE = 0x2e, // Core-originated cacheable demand requests to L3
		L3_LAT_CACHE_MASK_MISS = 0x100, // Core-originated cacheable demand requests missed L3
		L3_LAT_CACHE_MASK_REFERENCE = 0x200, // Core-originated cacheable demand requests that refer to L3
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS_MASK_MASKMOV = 0x2000, // The number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0
		MACHINE_CLEARS_MASK_MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS_MASK_SMC = 0x400, // Self-Modifying Code detected
		MACHINE_CLEARS_MASK_COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of machine clears (nukes) of any type
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED_MASK_XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired (deprecated use MEM_LOAD_UOPS_LLC_HIT_RETIRED)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED_MASK_XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_MISC_RETIRED = 0xd4, // Loads and some non simd split loads uops retired
		MEM_LOAD_UOPS_MISC_RETIRED_MASK_LLC_MISS = 0x200, // Counts load driven L3 misses and some non simd split loads (Precise Event)
		MEM_LOAD_MISC_RETIRED = 0xd4, // Loads and some non simd split loads uops retired (deprecated use MEM_LOAD_UOPS_MISC_RETIRED)
		MEM_LOAD_MISC_RETIRED_MASK_LLC_MISS = 0x200, // Counts load driven L3 misses and some non simd split loads (Precise Event)
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Memory loads uops retired
		MEM_LOAD_UOPS_RETIRED_MASK_HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_RETIRED_MASK_L3_MISS = 0x2000, // Retired load uops which data sources were data missed LLC (excluding unknown data source)
		MEM_LOAD_RETIRED = 0xd1, // Memory loads uops retired (deprecated use MEM_LOAD_UOPS_RETIRED)
		MEM_LOAD_RETIRED_MASK_HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_RETIRED_MASK_L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_RETIRED_MASK_L3_MISS = 0x2000, // Retired load uops which data sources were data missed LLC (excluding unknown data source)
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired
		MEM_TRANS_RETIRED_MASK_LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED_MASK_PRECISE_STORE = 0x200, // Capture where stores occur
		MEM_UOPS_RETIRED = 0xd0, // Memory uops retired
		MEM_UOPS_RETIRED_MASK_ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED_MASK_ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED_MASK_LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_LOCK_STORES = 0x2200, // Locked retired stores (Precise Event)
		MEM_UOPS_RETIRED_MASK_SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED_MASK_SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED_MASK_STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOPS_RETIRED_MASK_STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MEM_UOP_RETIRED = 0xd0, // Memory uops retired (deprecated use MEM_UOPS_RETIRED)
		MEM_UOP_RETIRED_MASK_ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED_MASK_ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED_MASK_LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_LOCK_STORES = 0x2200, // Locked retired stores (Precise Event)
		MEM_UOP_RETIRED_MASK_SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED_MASK_SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED_MASK_STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOP_RETIRED_MASK_STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF_MASK_LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF_MASK_STORES = 0x200, // Speculative cache-line split Store-address uops dispatched to L1D
		OFFCORE_REQUESTS = 0xb0, // Offcore requests
		OFFCORE_REQUESTS_MASK_ALL_DATA_RD = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS_MASK_ALL_DATA_READ = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS_MASK_DEMAND_CODE_RD = 0x200, // Offcore code read requests
		OFFCORE_REQUESTS_MASK_DEMAND_DATA_RD = 0x100, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS_MASK_DEMAND_RFO = 0x400, // Offcore Demand RFOs
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore requests buffer
		OFFCORE_REQUESTS_BUFFER_MASK_SQ_FULL = 0x100, // Offcore requests buffer cannot take more entries for this thread core
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING_MASK_ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING_MASK_ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING_MASK_DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ
		OTHER_ASSISTS = 0xc1, // Count hardware assists
		OTHER_ASSISTS_MASK_ITLB_MISS_RETIRED = 0x200, // Number of instructions that experienced an ITLB miss
		OTHER_ASSISTS_MASK_AVX_TO_SSE = 0x1000, // Number of transitions from AVX-256 to legacy SSE when penalty applicable
		OTHER_ASSISTS_MASK_SSE_TO_AVX = 0x2000, // Number of transitions from legacy SSE to AVX-256 when penalty applicable
		OTHER_ASSISTS_MASK_AVX_STORE = 0x800, // Number of GSSE memory assist for stores. GSSE microcode assist is being invoked whenever the hardware is unable to properly handle GSSE-256b operations
		PARTIAL_RAT_STALLS = 0x59, // Partial Register Allocation Table stalls
		PARTIAL_RAT_STALLS_MASK_FLAGS_MERGE_UOP = 0x2000, // Number of flags-merge uops in flight in each cycle
		PARTIAL_RAT_STALLS_MASK_CYCLES_FLAGS_MERGE_UOP = 0x2000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles in which flags-merge uops in flight
		PARTIAL_RAT_STALLS_MASK_MUL_SINGLE_UOP = 0x8000, // Number of Multiply packed/scalar single precision uops allocated
		PARTIAL_RAT_STALLS_MASK_SLOW_LEA_WINDOW = 0x4000, // Number of cycles with at least one slow LEA uop allocated
		RESOURCE_STALLS = 0xa2, // Resource related stall cycles
		RESOURCE_STALLS_MASK_ANY = 0x100, // Cycles stalled due to Resource Related reason
		RESOURCE_STALLS_MASK_LB = 0x200, // Cycles stalled due to lack of load buffers
		RESOURCE_STALLS_MASK_RS = 0x400, // Cycles stalled due to no eligible RS entry available
		RESOURCE_STALLS_MASK_SB = 0x800, // Cycles stalled due to no store buffers available (not including draining from sync)
		RESOURCE_STALLS_MASK_ROB = 0x1000, // Cycles stalled due to re-order buffer full
		RESOURCE_STALLS_MASK_FCSW = 0x2000, // Cycles stalled due to writing the FPU control word
		RESOURCE_STALLS_MASK_MXCSR = 0x4000, // Cycles stalled due to the MXCSR register ranme occurring too close to a previous MXCSR rename
		RESOURCE_STALLS_MASK_MEM_RS = 0xe00, // Cycles stalled due to LB
		RESOURCE_STALLS_MASK_LD_SB = 0xa00, // Resource stalls due to load or store buffers all being in use
		RESOURCE_STALLS_MASK_OOO_SRC = 0xf000, // Resource stalls due to Rob being full
		RESOURCE_STALLS2 = 0x5b, // Resource related stall cycles
		RESOURCE_STALLS2_MASK_ALL_FL_EMPTY = 0xc00, // Cycles stalled due to free list empty
		RESOURCE_STALLS2_MASK_ALL_PRF_CONTROL = 0xf00, // Cycles stalls due to control structures full for physical registers
		RESOURCE_STALLS2_MASK_ANY_PRF_CONTROL = 0xf00, // Cycles stalls due to control structures full for physical registers
		RESOURCE_STALLS2_MASK_BOB_FULL = 0x4000, // Cycles Allocator is stalled due Branch Order Buffer
		RESOURCE_STALLS2_MASK_OOO_RSRC = 0x4f00, // Cycles stalled due to out of order resources full
		ROB_MISC_EVENTS = 0xcc, // Reorder buffer events
		ROB_MISC_EVENTS_MASK_LBR_INSERTS = 0x2000, // Count each time an new LBR record is saved by HW
		RS_EVENTS = 0x5e, // Reservation station events
		RS_EVENTS_MASK_EMPTY_CYCLES = 0x100, // Cycles the RS is empty for this thread
		RS_EVENTS_MASK_EMPTY_END = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Counts number of time the Reservation Station (RS) goes from empty to non-empty
		SIMD_FP_256 = 0x11, // Counts 256-bit packed floating point instructions
		SIMD_FP_256_MASK_PACKED_SINGLE = 0x100, // Counts 256-bit packed single-precision
		SIMD_FP_256_MASK_PACKED_DOUBLE = 0x200, // Counts 256-bit packed double-precision
		SQ_MISC = 0xf4, // SuperQ events
		SQ_MISC_MASK_SPLIT_LOCK = 0x1000, // Split locks in SQ
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH_MASK_DTLB_THREAD = 0x100, // Number of DTLB flushes of thread-specific entries
		TLB_FLUSH_MASK_STLB_ANY = 0x2000, // Number of STLB flushes
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED_MASK_CORE = 0x200, // Counts total number of uops executed from any thread per cycle
		UOPS_EXECUTED_MASK_THREAD = 0x100, // Counts total number of uops executed per thread each cycle
		UOPS_EXECUTED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed
		UOPS_EXECUTED_MASK_CYCLES_GE_1_UOP_EXEC = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_2_UOPS_EXEC = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_3_UOPS_EXEC = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per thread
		UOPS_EXECUTED_MASK_CYCLES_GE_4_UOPS_EXEC = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_1 = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_2 = 0x200 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_3 = 0x200 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_GE_4 = 0x200 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed from any thread
		UOPS_EXECUTED_MASK_CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV, // Cycles where no uop is executed on any thread
		UOPS_DISPATCHED_PORT = 0xa1, // Uops dispatch to specific ports
		UOPS_DISPATCHED_PORT_MASK_PORT_0 = 0x100, // Cycles which a Uop is dispatched on port 0
		UOPS_DISPATCHED_PORT_MASK_PORT_1 = 0x200, // Cycles which a Uop is dispatched on port 1
		UOPS_DISPATCHED_PORT_MASK_PORT_2_LD = 0x400, // Cycles in which a load uop is dispatched on port 2
		UOPS_DISPATCHED_PORT_MASK_PORT_2_STA = 0x800, // Cycles in which a store uop is dispatched on port 2
		UOPS_DISPATCHED_PORT_MASK_PORT_2 = 0xc00, // Cycles in which a uop is dispatched on port 2
		UOPS_DISPATCHED_PORT_MASK_PORT_3 = 0x3000, // Cycles in which a uop is dispatched on port 3
		UOPS_DISPATCHED_PORT_MASK_PORT_4 = 0x4000, // Cycles which a uop is dispatched on port 4
		UOPS_DISPATCHED_PORT_MASK_PORT_5 = 0x8000, // Cycles which a Uop is dispatched on port 5
		UOPS_DISPATCHED_PORT_MASK_PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 0 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 1 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_2_CORE = 0xc00 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 2 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_3_CORE = 0x3000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 3 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_4_CORE = 0x4000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 4 for any thread
		UOPS_DISPATCHED_PORT_MASK_PORT_5_CORE = 0x8000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 5 for any thread
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED_MASK_ANY = 0x100, // Number of uops issued by the RAT to the Reservation Station (RS)
		UOPS_ISSUED_MASK_CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued on this core (by any thread)
		UOPS_ISSUED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued by this thread
		UOPS_RETIRED = 0xc2, // Uops retired
		UOPS_RETIRED_MASK_ALL = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED_MASK_ANY = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED_MASK_RETIRE_SLOTS = 0x200, // Number of retirement slots used (Precise Event)
		UOPS_RETIRED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no executable uop retired (Precise Event)
		UOPS_RETIRED_MASK_TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Total cycles using precise uop retired event (Precise Event)
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY_MASK_CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss loads
		CYCLE_ACTIVITY_MASK_CYCLES_L1D_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY_MASK_CYCLES_NO_DISPATCH = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles of dispatch stalls
		CYCLE_ACTIVITY_MASK_STALLS_L2_PENDING = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls due to L2 pending loads
		CYCLE_ACTIVITY_MASK_STALLS_L1D_PENDING = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls due to L1D pending loads
		EPT = 0x4f, // Extended page table
		EPT_MASK_WALK_CYCLES = 0x1000, // Cycles for an extended page table walk
		LSD = 0xa8, // Loop stream detector
		LSD_MASK_UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD_MASK_ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with uops delivered by the LSD but which did not come from decoder
		LSD_MASK_CYCLES_4_UOPS = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles with 4 uops delivered by the LSD but which did not come from decoder
		PAGE_WALKS = 0xbe, // page walker
		PAGE_WALKS_MASK_LLC_MISS = 0x100, // Number of page walks with a LLC miss
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // Load uops retired which miss the L3 cache
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_LOCAL_DRAM = 0x100, // Load uops that miss in the L3 and hit local DRAM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED_MASK_REMOTE_DRAM = 0x400, // Load uops that miss in the L3 and hit remote DRAM
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0_MASK_DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0_MASK_DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0_MASK_DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0_MASK_WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_0_MASK_PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_0_MASK_PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_0_MASK_PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0_MASK_BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_0_MASK_STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_0_MASK_OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0_MASK_ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_0_MASK_ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0_MASK_ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_0_MASK_ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_0_MASK_ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0_MASK_NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0_MASK_LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0_MASK_LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0_MASK_LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0_MASK_LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_LOCAL_DRAM = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0_MASK_L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_0_MASK_LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0_MASK_SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0_MASK_SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0_MASK_NO_SNP_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0_MASK_SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0_MASK_SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0_MASK_SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0_MASK_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0_MASK_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0_MASK_SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1_MASK_DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1_MASK_DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1_MASK_DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1_MASK_WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_1_MASK_PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1_MASK_PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_1_MASK_PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_1_MASK_PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1_MASK_BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_1_MASK_STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_1_MASK_OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1_MASK_ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_1_MASK_ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1_MASK_ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_1_MASK_ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_1_MASK_ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1_MASK_NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1_MASK_LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1_MASK_LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1_MASK_LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1_MASK_LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_LOCAL_DRAM = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1_MASK_L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_1_MASK_LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1_MASK_SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1_MASK_SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1_MASK_NO_SNP_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1_MASK_SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1_MASK_SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1_MASK_SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1_MASK_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1_MASK_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1_MASK_SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};