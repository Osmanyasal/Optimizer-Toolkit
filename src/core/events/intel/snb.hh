#include <cstdint>
#include <intel_priv.hh>
namespace optkit::intel{
	enum class snb : uint64_t {
		AGU_BYPASS_CANCEL = 0xb6, // Number of executed load operations with all the following traits: 1. addressing of the format [base + offset]
		AGU_BYPASS_CANCEL__MASK__SNB_AGU_BYPASS_CANCEL__COUNT = 0x100, // This event counts executed load operations
		ARITH = 0x14, // Counts arithmetic multiply operations
		ARITH__MASK__SNB_ARITH__FPU_DIV_ACTIVE = 0x100, // Cycles that the divider is active
		ARITH__MASK__SNB_ARITH__FPU_DIV = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Number of cycles the divider is activated
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS__MASK__SNB_BACLEARS__ANY = 0x1f00, // Counts the number of times the front end is re-steered
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__NONTAKEN_COND = 0x4100, // All macro conditional non-taken branch instructions
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__TAKEN_COND = 0x8100, // All macro conditional taken branch instructions
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__TAKEN_DIRECT_JUMP = 0x8200, // All macro unconditional taken branch instructions
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken indirect branches that are not calls nor returns
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__TAKEN_RETURN_NEAR = 0x8800, // All taken indirect branches that have a return mnemonic
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__TAKEN_DIRECT_NEAR_CALL = 0x9000, // All taken non-indirect calls
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken indirect calls
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ALL_BRANCHES = 0xff00, // All near executed branches instructions (not necessarily retired)
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ALL_CONDITIONAL = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ANY_COND = 0xc100, // All macro conditional branch instructions
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All indirect branches that are not calls nor returns
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ANY_DIRECT_NEAR_CALL = 0xd000, // All non-indirect calls
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ALL_DIRECT_JMP = 0xc200, // Speculative and retired macro-unconditional branches excluding calls and indirects
		BR_INST_EXEC__MASK__SNB_BR_INST_EXEC__ALL_INDIRECT_NEAR_RETURN = 0xc800, // Speculative and retired indirect return branches
		BR_INST_RETIRED = 0xc4, // Retired branch instructions
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__ALL_BRANCHES = 0x400, // All taken and not taken macro branches including far branches (Precise Event)
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__CONDITIONAL = 0x100, // All taken and not taken macro conditional branch instructions (Precise Event)
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__FAR_BRANCH = 0x4000, // Number of far branch instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__NEAR_CALL = 0x200, // All macro direct and indirect near calls
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__NEAR_RETURN = 0x800, // Number of near ret instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__NEAR_TAKEN = 0x2000, // Number of near branch taken instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__SNB_BR_INST_RETIRED__NOT_TAKEN = 0x1000, // All not taken macro branch instructions retired (Precise Event)
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__NONTAKEN_COND = 0x4100, // All non-taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__TAKEN_COND = 0x8100, // All taken mispredicted macro conditional branch instructions
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // All taken mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__TAKEN_RETURN_NEAR = 0x8800, // All taken mispredicted indirect branches that have a return mnemonic
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__TAKEN_DIRECT_NEAR_CALL = 0x9000, // All taken mispredicted non-indirect calls
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All taken mispredicted indirect calls
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__ANY_COND = 0xc100, // All mispredicted macro conditional branch instructions
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__ANY_DIRECT_NEAR_CALL = 0xd000, // All mispredicted non-indirect calls
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__ANY_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC__MASK__SNB_BR_MISP_EXEC__ALL_BRANCHES = 0xff00, // All mispredicted branch instructions
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches
		BR_MISP_RETIRED__MASK__SNB_BR_MISP_RETIRED__ALL_BRANCHES = 0x400, // All mispredicted macro branches (Precise Event)
		BR_MISP_RETIRED__MASK__SNB_BR_MISP_RETIRED__CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions (Precise Event)
		BR_MISP_RETIRED__MASK__SNB_BR_MISP_RETIRED__NEAR_CALL = 0x200, // All macro direct and indirect near calls (Precise Event)
		BR_MISP_RETIRED__MASK__SNB_BR_MISP_RETIRED__NOT_TAKEN = 0x1000, // Number of branch instructions retired that were mispredicted and not-taken (Precise Event)
		BR_MISP_RETIRED__MASK__SNB_BR_MISP_RETIRED__TAKEN = 0x2000, // Number of branch instructions retired that were mispredicted and taken (Precise Event)
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES__MASK__SNB_LOCK_CYCLES__SPLIT_LOCK_UC_LOCK_DURATION = 0x100, // Cycles in which the L1D and L2 are locked
		LOCK_CYCLES__MASK__SNB_LOCK_CYCLES__CACHE_LOCK_DURATION = 0x200, // Cycles in which the L1D is locked
		CPL_CYCLES = 0x5c, // Unhalted core cycles at a specific ring level
		CPL_CYCLES__MASK__SNB_CPL_CYCLES__RING0 = 0x100, // Unhalted core cycles the thread was in ring 0
		CPL_CYCLES__MASK__SNB_CPL_CYCLES__RING0_TRANS = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Transitions from rings 1
		CPL_CYCLES__MASK__SNB_CPL_CYCLES__RING123 = 0x200, // Unhalted core cycles the thread was in rings 1
		CPU_CLK_UNHALTED = 0x3c, // Cycles when processor is not in halted state
		CPU_CLK_UNHALTED__MASK__SNB_CPU_CLK_UNHALTED__REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_UNHALTED__MASK__SNB_CPU_CLK_UNHALTED__REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_UNHALTED__MASK__SNB_CPU_CLK_UNHALTED__REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses (100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_UNHALTED__MASK__SNB_CPU_CLK_UNHALTED__THREAD_P = 0x0, // Cycles when thread is not halted
		CPU_CLK_UNHALTED__MASK__SNB_CPU_CLK_UNHALTED__ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES__MASK__SNB_DSB2MITE_SWITCHES__COUNT = 0x100, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES__MASK__SNB_DSB2MITE_SWITCHES__PENALTY_CYCLES = 0x200, // Cycles SB to MITE switches caused delay
		DSB_FILL = 0xac, // DSB fills
		DSB_FILL__MASK__SNB_DSB_FILL__ALL_CANCEL = 0xa00, // Number of times a valid DSB fill has been cancelled for any reason
		DSB_FILL__MASK__SNB_DSB_FILL__EXCEED_DSB_LINES = 0x800, // DSB Fill encountered > 3 DSB lines
		DSB_FILL__MASK__SNB_DSB_FILL__OTHER_CANCEL = 0x200, // Number of times a valid DSB fill has been cancelled not because of exceeding way limit
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES__MASK__SNB_DTLB_LOAD_MISSES__MISS_CAUSES_A_WALK = 0x100, // Demand load miss in all TLB levels which causes an page walk of any page size
		DTLB_LOAD_MISSES__MASK__SNB_DTLB_LOAD_MISSES__CAUSES_A_WALK = 0x100, // Demand load miss in all TLB levels which causes an page walk of any page size
		DTLB_LOAD_MISSES__MASK__SNB_DTLB_LOAD_MISSES__STLB_HIT = 0x1000, // Number of DTLB lookups for loads which missed first level DTLB but hit second level DTLB (STLB); No page walk.
		DTLB_LOAD_MISSES__MASK__SNB_DTLB_LOAD_MISSES__WALK_COMPLETED = 0x200, // Demand load miss in all TLB levels which causes a page walk that completes for any page size
		DTLB_LOAD_MISSES__MASK__SNB_DTLB_LOAD_MISSES__WALK_DURATION = 0x400, // Cycles PMH is busy with a walk
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES__MASK__SNB_DTLB_STORE_MISSES__MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES__MASK__SNB_DTLB_STORE_MISSES__CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES__MASK__SNB_DTLB_STORE_MISSES__STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		DTLB_STORE_MISSES__MASK__SNB_DTLB_STORE_MISSES__WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		DTLB_STORE_MISSES__MASK__SNB_DTLB_STORE_MISSES__WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		FP_ASSIST = 0xca, // X87 Floating point assists
		FP_ASSIST__MASK__SNB_FP_ASSIST__ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SSE or FP assists
		FP_ASSIST__MASK__SNB_FP_ASSIST__SIMD_INPUT = 0x1000, // Number of SIMD FP assists due to input values
		FP_ASSIST__MASK__SNB_FP_ASSIST__SIMD_OUTPUT = 0x800, // Number of SIMD FP assists due to output values
		FP_ASSIST__MASK__SNB_FP_ASSIST__X87_INPUT = 0x400, // Number of X87 assists due to input value
		FP_ASSIST__MASK__SNB_FP_ASSIST__X87_OUTPUT = 0x200, // Number of X87 assists due to output value
		FP_ASSIST__MASK__SNB_FP_ASSIST__ALL = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input and output SSE or FP assist
		FP_COMP_OPS_EXE = 0x10, // Counts number of floating point events
		FP_COMP_OPS_EXE__MASK__SNB_FP_COMP_OPS_EXE__X87 = 0x100, // Number of X87 uops executed
		FP_COMP_OPS_EXE__MASK__SNB_FP_COMP_OPS_EXE__SSE_FP_PACKED_DOUBLE = 0x1000, // Number of SSE double precision FP packed uops executed
		FP_COMP_OPS_EXE__MASK__SNB_FP_COMP_OPS_EXE__SSE_FP_SCALAR_SINGLE = 0x2000, // Number of SSE single precision FP scalar uops executed
		FP_COMP_OPS_EXE__MASK__SNB_FP_COMP_OPS_EXE__SSE_PACKED_SINGLE = 0x4000, // Number of SSE single precision FP packed uops executed
		FP_COMP_OPS_EXE__MASK__SNB_FP_COMP_OPS_EXE__SSE_SCALAR_DOUBLE = 0x8000, // Number of SSE double precision FP scalar uops executed
		HW_PRE_REQ = 0x4e, // Hardware prefetch requests
		HW_PRE_REQ__MASK__SNB_HW_PRE_REQ__L1D_MISS = 0x200, // Hardware prefetch requests that misses the L1D cache. A request is counted each time it accesses the cache and misses it
		ICACHE = 0x80, // Instruction Cache accesses
		ICACHE__MASK__SNB_ICACHE__MISSES = 0x200, // Number of Instruction Cache
		ICACHE__MASK__SNB_ICACHE__HIT = 0x100, // Number of Instruction Cache
		IDQ = 0x79, // IDQ operations
		IDQ__MASK__SNB_IDQ__EMPTY = 0x200, // Cycles IDQ is empty
		IDQ__MASK__SNB_IDQ__MITE_UOPS = 0x400, // Number of uops delivered to IDQ from MITE path
		IDQ__MASK__SNB_IDQ__DSB_UOPS = 0x800, // Number of uops delivered to IDQ from DSB path
		IDQ__MASK__SNB_IDQ__MS_DSB_UOPS = 0x1000, // Number of uops delivered to IDQ when MS busy by DSB
		IDQ__MASK__SNB_IDQ__MS_MITE_UOPS = 0x2000, // Number of uops delivered to IDQ when MS busy by MITE
		IDQ__MASK__SNB_IDQ__MS_UOPS = 0x3000, // Number of uops were delivered to IDQ from MS by either DSB or MITE
		IDQ__MASK__SNB_IDQ__MITE_UOPS_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from MITE (MITE active)
		IDQ__MASK__SNB_IDQ__MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ__MASK__SNB_IDQ__DSB_UOPS_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops are delivered to IDQ from DSB (DSB active)
		IDQ__MASK__SNB_IDQ__MS_DSB_UOPS_CYCLES = 0x1000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by DSB
		IDQ__MASK__SNB_IDQ__MS_MITE_UOPS_CYCLES = 0x2000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ when MS busy by MITE
		IDQ__MASK__SNB_IDQ__MS_UOPS_CYCLES = 0x3000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where uops delivered to IDQ from MS by either BSD or MITE
		IDQ__MASK__SNB_IDQ__ALL_DSB_UOPS = 0x1800, // Number of uops deliver from either DSB paths
		IDQ__MASK__SNB_IDQ__ALL_DSB_CYCLES = 0x1800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles MITE/MS deliver anything
		IDQ__MASK__SNB_IDQ__ALL_DSB_CYCLES_4_UOPS = 0x1800 | (4 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering 4 Uops
		IDQ__MASK__SNB_IDQ__ALL_MITE_UOPS = 0x2400, // Number of uops delivered from either MITE paths
		IDQ__MASK__SNB_IDQ__ALL_MITE_CYCLES = 0x2400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles DSB/MS deliver anything
		IDQ__MASK__SNB_IDQ__ALL_MITE_CYCLES_4_UOPS = 0x2400 | (4 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering 4 Uops
		IDQ__MASK__SNB_IDQ__ANY_UOPS = 0x3c00, // Number of uops delivered to IDQ from any path
		IDQ__MASK__SNB_IDQ__MS_DSB_UOPS_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of DSB MS going active
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CORE = 0x100, // Number of non-delivered uops to RAT (use cmask to qualify further)
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles per thread when 4 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CYCLES_GE_1_UOP_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_INV, // Cycles per thread when 1 or more uops are delivered to the Resource Allocation Table (RAT) by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_1_UOP_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles per thread when 3 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_2_UOP_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles with less than 2 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_3_UOP_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with less than 3 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__SNB_IDQ_UOPS_NOT_DELIVERED__CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles Front-End (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling FE
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL__MASK__SNB_ILD_STALL__LCP = 0x100, // Stall caused by changing prefix length of the instruction
		ILD_STALL__MASK__SNB_ILD_STALL__IQ_FULL = 0x400, // Stall cycles due to IQ full
		INSTS_WRITTEN_TO_IQ = 0x17, // Instructions written to IQ
		INSTS_WRITTEN_TO_IQ__MASK__SNB_INSTS_WRITTEN_TO_IQ__INSTS = 0x100, // Number of instructions written to IQ every cycle
		INST_RETIRED = 0xc0, // Instructions retired
		INST_RETIRED__MASK__SNB_INST_RETIRED__ANY_P = 0x0, // Number of instructions retired
		INST_RETIRED__MASK__SNB_INST_RETIRED__PREC_DIST = 0x100, // Precise instruction retired event to reduce effect of PEBS shadow IP distribution (Precise Event)
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		INT_MISC = 0xd, // Miscellaneous internals
		INT_MISC__MASK__SNB_INT_MISC__RAT_STALL_CYCLES = 0x4000, // Cycles RAT external stall is sent to IDQ for this thread
		INT_MISC__MASK__SNB_INT_MISC__RECOVERY_CYCLES = 0x300 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles waiting to be recovered after Machine Clears due to all other cases except JEClear
		INT_MISC__MASK__SNB_INT_MISC__RECOVERY_STALLS_COUNT = 0x300 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Number of times need to wait after Machine Clears due to all other cases except JEClear
		INT_MISC__MASK__SNB_INT_MISC__RECOVERY_CYCLES_ANY = 0x300 | (0x1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles during which the allocator was stalled due to recovery from earlier clear event for any thread (e.g. misprediction or memory nuke)
		ITLB = 0xae, // Instruction TLB
		ITLB__MASK__SNB_ITLB__ITLB_FLUSH = 0x100, // Number of ITLB flushes
		ITLB__MASK__SNB_ITLB__FLUSH = 0x100, // Number of ITLB flushes
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES__MASK__SNB_DTLB_STORE_MISSES__MISS_CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES__MASK__SNB_DTLB_STORE_MISSES__CAUSES_A_WALK = 0x100, // Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
		ITLB_MISSES__MASK__SNB_DTLB_STORE_MISSES__STLB_HIT = 0x1000, // First level miss but second level hit; no page walk. Only relevant if multiple levels
		ITLB_MISSES__MASK__SNB_DTLB_STORE_MISSES__WALK_COMPLETED = 0x200, // Miss in all TLB levels that causes a page walk that completes of any page size (4K/2M/4M/1G)
		ITLB_MISSES__MASK__SNB_DTLB_STORE_MISSES__WALK_DURATION = 0x400, // Cycles PMH is busy with this walk
		L1D = 0x51, // L1D cache
		L1D__MASK__SNB_L1D__ALLOCATED_IN_M = 0x200, // Number of allocations of L1D cache lines in modified (M) state
		L1D__MASK__SNB_L1D__ALL_M_REPLACEMENT = 0x800, // Number of cache lines in M-state evicted of L1D due to snoop HITM or dirty line replacement
		L1D__MASK__SNB_L1D__M_EVICT = 0x400, // Number of modified lines evicted from L1D due to replacement
		L1D__MASK__SNB_L1D__REPLACEMENT = 0x100, // Number of cache lines brought into the L1D cache
		L1D_BLOCKS = 0xbf, // L1D is blocking
		L1D_BLOCKS__MASK__SNB_L1D_BLOCKS__BANK_CONFLICT = 0x500, // Number of dispatched loads cancelled due to L1D bank conflicts with other load ports
		L1D_BLOCKS__MASK__SNB_L1D_BLOCKS__BANK_CONFLICT_CYCLES = 0x500 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when dispatched loads are cancelled due to L1D bank conflicts with other load ports
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS__MASK__SNB_L1D_PEND_MISS__OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS__MASK__SNB_L1D_PEND_MISS__EDGE = 0x100 | INTEL_X86_MOD_EDGE | (0x1 << INTEL_X86_CMASK_BIT), // Occurrences of L1D_PEND_MISS going active
		L1D_PEND_MISS__MASK__SNB_L1D_PEND_MISS__PENDING = 0x100, // Number of L1D load misses outstanding every cycle
		L1D_PEND_MISS__MASK__SNB_L1D_PEND_MISS__PENDING_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load misses outstanding
		L1D_PEND_MISS__MASK__SNB_L1D_PEND_MISS__PENDING_CYCLES_ANY = 0x100 | (0x1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles with L1D load misses outstanding from any thread
		L1D_PEND_MISS__MASK__SNB_L1D_PEND_MISS__FB_FULL = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles a demand request was blocked due to Fill Buffer (FB) unavailability
		L2_L1D_WB_RQSTS = 0x28, // Writeback requests from L1D to L2
		L2_L1D_WB_RQSTS__MASK__SNB_L2_L1D_WB_RQSTS__ALL = 0xf00, // Non rejected writebacks from L1D to L2 cache lines in E state
		L2_L1D_WB_RQSTS__MASK__SNB_L2_L1D_WB_RQSTS__HIT_E = 0x400, // Non rejected writebacks from L1D to L2 cache lines in E state
		L2_L1D_WB_RQSTS__MASK__SNB_L2_L1D_WB_RQSTS__HIT_M = 0x800, // Non rejected writebacks from L1D to L2 cache lines in M state
		L2_L1D_WB_RQSTS__MASK__SNB_L2_L1D_WB_RQSTS__HIT_S = 0x200, // Non rejected writebacks from L1D to L2 cache lines in S state
		L2_L1D_WB_RQSTS__MASK__SNB_L2_L1D_WB_RQSTS__MISS = 0x100, // Number of modified lines evicted from L1 and missing L2 (non-rejected WB from DCU)
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN__MASK__SNB_L2_LINES_IN__ANY = 0x700, // L2 cache lines filling (counting does not cover rejects)
		L2_LINES_IN__MASK__SNB_L2_LINES_IN__E = 0x400, // L2 cache lines in E state (counting does not cover rejects)
		L2_LINES_IN__MASK__SNB_L2_LINES_IN__I = 0x100, // L2 cache lines in I state (counting does not cover rejects)
		L2_LINES_IN__MASK__SNB_L2_LINES_IN__S = 0x200, // L2 cache lines in S state (counting does not cover rejects)
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT__MASK__SNB_L2_LINES_OUT__DEMAND_CLEAN = 0x100, // L2 clean line evicted by a demand
		L2_LINES_OUT__MASK__SNB_L2_LINES_OUT__DEMAND_DIRTY = 0x200, // L2 dirty line evicted by a demand
		L2_LINES_OUT__MASK__SNB_L2_LINES_OUT__PREFETCH_CLEAN = 0x400, // L2 clean line evicted by a prefetch
		L2_LINES_OUT__MASK__SNB_L2_LINES_OUT__PREFETCH_DIRTY = 0x800, // L2 dirty line evicted by an MLC Prefetch
		L2_LINES_OUT__MASK__SNB_L2_LINES_OUT__DIRTY_ANY = 0xa00, // Any L2 dirty line evicted (does not cover rejects)
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS__MASK__SNB_L2_RQSTS__ALL_CODE_RD = 0x3000, // Any ifetch request to L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__CODE_RD_HIT = 0x1000, // L2 cache hits when fetching instructions
		L2_RQSTS__MASK__SNB_L2_RQSTS__CODE_RD_MISS = 0x2000, // L2 cache misses when fetching instructions
		L2_RQSTS__MASK__SNB_L2_RQSTS__ALL_DEMAND_DATA_RD = 0x300, // Demand  data read requests to L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__ALL_DEMAND_RD_HIT = 0x100, // Demand data read requests that hit L2
		L2_RQSTS__MASK__SNB_L2_RQSTS__ALL_PF = 0xc000, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__PF_HIT = 0x4000, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__PF_MISS = 0x8000, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__RFO_ANY = 0xc00, // Any RFO requests to L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__RFO_HITS = 0x400, // RFO requests that hit L2 cache
		L2_RQSTS__MASK__SNB_L2_RQSTS__RFO_MISS = 0x800, // RFO requests that miss L2 cache
		L2_STORE_LOCK_RQSTS = 0x27, // L2 store lock requests
		L2_STORE_LOCK_RQSTS__MASK__SNB_L2_STORE_LOCK_RQSTS__HIT_E = 0x400, // RFOs that hit cache lines in E state
		L2_STORE_LOCK_RQSTS__MASK__SNB_L2_STORE_LOCK_RQSTS__MISS = 0x100, // RFOs that miss cache (I state)
		L2_STORE_LOCK_RQSTS__MASK__SNB_L2_STORE_LOCK_RQSTS__HIT_M = 0x800, // RFOs that hit cache lines in M state
		L2_STORE_LOCK_RQSTS__MASK__SNB_L2_STORE_LOCK_RQSTS__ALL = 0xf00, // RFOs that access cache lines in any state
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS__MASK__SNB_L2_TRANS__ALL = 0x8000, // Transactions accessing MLC pipe
		L2_TRANS__MASK__SNB_L2_TRANS__CODE_RD = 0x400, // L2 cache accesses when fetching instructions
		L2_TRANS__MASK__SNB_L2_TRANS__L1D_WB = 0x1000, // L1D writebacks that access L2 cache
		L2_TRANS__MASK__SNB_L2_TRANS__LOAD = 0x100, // Demand Data Read* requests that access L2 cache
		L2_TRANS__MASK__SNB_L2_TRANS__L2_FILL = 0x2000, // L2 fill requests that access L2 cache
		L2_TRANS__MASK__SNB_L2_TRANS__L2_WB = 0x4000, // L2 writebacks that access L2 cache
		L2_TRANS__MASK__SNB_L2_TRANS__ALL_PREFETCH = 0x800, // L2 or L3 HW prefetches that access L2 cache (including rejects)
		L2_TRANS__MASK__SNB_L2_TRANS__RFO = 0x200, // RFO requests that access L2 cache
		LAST_LEVEL_CACHE_MISSES = 0x412e, // This is an alias for L3_LAT_CACHE:MISS
		LLC_MISSES = 0x412e, // Alias for LAST_LEVEL_CACHE_MISSES
		LAST_LEVEL_CACHE_REFERENCES = 0x4f2e, // This is an alias for L3_LAT_CACHE:REFERENCE
		LLC_REFERENCES = 0x4f2e, // Alias for LAST_LEVEL_CACHE_REFERENCES
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS__MASK__SNB_LD_BLOCKS__DATA_UNKNOWN = 0x100, // Blocked loads due to store buffer blocks with unknown data
		LD_BLOCKS__MASK__SNB_LD_BLOCKS__STORE_FORWARD = 0x200, // Loads blocked by overlapping with store buffer that cannot be forwarded
		LD_BLOCKS__MASK__SNB_LD_BLOCKS__NO_SR = 0x800, // Number of split loads blocked due to resource not available
		LD_BLOCKS__MASK__SNB_LD_BLOCKS__ALL_BLOCK = 0x1000, // Number of cases where any load is blocked but has not DCU miss
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL__MASK__SNB_LD_BLOCKS_PARTIAL__ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LD_BLOCKS_PARTIAL__MASK__SNB_LD_BLOCKS_PARTIAL__ALL_STA_BLOCK = 0x800, // Number of times that load operations are temporarily blocked because of older stores
		LOAD_HIT_PRE = 0x4c, // Load dispatches that hit fill buffer
		LOAD_HIT_PRE__MASK__SNB_LOAD_HIT_PRE__HW_PF = 0x200, // Non sw-prefetch load dispatches that hit the fill buffer allocated for HW prefetch
		LOAD_HIT_PRE__MASK__SNB_LOAD_HIT_PRE__SW_PF = 0x100, // Non sw-prefetch load dispatches that hit the fill buffer allocated for SW prefetch
		L3_LAT_CACHE = 0x2e, // Core-originated cacheable demand requests to L3
		L3_LAT_CACHE__MASK__SNB_L3_LAT_CACHE__MISS = 0x100, // Core-originated cacheable demand requests missed L3
		L3_LAT_CACHE__MASK__SNB_L3_LAT_CACHE__REFERENCE = 0x200, // Core-originated cacheable demand requests that refer to L3
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS__MASK__SNB_MACHINE_CLEARS__MASKMOV = 0x2000, // The number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0
		MACHINE_CLEARS__MASK__SNB_MACHINE_CLEARS__MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS__MASK__SNB_MACHINE_CLEARS__SMC = 0x400, // Self-Modifying Code detected
		MACHINE_CLEARS__MASK__SNB_MACHINE_CLEARS__COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of machine clears (nukes) of any type
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED = 0xd2, // L3 hit loads uops retired (deprecated use MEM_LOAD_UOPS_LLC_HIT_RETIRED)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HIT = 0x200, // Load LLC Hit and a cross-core Snoop hits in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared LLC) (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_MISS = 0x100, // Load LLC Hit and a cross-core Snoop missed in on-pkg core cache (Precise Event)
		MEM_LOAD_LLC_HIT_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_HIT_RETIRED__XSNP_NONE = 0x800, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_MISC_RETIRED = 0xd4, // Loads and some non simd split loads uops retired
		MEM_LOAD_UOPS_MISC_RETIRED__MASK__SNB_MEM_LOAD_UOPS_MISC_RETIRED__LLC_MISS = 0x200, // Counts load driven L3 misses and some non simd split loads (Precise Event)
		MEM_LOAD_MISC_RETIRED = 0xd4, // Loads and some non simd split loads uops retired (deprecated use MEM_LOAD_UOPS_MISC_RETIRED)
		MEM_LOAD_MISC_RETIRED__MASK__SNB_MEM_LOAD_UOPS_MISC_RETIRED__LLC_MISS = 0x200, // Counts load driven L3 misses and some non simd split loads (Precise Event)
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Memory loads uops retired
		MEM_LOAD_UOPS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L3_MISS = 0x2000, // Retired load uops which data sources were data missed LLC (excluding unknown data source)
		MEM_LOAD_RETIRED = 0xd1, // Memory loads uops retired (deprecated use MEM_LOAD_UOPS_RETIRED)
		MEM_LOAD_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__HIT_LFB = 0x4000, // A load missed L1D but hit the Fill Buffer (Precise Event)
		MEM_LOAD_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L1_HIT = 0x100, // Load hit in nearest-level (L1D) cache (Precise Event)
		MEM_LOAD_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L2_HIT = 0x200, // Load hit in mid-level (L2) cache (Precise Event)
		MEM_LOAD_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L3_HIT = 0x400, // Load hit in last-level (L3) cache with no snoop needed (Precise Event)
		MEM_LOAD_RETIRED__MASK__SNB_MEM_LOAD_UOPS_RETIRED__L3_MISS = 0x2000, // Retired load uops which data sources were data missed LLC (excluding unknown data source)
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired
		MEM_TRANS_RETIRED__MASK__SNB_MEM_TRANS_RETIRED__LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED__MASK__SNB_MEM_TRANS_RETIRED__PRECISE_STORE = 0x200, // Capture where stores occur
		MEM_UOPS_RETIRED = 0xd0, // Memory uops retired
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__LOCK_STORES = 0x2200, // Locked retired stores (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOPS_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MEM_UOP_RETIRED = 0xd0, // Memory uops retired (deprecated use MEM_UOPS_RETIRED)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ALL_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ANY_LOADS = 0x8100, // Any retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ALL_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__ANY_STORES = 0x8200, // Any retired stores (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__LOCK_LOADS = 0x2100, // Locked retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__LOCK_STORES = 0x2200, // Locked retired stores (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__SPLIT_LOADS = 0x4100, // Retired loads causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__SPLIT_STORES = 0x4200, // Retired stores causing cacheline splits (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__STLB_MISS_LOADS = 0x1100, // STLB misses dues to retired loads (Precise Event)
		MEM_UOP_RETIRED__MASK__SNB_MEM_UOPS_RETIRED__STLB_MISS_STORES = 0x1200, // STLB misses dues to retired stores (Precise Event)
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF__MASK__SNB_MISALIGN_MEM_REF__LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF__MASK__SNB_MISALIGN_MEM_REF__STORES = 0x200, // Speculative cache-line split Store-address uops dispatched to L1D
		OFFCORE_REQUESTS = 0xb0, // Offcore requests
		OFFCORE_REQUESTS__MASK__SNB_OFFCORE_REQUESTS__ALL_DATA_RD = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS__MASK__SNB_OFFCORE_REQUESTS__ALL_DATA_READ = 0x800, // Demand and prefetch read requests sent to uncore
		OFFCORE_REQUESTS__MASK__SNB_OFFCORE_REQUESTS__DEMAND_CODE_RD = 0x200, // Offcore code read requests
		OFFCORE_REQUESTS__MASK__SNB_OFFCORE_REQUESTS__DEMAND_DATA_RD = 0x100, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS__MASK__SNB_OFFCORE_REQUESTS__DEMAND_RFO = 0x400, // Offcore Demand RFOs
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore requests buffer
		OFFCORE_REQUESTS_BUFFER__MASK__SNB_OFFCORE_REQUESTS_BUFFER__SQ_FULL = 0x100, // Offcore requests buffer cannot take more entries for this thread core
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SNB_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ
		OTHER_ASSISTS = 0xc1, // Count hardware assists
		OTHER_ASSISTS__MASK__SNB_OTHER_ASSISTS__ITLB_MISS_RETIRED = 0x200, // Number of instructions that experienced an ITLB miss
		OTHER_ASSISTS__MASK__SNB_OTHER_ASSISTS__AVX_TO_SSE = 0x1000, // Number of transitions from AVX-256 to legacy SSE when penalty applicable
		OTHER_ASSISTS__MASK__SNB_OTHER_ASSISTS__SSE_TO_AVX = 0x2000, // Number of transitions from legacy SSE to AVX-256 when penalty applicable
		OTHER_ASSISTS__MASK__SNB_OTHER_ASSISTS__AVX_STORE = 0x800, // Number of GSSE memory assist for stores. GSSE microcode assist is being invoked whenever the hardware is unable to properly handle GSSE-256b operations
		PARTIAL_RAT_STALLS = 0x59, // Partial Register Allocation Table stalls
		PARTIAL_RAT_STALLS__MASK__SNB_PARTIAL_RAT_STALLS__FLAGS_MERGE_UOP = 0x2000, // Number of flags-merge uops in flight in each cycle
		PARTIAL_RAT_STALLS__MASK__SNB_PARTIAL_RAT_STALLS__CYCLES_FLAGS_MERGE_UOP = 0x2000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles in which flags-merge uops in flight
		PARTIAL_RAT_STALLS__MASK__SNB_PARTIAL_RAT_STALLS__MUL_SINGLE_UOP = 0x8000, // Number of Multiply packed/scalar single precision uops allocated
		PARTIAL_RAT_STALLS__MASK__SNB_PARTIAL_RAT_STALLS__SLOW_LEA_WINDOW = 0x4000, // Number of cycles with at least one slow LEA uop allocated
		RESOURCE_STALLS = 0xa2, // Resource related stall cycles
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__ANY = 0x100, // Cycles stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__LB = 0x200, // Cycles stalled due to lack of load buffers
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__RS = 0x400, // Cycles stalled due to no eligible RS entry available
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__SB = 0x800, // Cycles stalled due to no store buffers available (not including draining from sync)
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__ROB = 0x1000, // Cycles stalled due to re-order buffer full
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__FCSW = 0x2000, // Cycles stalled due to writing the FPU control word
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__MXCSR = 0x4000, // Cycles stalled due to the MXCSR register ranme occurring too close to a previous MXCSR rename
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__MEM_RS = 0xe00, // Cycles stalled due to LB
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__LD_SB = 0xa00, // Resource stalls due to load or store buffers all being in use
		RESOURCE_STALLS__MASK__SNB_RESOURCE_STALLS__OOO_SRC = 0xf000, // Resource stalls due to Rob being full
		RESOURCE_STALLS2 = 0x5b, // Resource related stall cycles
		RESOURCE_STALLS2__MASK__SNB_RESOURCE_STALLS2__ALL_FL_EMPTY = 0xc00, // Cycles stalled due to free list empty
		RESOURCE_STALLS2__MASK__SNB_RESOURCE_STALLS2__ALL_PRF_CONTROL = 0xf00, // Cycles stalls due to control structures full for physical registers
		RESOURCE_STALLS2__MASK__SNB_RESOURCE_STALLS2__ANY_PRF_CONTROL = 0xf00, // Cycles stalls due to control structures full for physical registers
		RESOURCE_STALLS2__MASK__SNB_RESOURCE_STALLS2__BOB_FULL = 0x4000, // Cycles Allocator is stalled due Branch Order Buffer
		RESOURCE_STALLS2__MASK__SNB_RESOURCE_STALLS2__OOO_RSRC = 0x4f00, // Cycles stalled due to out of order resources full
		ROB_MISC_EVENTS = 0xcc, // Reorder buffer events
		ROB_MISC_EVENTS__MASK__SNB_ROB_MISC_EVENTS__LBR_INSERTS = 0x2000, // Count each time an new LBR record is saved by HW
		RS_EVENTS = 0x5e, // Reservation station events
		RS_EVENTS__MASK__SNB_RS_EVENTS__EMPTY_CYCLES = 0x100, // Cycles the RS is empty for this thread
		RS_EVENTS__MASK__SNB_RS_EVENTS__EMPTY_END = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Counts number of time the Reservation Station (RS) goes from empty to non-empty
		SIMD_FP_256 = 0x11, // Counts 256-bit packed floating point instructions
		SIMD_FP_256__MASK__SNB_SIMD_FP_256__PACKED_SINGLE = 0x100, // Counts 256-bit packed single-precision
		SIMD_FP_256__MASK__SNB_SIMD_FP_256__PACKED_DOUBLE = 0x200, // Counts 256-bit packed double-precision
		SQ_MISC = 0xf4, // SuperQ events
		SQ_MISC__MASK__SNB_SQ_MISC__SPLIT_LOCK = 0x1000, // Split locks in SQ
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH__MASK__SNB_TLB_FLUSH__DTLB_THREAD = 0x100, // Number of DTLB flushes of thread-specific entries
		TLB_FLUSH__MASK__SNB_TLB_FLUSH__STLB_ANY = 0x2000, // Number of STLB flushes
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CORE = 0x200, // Counts total number of uops executed from any thread per cycle
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__THREAD = 0x100, // Counts total number of uops executed per thread each cycle
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CYCLES_GE_1_UOP_EXEC = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CYCLES_GE_2_UOPS_EXEC = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CYCLES_GE_3_UOPS_EXEC = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CYCLES_GE_4_UOPS_EXEC = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CORE_CYCLES_GE_1 = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed from any thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CORE_CYCLES_GE_2 = 0x200 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed from any thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CORE_CYCLES_GE_3 = 0x200 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed from any thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CORE_CYCLES_GE_4 = 0x200 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed from any thread
		UOPS_EXECUTED__MASK__SNB_UOPS_EXECUTED__CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV, // Cycles where no uop is executed on any thread
		UOPS_DISPATCHED_PORT = 0xa1, // Uops dispatch to specific ports
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_0 = 0x100, // Cycles which a Uop is dispatched on port 0
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_1 = 0x200, // Cycles which a Uop is dispatched on port 1
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_2_LD = 0x400, // Cycles in which a load uop is dispatched on port 2
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_2_STA = 0x800, // Cycles in which a store uop is dispatched on port 2
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_2 = 0xc00, // Cycles in which a uop is dispatched on port 2
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_3 = 0x3000, // Cycles in which a uop is dispatched on port 3
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_4 = 0x4000, // Cycles which a uop is dispatched on port 4
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_5 = 0x8000, // Cycles which a Uop is dispatched on port 5
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 0 for any thread
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 1 for any thread
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_2_CORE = 0xc00 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 2 for any thread
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_3_CORE = 0x3000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 3 for any thread
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_4_CORE = 0x4000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 4 for any thread
		UOPS_DISPATCHED_PORT__MASK__SNB_UOPS_DISPATCHED_PORT__PORT_5_CORE = 0x8000 | INTEL_X86_MOD_ANY, // Cycles in which a uop is dispatched on port 5 for any thread
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED__MASK__SNB_UOPS_ISSUED__ANY = 0x100, // Number of uops issued by the RAT to the Reservation Station (RS)
		UOPS_ISSUED__MASK__SNB_UOPS_ISSUED__CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued on this core (by any thread)
		UOPS_ISSUED__MASK__SNB_UOPS_ISSUED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops issued by this thread
		UOPS_RETIRED = 0xc2, // Uops retired
		UOPS_RETIRED__MASK__SNB_UOPS_RETIRED__ALL = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED__MASK__SNB_UOPS_RETIRED__ANY = 0x100, // All uops that actually retired (Precise Event)
		UOPS_RETIRED__MASK__SNB_UOPS_RETIRED__RETIRE_SLOTS = 0x200, // Number of retirement slots used (Precise Event)
		UOPS_RETIRED__MASK__SNB_UOPS_RETIRED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no executable uop retired (Precise Event)
		UOPS_RETIRED__MASK__SNB_UOPS_RETIRED__TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Total cycles using precise uop retired event (Precise Event)
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY__MASK__SNB_CYCLE_ACTIVITY__CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss loads
		CYCLE_ACTIVITY__MASK__SNB_CYCLE_ACTIVITY__CYCLES_L1D_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY__MASK__SNB_CYCLE_ACTIVITY__CYCLES_NO_DISPATCH = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles of dispatch stalls
		CYCLE_ACTIVITY__MASK__SNB_CYCLE_ACTIVITY__STALLS_L2_PENDING = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls due to L2 pending loads
		CYCLE_ACTIVITY__MASK__SNB_CYCLE_ACTIVITY__STALLS_L1D_PENDING = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls due to L1D pending loads
		EPT = 0x4f, // Extended page table
		EPT__MASK__SNB_EPT__WALK_CYCLES = 0x1000, // Cycles for an extended page table walk
		LSD = 0xa8, // Loop stream detector
		LSD__MASK__SNB_LSD__UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD__MASK__SNB_LSD__ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with uops delivered by the LSD but which did not come from decoder
		LSD__MASK__SNB_LSD__CYCLES_4_UOPS = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles with 4 uops delivered by the LSD but which did not come from decoder
		PAGE_WALKS = 0xbe, // page walker
		PAGE_WALKS__MASK__SNB_PAGE_WALKS__LLC_MISS = 0x100, // Number of page walks with a LLC miss
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // Load uops retired which miss the L3 cache
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_MISS_RETIRED__LOCAL_DRAM = 0x100, // Load uops that miss in the L3 and hit local DRAM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SNB_MEM_LOAD_UOPS_LLC_MISS_RETIRED__REMOTE_DRAM = 0x400, // Load uops that miss in the L3 and hit remote DRAM
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_LOCAL_DRAM = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__NO_SNP_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0__MASK__SNB_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__ANY_IFETCH = 0x24400, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__ANY_RFO = 0x12200, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_LOCAL_DRAM = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_LOCAL = 1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__L3_MISS = 0x1ULL << (22+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_REMOTE = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_MISS_REMOTE_DRAM = 0xffULL << (23+8), // Supplier: counts L3 misses to remote DRAM
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__L3_MISS = 0x3ULL << (22+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__NO_SNP_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1__MASK__SNB_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};