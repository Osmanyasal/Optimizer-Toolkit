#pragma once
#include <cstdint>
#include "intel_priv.hh"
namespace optkit::intel::bdw{
	enum bdw : uint64_t {
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS__MASK__BDW_BACLEARS__ANY = 0x1f00, // Number of front-end re-steers due to BPU misprediction
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__NONTAKEN_CONDITIONAL = 0x4100, // All macro conditional nontaken branch instructions
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__NONTAKEN_COND = 0x4100, // All macro conditional nontaken branch instructions
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_CONDITIONAL = 0x8100, // Taken speculative and retired macro-conditional branches
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_COND = 0x8100, // Taken speculative and retired macro-conditional branches
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_DIRECT_JUMP = 0x8200, // Taken speculative and retired macro-conditional branch instructions excluding calls and indirects
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // Taken speculative and retired indirect branches excluding calls and returns
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_INDIRECT_NEAR_RETURN = 0x8800, // Taken speculative and retired indirect branches with return mnemonic
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_DIRECT_NEAR_CALL = 0x9000, // Taken speculative and retired direct near calls
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_CONDITIONAL = 0xc100, // Speculative and retired macro-conditional branches
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_COND = 0xc100, // Speculative and retired macro-conditional branches
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ANY_COND = 0xc100, // Speculative and retired macro-conditional branches
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_DIRECT_JMP = 0xc200, // Speculative and retired macro-unconditional branches excluding calls and indirects
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // Speculative and retired indirect branches excluding calls and returns
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_INDIRECT_NEAR_RETURN = 0xc800, // Speculative and retired indirect return branches
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_DIRECT_NEAR_CALL = 0xd000, // Speculative and retired direct near calls
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__TAKEN_INDIRECT_NEAR_CALL = 0xa000, // All indirect calls
		BR_INST_EXEC__MASK__BDW_BR_INST_EXEC__ALL_BRANCHES = 0xff00, // All branch instructions executed
		BR_INST_RETIRED = 0xc4, // Branch instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__CONDITIONAL = 0x100, // Counts all taken and not taken macro conditional branch instructions
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__COND = 0x100, // Counts all taken and not taken macro conditional branch instructions
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__NEAR_CALL = 0x200, // Counts all macro direct and indirect near calls
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__ALL_BRANCHES = 0x0, // Counts all taken and not taken macro branches including far branches (architectural event)
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__NEAR_RETURN = 0x800, // Counts the number of near ret instructions retired
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__NOT_TAKEN = 0x1000, // Counts all not taken macro branch instructions retired
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__NEAR_TAKEN = 0x2000, // Counts the number of near branch taken instructions retired
		BR_INST_RETIRED__MASK__BDW_BR_INST_RETIRED__FAR_BRANCH = 0x4000, // Counts the number of far branch instructions retired
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__NONTAKEN_CONDITIONAL = 0x4100, // Not taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__NONTAKEN_COND = 0x4100, // Not taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__TAKEN_CONDITIONAL = 0x8100, // Taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__TAKEN_COND = 0x8100, // Taken speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__TAKEN_INDIRECT_JUMP_NON_CALL_RET = 0x8400, // Taken speculative and retired mispredicted indirect branches excluding calls and returns
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__ALL_CONDITIONAL = 0xc100, // Speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__ANY_COND = 0xc100, // Speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__ALL_INDIRECT_JUMP_NON_CALL_RET = 0xc400, // All mispredicted indirect branches that are not calls nor returns
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__ALL_BRANCHES = 0xff00, // Speculative and retired mispredicted macro conditional branches
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__TAKEN_INDIRECT_NEAR_CALL = 0xa000, // Taken speculative and retired mispredicted indirect calls
		BR_MISP_EXEC__MASK__BDW_BR_MISP_EXEC__TAKEN_RETURN_NEAR = 0x8800, // Taken speculative and retired mispredicted direct returns
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches (Precise Event)
		BR_MISP_RETIRED__MASK__BDW_BR_MISP_RETIRED__CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions
		BR_MISP_RETIRED__MASK__BDW_BR_MISP_RETIRED__COND = 0x100, // All mispredicted macro conditional branch instructions
		BR_MISP_RETIRED__MASK__BDW_BR_MISP_RETIRED__ALL_BRANCHES = 0x0, // All mispredicted macro branches (architectural event)
		BR_MISP_RETIRED__MASK__BDW_BR_MISP_RETIRED__NEAR_TAKEN = 0x2000, // Number of near branch instructions retired that were mispredicted and taken
		BR_MISP_RETIRED__MASK__BDW_BR_MISP_RETIRED__RET = 0x800, // Number of mispredicted ret instructions retired
		CPL_CYCLES = 0x5c, // Unhalted core cycles at a specific ring level
		CPL_CYCLES__MASK__BDW_CPL_CYCLES__RING0 = 0x100, // Unhalted core cycles when the thread is in ring 0
		CPL_CYCLES__MASK__BDW_CPL_CYCLES__RING123 = 0x200, // Unhalted core cycles when thread is in rings 1
		CPL_CYCLES__MASK__BDW_CPL_CYCLES__RING0_TRANS = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of intervals between processor halts while thread is in ring 0
		CPU_CLK_THREAD_UNHALTED = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		CPU_CLK_THREAD_UNHALTED__MASK__BDW_CPU_CLK_THREAD_UNHALTED__REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_THREAD_UNHALTED__MASK__BDW_CPU_CLK_THREAD_UNHALTED__REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses (100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_THREAD_UNHALTED__MASK__BDW_CPU_CLK_THREAD_UNHALTED__REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_THREAD_UNHALTED__MASK__BDW_CPU_CLK_THREAD_UNHALTED__THREAD_P = 0x000, // Cycles when thread is not halted
		CPU_CLK_THREAD_UNHALTED__MASK__BDW_CPU_CLK_THREAD_UNHALTED__ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		CPU_CLK_UNHALTED = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss loads (must use with HT off only)
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__CYCLES_LDM_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending memory loads
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__CYCLES_MEM_ANY = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with pending memory loads
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__CYCLES_L1D_PENDING = 0x0800 | (0x8 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__STALLS_LDM_PENDING = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Executions stalls when there is at least one pending demand load request
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__STALLS_L1D_PENDING = 0x0c00 | (0xc << INTEL_X86_CMASK_BIT), // Executions stalls while there is at least one L1D demand load outstanding
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__STALLS_L2_PENDING = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls while there is at least one L2 demand load pending outstanding
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__STALLS_TOTAL = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles during which no instructions were executed in the execution stage of the pipeline
		CYCLE_ACTIVITY__MASK__BDW_CYCLE_ACTIVITY__CYCLES_NO_EXECUTE = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Cycles during which no instructions were executed in the execution stage of the pipeline
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED_4K = 0x200, // Misses in all TLB levels causes a page walk that completes (4K)
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED_2M_4M = 0x400, // Misses in all TLB levels causes a page walk of 2MB/4MB page sizes that completes
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED_1G = 0x800, // Misses in all TLB levels causes a page walk of 1GB page sizes that completes
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED = 0xe00, // Misses in all TLB levels causes a page walk of any page size that completes
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__STLB_HIT_4K = 0x2000, // Misses that miss the DTLB and hit the STLB (4KB)
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__STLB_HIT_2M = 0x4000, // Misses that miss the DTLB and hit the STLB (2MB)
		DTLB_LOAD_MISSES__MASK__BDW_DTLB_LOAD_MISSES__STLB_HIT = 0x6000, // Number of cache load STLB hits. No page walk
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED_4K = 0x200, // Misses in all TLB levels causes a page walk that completes (4K)
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED_2M_4M = 0x400, // Misses in all TLB levels causes a page walk of 2MB/4MB page sizes that completes
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED_1G = 0x800, // Misses in all TLB levels causes a page walk of 1GB page sizes that completes
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_COMPLETED = 0xe00, // Misses in all TLB levels causes a page walk of any page size that completes
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__STLB_HIT_4K = 0x2000, // Misses that miss the DTLB and hit the STLB (4KB)
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__STLB_HIT_2M = 0x4000, // Misses that miss the DTLB and hit the STLB (2MB)
		DTLB_STORE_MISSES__MASK__BDW_DTLB_LOAD_MISSES__STLB_HIT = 0x6000, // Number of cache load STLB hits. No page walk
		FP_ASSIST = 0xca, // X87 floating-point assists
		FP_ASSIST__MASK__BDW_FP_ASSIST__X87_OUTPUT = 0x200, // Number of X87 FP assists due to output values
		FP_ASSIST__MASK__BDW_FP_ASSIST__X87_INPUT = 0x400, // Number of X87 FP assists due to input values
		FP_ASSIST__MASK__BDW_FP_ASSIST__SIMD_OUTPUT = 0x800, // Number of SIMD FP assists due to output values
		FP_ASSIST__MASK__BDW_FP_ASSIST__SIMD_INPUT = 0x1000, // Number of SIMD FP assists due to input values
		FP_ASSIST__MASK__BDW_FP_ASSIST__ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SEE or FP assists
		FP_ASSIST__MASK__BDW_FP_ASSIST__ALL = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input and output SSE or FP assist
		HLE_RETIRED = 0xc8, // HLE execution (Precise Event)
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__START = 0x100, // Number of times an HLE execution started
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__COMMIT = 0x200, // Number of times an HLE execution successfully committed
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__ABORTED = 0x400, // Number of times an HLE execution aborted due to any reasons (multiple categories may count as one) (Precise Event)
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__ABORTED_MISC1 = 0x800, // Number of times an HLE execution aborted due to various memory events
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__ABORTED_MISC2 = 0x1000, // Number of times an HLE execution aborted due to uncommon conditions
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__ABORTED_MISC3 = 0x2000, // Number of times an HLE execution aborted due to HLE-unfriendly instructions
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__ABORTED_MISC4 = 0x4000, // Number of times an HLE execution aborted due to incompatible memory type
		HLE_RETIRED__MASK__BDW_HLE_RETIRED__ABORTED_MISC5 = 0x8000, // Number of times an HLE execution aborted due to none of the other 4 reasons (e.g.
		ICACHE = 0x80, // Instruction Cache
		ICACHE__MASK__BDW_ICACHE__MISSES = 0x200, // Number of Instruction Cache
		ICACHE__MASK__BDW_ICACHE__IFDATA_STALL = 0x400, // Number of cycles where a code fetch is stalled due to L1 miss
		ICACHE__MASK__BDW_ICACHE__HIT = 0x100, // Number of Instruction Cache
		IDQ = 0x79, // IDQ operations
		IDQ__MASK__BDW_IDQ__EMPTY = 0x200, // Cycles the Instruction Decode Queue (IDQ) is empty
		IDQ__MASK__BDW_IDQ__MITE_UOPS = 0x400, // Number of uops delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ__MASK__BDW_IDQ__DSB_UOPS = 0x800, // Number of uops delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path
		IDQ__MASK__BDW_IDQ__MS_DSB_UOPS = 0x1000, // Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ__MASK__BDW_IDQ__MS_MITE_UOPS = 0x2000, // Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ__MASK__BDW_IDQ__MS_UOPS = 0x3000, // Number of Uops were delivered into Instruction Decode Queue (IDQ) from MS
		IDQ__MASK__BDW_IDQ__MS_UOPS_CYCLES = 0x3000 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ__MASK__BDW_IDQ__MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ__MASK__BDW_IDQ__MITE_UOPS_CYCLES = 0x400 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ__MASK__BDW_IDQ__DSB_UOPS_CYCLES = 0x800 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path
		IDQ__MASK__BDW_IDQ__MS_DSB_UOPS_CYCLES = 0x1000 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ__MASK__BDW_IDQ__MS_DSB_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequencer (MS) is busy
		IDQ__MASK__BDW_IDQ__ALL_DSB_CYCLES_4_UOPS = 0x1800 | (4 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering 4 Uops
		IDQ__MASK__BDW_IDQ__ALL_DSB_CYCLES_ANY_UOPS = 0x1800 | (1 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering any Uop
		IDQ__MASK__BDW_IDQ__ALL_MITE_CYCLES_4_UOPS = 0x2400 | (4 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering 4 Uops
		IDQ__MASK__BDW_IDQ__ALL_MITE_CYCLES_ANY_UOPS = 0x2400 | (1 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering any Uop
		IDQ__MASK__BDW_IDQ__ALL_MITE_UOPS = 0x3c00, // Number of uops delivered to Instruction Decode Queue (IDQ) from any path
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED__MASK__BDW_IDQ_UOPS_NOT_DELIVERED__CORE = 0x100, // Count number of non-delivered uops to Resource Allocation Table (RAT)
		IDQ_UOPS_NOT_DELIVERED__MASK__BDW_IDQ_UOPS_NOT_DELIVERED__CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles per thread when 4 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__BDW_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_1_UOP_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles per thread when 3 or more uops are not delivered to the Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__BDW_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_2_UOP_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles with less than 2 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__BDW_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_3_UOP_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with less than 3 uops delivered by the front end
		IDQ_UOPS_NOT_DELIVERED__MASK__BDW_IDQ_UOPS_NOT_DELIVERED__CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles Front-End (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling FE
		INST_RETIRED = 0xc0, // Number of instructions retired (Precise Event)
		INST_RETIRED__MASK__BDW_INST_RETIRED__ANY_P = 0x000, // Number of instructions retired. General Counter - architectural event
		INST_RETIRED__MASK__BDW_INST_RETIRED__ALL = 0x100, // Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Precise Event)
		INST_RETIRED__MASK__BDW_INST_RETIRED__TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Number of cycles using always true condition
		INST_RETIRED__MASK__BDW_INST_RETIRED__PREC_DIST = 0x100, // Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Precise event)
		INST_RETIRED__MASK__BDW_INST_RETIRED__X87 = 0x200, // Number of FPU operations retired (instructions with no exceptions)
		INT_MISC = 0xd, // Miscellaneous interruptions
		INT_MISC__MASK__BDW_INT_MISC__RECOVERY_CYCLES = 0x300 | (1 << INTEL_X86_CMASK_BIT), // Cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception
		INT_MISC__MASK__BDW_INT_MISC__RECOVERY_CYCLES_ANY = 0x300 | (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)
		INT_MISC__MASK__BDW_INT_MISC__RECOVERY_STALLS_COUNT = 0x300 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of occurrences waiting for Machine Clears
		INT_MISC__MASK__BDW_INT_MISC__RAT_STALL_CYCLES = 0x800, // Cycles when the Resource Allocation Table (RAT) external stall event is sent to the Instruction Decode Queue (IDQ) for the thread. Also includes cycles when the allocator is serving another thread
		ITLB = 0xae, // Instruction TLB
		ITLB__MASK__BDW_ITLB__ITLB_FLUSH = 0x100, // Flushing of the Instruction TLB (ITLB) pages independent of page size
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__WALK_COMPLETED_4K = 0x200, // Misses in all TLB levels causes a page walk that completes (4KB)
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__WALK_COMPLETED_2M_4M = 0x400, // Misses in all TLB levels causes a page walk that completes (2MB/4MB)
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__WALK_COMPLETED_1G = 0x800, // Misses in all TLB levels causes a page walk that completes (1GB)
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__WALK_COMPLETED = 0xe00, // Misses in all TLB levels causes a page walk of any page size that completes
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__STLB_HIT_4K = 0x2000, // Misses that miss the DTLB and hit the STLB (4KB)
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__STLB_HIT_2M = 0x4000, // Misses that miss the DTLB and hit the STLB (2MB)
		ITLB_MISSES__MASK__BDW_ITLB_MISSES__STLB_HIT = 0x6000, // Number of cache load STLB hits. No page walk
		L1D = 0x51, // L1D cache
		L1D__MASK__BDW_L1D__REPLACEMENT = 0x100, // L1D Data line replacements
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS__MASK__BDW_L1D_PEND_MISS__PENDING = 0x100, // Cycles with L1D load misses outstanding
		L1D_PEND_MISS__MASK__BDW_L1D_PEND_MISS__PENDING_CYCLES = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load misses outstanding
		L1D_PEND_MISS__MASK__BDW_L1D_PEND_MISS__PENDING_CYCLES_ANY = 0x100 | (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles with L1D load misses outstanding from any thread
		L1D_PEND_MISS__MASK__BDW_L1D_PEND_MISS__OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number L1D miss outstanding
		L1D_PEND_MISS__MASK__BDW_L1D_PEND_MISS__EDGE = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number L1D miss outstanding
		L1D_PEND_MISS__MASK__BDW_L1D_PEND_MISS__FB_FULL = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles a demand request was blocked due to Fill Buffer (FB) unavailability
		L2_DEMAND_RQSTS = 0x27, // Demand Data Read requests to L2
		L2_DEMAND_RQSTS__MASK__BDW_L2_DEMAND_RQSTS__WB_HIT = 0x5000, // WB requests that hit L2 cache
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN__MASK__BDW_L2_LINES_IN__I = 0x100, // L2 cache lines in I state filling L2
		L2_LINES_IN__MASK__BDW_L2_LINES_IN__S = 0x200, // L2 cache lines in S state filling L2
		L2_LINES_IN__MASK__BDW_L2_LINES_IN__E = 0x400, // L2 cache lines in E state filling L2
		L2_LINES_IN__MASK__BDW_L2_LINES_IN__ALL = 0x700, // L2 cache lines filling L2
		L2_LINES_IN__MASK__BDW_L2_LINES_IN__ANY = 0x700, // L2 cache lines filling L2
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT__MASK__BDW_L2_LINES_OUT__DEMAND_CLEAN = 0x500, // Number of clean L2 cachelines evicted by demand
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS__MASK__BDW_L2_RQSTS__DEMAND_DATA_RD_MISS = 0x2100, // Demand Data Read requests that miss L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__DEMAND_DATA_RD_HIT = 0x4100, // Demand Data Read requests that hit L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__DEMAND_RFO_MISS = 0x2200, // RFO requests that miss L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__RFO_MISS = 0x2200, // RFO requests that miss L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__DEMAND_RFO_HIT = 0x4200, // RFO requests that hit L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__RFO_HIT = 0x4200, // RFO requests that hit L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__CODE_RD_MISS = 0x2400, // L2 cache misses when fetching instructions
		L2_RQSTS__MASK__BDW_L2_RQSTS__ALL_DEMAND_MISS = 0x2700, // All demand requests that miss the L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__CODE_RD_HIT = 0x4400, // L2 cache hits when fetching instructions
		L2_RQSTS__MASK__BDW_L2_RQSTS__L2_PF_MISS = 0x3800, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__PF_MISS = 0x3800, // Requests from the L2 hardware prefetchers that miss L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__MISS = 0x3f00, // All requests that miss the L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__L2_PF_HIT = 0xd800, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__PF_HIT = 0xd800, // Requests from the L2 hardware prefetchers that hit L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__ALL_DEMAND_DATA_RD = 0xe100, // Any data read request to L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__ALL_RFO = 0xe200, // Any data RFO request to L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__ALL_CODE_RD = 0xe400, // Any code read request to L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__ALL_DEMAND_REFERENCES = 0xe700, // All demand requests to L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__ALL_PF = 0xf800, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS__MASK__BDW_L2_RQSTS__REFERENCES = 0xff00, // All requests to L2 cache
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS__MASK__BDW_L2_TRANS__DEMAND_DATA_RD = 0x100, // Demand Data Read requests that access L2 cache
		L2_TRANS__MASK__BDW_L2_TRANS__RFO = 0x200, // RFO requests that access L2 cache
		L2_TRANS__MASK__BDW_L2_TRANS__CODE_RD = 0x400, // L2 cache accesses when fetching instructions
		L2_TRANS__MASK__BDW_L2_TRANS__ALL_PF = 0x800, // L2 or L3 HW prefetches that access L2 cache
		L2_TRANS__MASK__BDW_L2_TRANS__L1D_WB = 0x1000, // L1D writebacks that access L2 cache
		L2_TRANS__MASK__BDW_L2_TRANS__L2_FILL = 0x2000, // L2 fill requests that access L2 cache
		L2_TRANS__MASK__BDW_L2_TRANS__L2_WB = 0x4000, // L2 writebacks that access L2 cache
		L2_TRANS__MASK__BDW_L2_TRANS__ALL_REQUESTS = 0x8000, // Transactions accessing L2 pipe
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS__MASK__BDW_LD_BLOCKS__STORE_FORWARD = 0x200, // Counts the number of loads blocked by overlapping with store buffer entries that cannot be forwarded
		LD_BLOCKS__MASK__BDW_LD_BLOCKS__NO_SR = 0x800, // number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL__MASK__BDW_LD_BLOCKS_PARTIAL__ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LOAD_HIT_PRE = 0x4c, // Load dispatches
		LOAD_HIT_PRE__MASK__BDW_LOAD_HIT_PRE__HW_PF = 0x200, // Non software-prefetch load dispatches that hit FB allocated for hardware prefetch
		LOAD_HIT_PRE__MASK__BDW_LOAD_HIT_PRE__SW_PF = 0x100, // Non software-prefetch load dispatches that hit FB allocated for software prefetch
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES__MASK__BDW_LOCK_CYCLES__SPLIT_LOCK_UC_LOCK_DURATION = 0x100, // Cycles in which the L1D and L2 are locked
		LOCK_CYCLES__MASK__BDW_LOCK_CYCLES__CACHE_LOCK_DURATION = 0x200, // cycles that the L1D is locked
		LONGEST_LAT_CACHE = 0x2e, // L3 cache
		LONGEST_LAT_CACHE__MASK__BDW_LONGEST_LAT_CACHE__MISS = 0x4100, // Core-originated cacheable demand requests missed LLC - architectural event
		LONGEST_LAT_CACHE__MASK__BDW_LONGEST_LAT_CACHE__REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to LLC - architectural event
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS__MASK__BDW_MACHINE_CLEARS__CYCLES = 0x100, // Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes
		MACHINE_CLEARS__MASK__BDW_MACHINE_CLEARS__MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS__MASK__BDW_MACHINE_CLEARS__SMC = 0x400, // Number of Self-modifying code (SMC) Machine Clears detected
		MACHINE_CLEARS__MASK__BDW_MACHINE_CLEARS__MASKMOV = 0x2000, // This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0
		MACHINE_CLEARS__MASK__BDW_MACHINE_CLEARS__COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of machine clears (nukes) of any type
		MEM_LOAD_UOPS_L3_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_HIT_RETIRED__XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_UOPS_L3_MISS_RETIRED = 0xd3, // Load uops retired that missed the L3 (Precise Event)
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__LOCAL_DRAM = 0x100, // Retired load uops missing L3 cache but hitting local memory (Precise Event)
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__REMOTE_DRAM = 0x400, // Number of retired load uops that missed L3 but were service by remote RAM
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__REMOTE_HITM = 0x1000, // Number of retired load uops whose data sources was remote HITM (Precise Event)
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__REMOTE_FWD = 0x2000, // Load uops that miss in the L3 whose data source was forwarded from a remote cache (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // Load uops retired that missed the L3 (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__LOCAL_DRAM = 0x100, // Retired load uops missing L3 cache but hitting local memory (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__REMOTE_DRAM = 0x400, // Number of retired load uops that missed L3 but were service by remote RAM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__REMOTE_HITM = 0x1000, // Number of retired load uops whose data sources was remote HITM (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_L3_MISS_RETIRED__REMOTE_FWD = 0x2000, // Load uops that miss in the L3 whose data source was forwarded from a remote cache (Precise Event)
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Retired load uops (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__L1_HIT = 0x100, // Retired load uops with L1 cache hits as data source
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__L2_HIT = 0x200, // Retired load uops with L2 cache hits as data source
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__L3_HIT = 0x400, // Retired load uops with L3 cache hits as data source
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__L1_MISS = 0x800, // Retired load uops which missed the L1D
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__L2_MISS = 0x1000, // Retired load uops which missed the L2. Unknown data source excluded
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__L3_MISS = 0x2000, // Retired load uops which missed the L3
		MEM_LOAD_UOPS_RETIRED__MASK__BDW_MEM_LOAD_UOPS_RETIRED__HIT_LFB = 0x4000, // Retired load uops which missed L1 but hit line fill buffer (LFB)
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired (Precise Event)
		MEM_TRANS_RETIRED__MASK__BDW_MEM_TRANS_RETIRED__LOAD_LATENCY = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED__MASK__BDW_MEM_TRANS_RETIRED__LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_UOPS_RETIRED = 0xd0, // Memory uops retired (Precise Event)
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__STLB_MISS_LOADS = 0x1100, // Load uops with true STLB miss retired to architected path
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__STLB_MISS_STORES = 0x1200, // Store uops with true STLB miss retired to architected path
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__LOCK_LOADS = 0x2100, // Load uops with locked access retired
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__SPLIT_LOADS = 0x4100, // Line-splitted load uops retired
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__SPLIT_STORES = 0x4200, // Line-splitted store uops retired
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__ALL_LOADS = 0x8100, // All load uops retired
		MEM_UOPS_RETIRED__MASK__BDW_MEM_UOPS_RETIRED__ALL_STORES = 0x8200, // All store uops retired
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF__MASK__BDW_MISALIGN_MEM_REF__LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF__MASK__BDW_MISALIGN_MEM_REF__STORES = 0x200, // Speculative cache-line split store-address uops dispatched to L1D
		MOVE_ELIMINATION = 0x58, // Move Elimination
		MOVE_ELIMINATION__MASK__BDW_MOVE_ELIMINATION__INT_ELIMINATED = 0x100, // Number of integer Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION__MASK__BDW_MOVE_ELIMINATION__SIMD_ELIMINATED = 0x200, // Number of SIMD Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION__MASK__BDW_MOVE_ELIMINATION__INT_NOT_ELIMINATED = 0x400, // Number of integer Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION__MASK__BDW_MOVE_ELIMINATION__SIMD_NOT_ELIMINATED = 0x800, // Number of SIMD Move Elimination candidate uops that were not eliminated
		OFFCORE_REQUESTS = 0xb0, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS__MASK__BDW_OFFCORE_REQUESTS__DEMAND_DATA_RD = 0x100, // Demand data read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__BDW_OFFCORE_REQUESTS__DEMAND_CODE_RD = 0x200, // Demand code read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__BDW_OFFCORE_REQUESTS__DEMAND_RFO = 0x400, // Demand RFOs requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__BDW_OFFCORE_REQUESTS__ALL_DATA_RD = 0x800, // Data read requests sent to uncore (use with HT off only)
		OTHER_ASSISTS = 0xc1, // Software assist
		OTHER_ASSISTS__MASK__BDW_OTHER_ASSISTS__AVX_TO_SSE = 0x800, // Number of transitions from AVX-256 to legacy SSE when penalty applicable
		OTHER_ASSISTS__MASK__BDW_OTHER_ASSISTS__SSE_TO_AVX = 0x1000, // Number of transitions from legacy SSE to AVX-256 when penalty applicable
		OTHER_ASSISTS__MASK__BDW_OTHER_ASSISTS__ANY_WB_ASSIST = 0x4000, // Number of times any microcode assist is invoked by HW upon uop writeback
		RESOURCE_STALLS = 0xa2, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__BDW_RESOURCE_STALLS__ANY = 0x100, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__BDW_RESOURCE_STALLS__ALL = 0x100, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__BDW_RESOURCE_STALLS__RS = 0x400, // Stall cycles caused by absence of eligible entries in Reservation Station (RS)
		RESOURCE_STALLS__MASK__BDW_RESOURCE_STALLS__SB = 0x800, // Cycles Allocator is stalled due to Store Buffer full (not including draining from synch)
		RESOURCE_STALLS__MASK__BDW_RESOURCE_STALLS__ROB = 0x1000, // ROB full stall cycles
		ROB_MISC_EVENTS = 0xcc, // ROB miscellaneous events
		ROB_MISC_EVENTS__MASK__BDW_ROB_MISC_EVENTS__LBR_INSERTS = 0x2000, // Count each time an new Last Branch Record (LBR) is inserted
		RS_EVENTS = 0x5e, // Reservation Station
		RS_EVENTS__MASK__BDW_RS_EVENTS__EMPTY_CYCLES = 0x100, // Cycles the Reservation Station (RS) is empty for this thread
		RS_EVENTS__MASK__BDW_RS_EVENTS__EMPTY_END = 0x100 | INTEL_X86_MOD_INV |  (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_EDGE, // Number of times the reservation station (RS) was empty
		RTM_RETIRED = 0xc9, // Restricted Transaction Memory execution (Precise Event)
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__START = 0x100, // Number of times an RTM execution started
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__COMMIT = 0x200, // Number of times an RTM execution successfully committed
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__ABORTED = 0x400, // Number of times an RTM execution aborted due to any reasons (multiple categories may count as one) (Precise Event)
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__ABORTED_MISC1 = 0x800, // Number of times an RTM execution aborted due to various memory events
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__ABORTED_MISC2 = 0x1000, // Number of times an RTM execution aborted due to uncommon conditions
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__ABORTED_MISC3 = 0x2000, // Number of times an RTM execution aborted due to RTM-unfriendly instructions
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__ABORTED_MISC4 = 0x4000, // Number of times an RTM execution aborted due to incompatible memory type
		RTM_RETIRED__MASK__BDW_RTM_RETIRED__ABORTED_MISC5 = 0x8000, // Number of times an RTM execution aborted due to none of the other 4 reasons (e.g.
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH__MASK__BDW_TLB_FLUSH__DTLB_THREAD = 0x100, // Count number of DTLB flushes of thread-specific entries
		TLB_FLUSH__MASK__BDW_TLB_FLUSH__STLB_ANY = 0x2000, // Count number of any STLB flushes
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CORE = 0x200, // Number of uops executed from any thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__THREAD = 0x100, // Number of uops executed per thread each cycle
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CYCLES_GE_1_UOP_EXEC = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CYCLES_GE_2_UOPS_EXEC = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CYCLES_GE_3_UOPS_EXEC = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CYCLES_GE_4_UOPS_EXEC = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CORE_CYCLES_GE_1 = 0x200 | (1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed from any thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CORE_CYCLES_GE_2 = 0x200 | (2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed from any thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CORE_CYCLES_GE_3 = 0x200 | (3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed from any thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CORE_CYCLES_GE_4 = 0x200 | (4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed from any thread
		UOPS_EXECUTED__MASK__BDW_UOPS_EXECUTED__CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV, // Cycles where no uop is executed on any thread
		LSD = 0xa8, // Loop stream detector
		LSD__MASK__BDW_LSD__UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD__MASK__BDW_LSD__ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with uops delivered by the LSD but which did not come from decoder
		LSD__MASK__BDW_LSD__CYCLES_4_UOPS = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Cycles with 4 uops delivered by the LSD but which did not come from decoder
		UOPS_EXECUTED_PORT = 0xa1, // Uops dispatch to specific ports
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_0 = 0x100, // Cycles which a Uop is executed on port 0
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_1 = 0x200, // Cycles which a Uop is executed on port 1
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_2 = 0x400, // Cycles which a Uop is executed on port 2
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_3 = 0x800, // Cycles which a Uop is executed on port 3
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_4 = 0x1000, // Cycles which a Uop is executed on port 4
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_5 = 0x2000, // Cycles which a Uop is executed on port 5
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_6 = 0x4000, // Cycles which a Uop is executed on port 6
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_7 = 0x8000, // Cycles which a Uop is executed on port 7
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_2_CORE = 0x400 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_3_CORE = 0x800 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_4_CORE = 0x1000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_5_CORE = 0x2000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_6_CORE = 0x4000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_EXECUTED_PORT__MASK__BDW_UOPS_EXECUTED_PORT__PORT_7_CORE = 0x8000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__ANY = 0x100, // Number of Uops issued by the Resource Allocation Table (RAT) to the Reservation Station (RS)
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__ALL = 0x100, // Number of Uops issued by the Resource Allocation Table (RAT) to the Reservation Station (RS)
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__FLAGS_MERGE = 0x1000, // Number of flags-merge uops being allocated. Such uops adds delay
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__SLOW_LEA = 0x2000, // Number of slow LEA or similar uops allocated. Such uop has 3 sources regardless if result of LEA instruction or not
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__SINGLE_MUL = 0x4000, // Number of Multiply packed/scalar single precision uops allocated
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Counts the number of cycles no uops issued by this thread
		UOPS_ISSUED__MASK__BDW_UOPS_ISSUED__CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Counts the number of cycles no uops issued on this core
		ARITH = 0x14, // Arithmetic uop
		ARITH__MASK__BDW_ARITH__FPU_DIV_ACTIVE = 0x0100, // Cycles when divider is busy execuing divide operations
		UOPS_RETIRED = 0xc2, // Uops retired (Precise Event)
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__ALL = 0x100, // All uops that actually retired
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__ANY = 0x100, // All uops that actually retired
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__RETIRE_SLOTS = 0x200, // number of retirement slots used non PEBS
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles no executable uops retired (Precise Event)
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Number of cycles using always true condition applied to PEBS uops retired event
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles no executable uops retired on core (Precise Event)
		UOPS_RETIRED__MASK__BDW_UOPS_RETIRED__STALL_OCCURRENCES = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE| (1 << INTEL_X86_CMASK_BIT), // Number of transitions from stalled to unstalled execution (Precise Event)
		TX_MEM = 0x54, // Transactional memory aborts
		TX_MEM__MASK__BDW_TX_MEM__ABORT_CONFLICT = 0x100, // Number of times a transactional abort was signaled due to data conflict on a transactionally accessed address
		TX_MEM__MASK__BDW_TX_MEM__ABORT_CAPACITY = 0x200, // Number of times a transactional abort was signaled due to data capacity limitation
		TX_MEM__MASK__BDW_TX_MEM__ABORT_HLE_STORE_TO_ELIDED_LOCK = 0x400, // Number of times a HLE transactional execution aborted due to a non xrelease prefixed instruction writing to an elided lock in the elision buffer
		TX_MEM__MASK__BDW_TX_MEM__ABORT_HLE_ELISION_BUFFER_NOT_EMPTY = 0x800, // Number of times a HLE transactional execution aborted due to NoAllocatedElisionBuffer being non-zero
		TX_MEM__MASK__BDW_TX_MEM__ABORT_HLE_ELISION_BUFFER_MISMATCH = 0x1000, // Number of times a HLE transaction execution aborted due to xrelease lock not satisfying the address and value requirements in the elision buffer
		TX_MEM__MASK__BDW_TX_MEM__ABORT_HLE_ELISION_BUFFER_UNSUPPORTED_ALIGNMENT = 0x2000, // Number of times a HLE transaction execution aborted due to an unsupported read alignment from the elision buffer
		TX_MEM__MASK__BDW_TX_MEM__ABORT_HLE_ELISION_BUFFER_FULL = 0x4000, // Number of times a HLE clock could not be elided due to ElisionBufferAvailable being zero
		TX_EXEC = 0x5d, // Transactional execution
		TX_EXEC__MASK__BDW_TX_EXEC__MISC1 = 0x100, // Number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution
		TX_EXEC__MASK__BDW_TX_EXEC__MISC2 = 0x200, // Number of times a class of instructions that may cause a transactional abort was executed inside a transactional region
		TX_EXEC__MASK__BDW_TX_EXEC__MISC3 = 0x400, // Number of times an instruction execution caused the supported nest count to be exceeded
		TX_EXEC__MASK__BDW_TX_EXEC__MISC4 = 0x800, // Number of times an instruction a xbegin instruction was executed inside HLE transactional region
		TX_EXEC__MASK__BDW_TX_EXEC__MISC5 = 0x1000, // Number of times an instruction with HLE xacquire prefix was executed inside a RTM transactional region
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__BDW_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ (use with HT off only)
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL__MASK__BDW_ILD_STALL__LCP = 0x100, // Stall caused by changing prefix length of the instruction
		PAGE_WALKER_LOADS = 0xbc, // Page walker loads
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__DTLB_L1 = 0x1100, // Number of DTLB page walker loads that hit in the L1D and line fill buffer
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__ITLB_L1 = 0x2100, // Number of ITLB page walker loads that hit in the L1I and line fill buffer
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__DTLB_L2 = 0x1200, // Number of DTLB page walker loads that hit in the L2
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__ITLB_L2 = 0x2200, // Number of ITLB page walker loads that hit in the L2
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__DTLB_L3 = 0x1400, // Number of DTLB page walker loads that hit in the L3
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__ITLB_L3 = 0x2400, // Number of ITLB page walker loads that hit in the L3
		PAGE_WALKER_LOADS__MASK__BDW_PAGE_WALKER_LOADS__DTLB_MEMORY = 0x1800, // Number of DTLB page walker loads that hit memory
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES__MASK__BDW_DSB2MITE_SWITCHES__PENALTY_CYCLES = 0x0200, // Number of DSB to MITE switch true penalty cycles
		EPT = 0x4f, // Extended page table
		EPT__MASK__BDW_EPT__WALK_CYCLES = 0x1000, // Cycles for an extended page table walk
		FP_ARITH = 0xc7, // Floating-point instructions retired
		FP_ARITH__MASK__BDW_FP_ARITH__SCALAR_DOUBLE = 0x0100, // Number of scalar double precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH__MASK__BDW_FP_ARITH__SCALAR_SINGLE = 0x0200, // Number of scalar single precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH__MASK__BDW_FP_ARITH__SCALAR = 0x0300, // Number of SSE/AVX computational scalar floating-point instructions retired. Applies to SSE* and AVX* scalar
		FP_ARITH__MASK__BDW_FP_ARITH__128B_PACKED_DOUBLE = 0x0400, // Number of scalar 128-bit packed double precision floating-point arithmetic instructions (multiply by 2 to get flops)
		FP_ARITH__MASK__BDW_FP_ARITH__128B_PACKED_SINGLE = 0x0800, // Number of scalar 128-bit packed single precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH__MASK__BDW_FP_ARITH__256B_PACKED_DOUBLE = 0x1000, // Number of scalar 256-bit packed double precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH__MASK__BDW_FP_ARITH__256B_PACKED_SINGLE = 0x2000, // Number of scalar 256-bit packed single precision floating-point arithmetic instructions (multiply by 8 to get flops)
		FP_ARITH__MASK__BDW_FP_ARITH__PACKED = 0x3c00, // Number of SSE/AVX computational packed floating-point instructions retired. Applies to SSE* and AVX*
		FP_ARITH__MASK__BDW_FP_ARITH__SINGLE = 0x2a00, // Number of SSE/AVX computational single precision floating-point instructions retired. Applies to SSE* and AVX*scalar
		FP_ARITH__MASK__BDW_FP_ARITH__DOUBLE = 0x1500, // Number of SSE/AVX computational double precision floating-point instructions retired. Applies to SSE* and AVX*scalar
		FP_ARITH_INST_RETIRED = 0xc7, // Floating-point instructions retired
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__SCALAR_DOUBLE = 0x0100, // Number of scalar double precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__SCALAR_SINGLE = 0x0200, // Number of scalar single precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__SCALAR = 0x0300, // Number of SSE/AVX computational scalar floating-point instructions retired. Applies to SSE* and AVX* scalar
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__128B_PACKED_DOUBLE = 0x0400, // Number of scalar 128-bit packed double precision floating-point arithmetic instructions (multiply by 2 to get flops)
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__128B_PACKED_SINGLE = 0x0800, // Number of scalar 128-bit packed single precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__256B_PACKED_DOUBLE = 0x1000, // Number of scalar 256-bit packed double precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__256B_PACKED_SINGLE = 0x2000, // Number of scalar 256-bit packed single precision floating-point arithmetic instructions (multiply by 8 to get flops)
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__PACKED = 0x3c00, // Number of SSE/AVX computational packed floating-point instructions retired. Applies to SSE* and AVX*
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__SINGLE = 0x2a00, // Number of SSE/AVX computational single precision floating-point instructions retired. Applies to SSE* and AVX*scalar
		FP_ARITH_INST_RETIRED__MASK__BDW_FP_ARITH__DOUBLE = 0x1500, // Number of SSE/AVX computational double precision floating-point instructions retired. Applies to SSE* and AVX*scalar
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore reqest buffer
		OFFCORE_REQUESTS_BUFFER__MASK__BDW_OFFCORE_REQUESTS_BUFFER__SQ_FULL = 0x0100, // Number of cycles the offcore requests buffer is full
		UOPS_DISPATCHES_CANCELLED = 0xa0, // Micro-ops cancelled
		UOPS_DISPATCHES_CANCELLED__MASK__BDW_UOPS_DISPATCHES_CANCELLED__SIMD_PRF = 0x0300, // Number of uops cancelled after they were dispatched from the scheduler to the execution units when the total number of physical register read ports exceeds the read bandwidth of the register file. This umask applies to instructions: DPPS
		SQ_MISC = 0xf4, // SuperQueue miscellaneous
		SQ_MISC__MASK__BDW_SQ_MISC__SPLIT_LOCK = 0x1000, // Number of split locks in the super queue (SQ)
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__DMND_CODE_RD = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__ANY_IFETCH = 0x24100, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__ANY_RFO = 0x10300, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_HITF = 1ULL << (20+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_HIT = 0xfULL << (18+8), // Alias for L3_HITMESF
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_HIT = 0xfULL << (18+8), // Alias for LLC_HITMESF
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_LOCAL = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_MISS_LOCAL = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__LLC_MISS_LOCAL_DRAM = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS__REPEAT__1 = 0xfULL << (26+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP0 = 0x1ULL << (27+8), // Supplier: counts L3 misses to remote DRAM with 0 hop
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP0_DRAM = 0x1ULL << (27+8), // Supplier: counts L3 misses to remote DRAM with 0 hop
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP1 = 0x1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP1_DRAM = 0x1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP2P = 0x1ULL << (29+8), // Supplier: counts L3 misses to remote DRAM with 2P hops
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP2P_DRAM = 0x1ULL << (29+8), // Supplier: counts L3 misses to remote DRAM with 2P hops
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_DRAM = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SPL_HIT = 0x1ULL << (30+8), // Supplier: counts L3 supplier hit
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0__MASK__BDW_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__DMND_CODE_RD = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__DMND_IFETCH = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__WB = 1ULL << (3 + 8), // Request: number of writebacks (modified to exclusive) transactions
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__PF_DATA_RD = 1ULL << (4 + 8), // Request: number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__PF_RFO = 1ULL << (5 + 8), // Request: number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__PF_IFETCH = 1ULL << (6 + 8), // Request: number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__PF_LLC_DATA_RD = 1ULL << (7 + 8), // Request: number of L3 prefetcher requests to L2 for loads
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__PF_LLC_RFO = 1ULL << (8 + 8), // Request: number of RFO requests generated by L2 prefetcher
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__PF_LLC_IFETCH = 1ULL << (9 + 8), // Request: number of L2 prefetcher requests to L3 for instruction fetches
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__BUS_LOCKS = 1ULL << (10 + 8), // Request: number bus lock and split lock requests
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__STRM_ST = 1ULL << (11 + 8), // Request: number of streaming store requests
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__ANY_IFETCH = 0x24100, // Request: combination of PF_IFETCH | DMND_IFETCH | PF_LLC_IFETCH
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__ANY_REQUEST = 0x8fff00, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__ANY_DATA = 0x9100, // Request: combination of DMND_DATA | PF_DATA_RD | PF_LLC_DATA_RD
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__ANY_RFO = 0x10300, // Request: combination of DMND_RFO | PF_RFO | PF_LLC_RFO
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_HITF = 1ULL << (20+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_HIT = 0xfULL << (18+8), // Alias for L3_HITMESF
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_HIT = 0xfULL << (18+8), // Alias for LLC_HITMESF
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_LOCAL = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_MISS_LOCAL = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__LLC_MISS_LOCAL_DRAM = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS__REPEAT__1 = 0xfULL << (26+8), // Supplier: counts L3 misses to local or remote DRAM
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP0 = 0x1ULL << (27+8), // Supplier: counts L3 misses to remote DRAM with 0 hop
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP0_DRAM = 0x1ULL << (27+8), // Supplier: counts L3 misses to remote DRAM with 0 hop
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP1 = 0x1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP1_DRAM = 0x1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP2P = 0x1ULL << (29+8), // Supplier: counts L3 misses to remote DRAM with 2P hops
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP2P_DRAM = 0x1ULL << (29+8), // Supplier: counts L3 misses to remote DRAM with 2P hops
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__L3_MISS_REMOTE_DRAM = 0x7ULL << (27+8), // Supplier: counts L3 misses to remote node
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SPL_HIT = 0x1ULL << (30+8), // Supplier: counts L3 supplier hit
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1__MASK__BDW_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};

namespace bdw = optkit::intel::bdw;