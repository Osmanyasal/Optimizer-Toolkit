#include <cstdint>

namespace optkit_intel{
	enum class nhm : uint64_t {
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		INSTRUCTION_RETIRED = 0xc0, // Count the number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		LLC_REFERENCES = 0x4f2e, // Count each request originating equiv the core to reference a cache line in the last level cache. The count may include speculation
		LAST_LEVEL_CACHE_REFERENCES = 0x4f2e, // This is an alias for LLC_REFERENCES
		LLC_MISSES = 0x412e, // Count each cache miss condition for references to the last level cache. The event count may include speculation
		LAST_LEVEL_CACHE_MISSES = 0x412e, // This is an equiv for LLC_MISSES
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		ARITH = 0x14, // Counts arithmetic multiply and divide operations
		ARITH_MASK_CYCLES_DIV_BUSY = 0x100, // Counts the number of cycles the divider is busy executing divide or square root operations. The divide can be integer
		ARITH_MASK_DIV = 0x100 | INTEL_X86_MOD_EDGE | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Counts the number of divide or square root operations. The divide can be integer
		ARITH_MASK_MUL = 0x200, // Counts the number of multiply operations executed. This includes integer as well as floating point multiply operations but excludes DPPS mul and MPSAD.
		BACLEAR = 0xe6, // Branch address calculator
		BACLEAR_MASK_BAD_TARGET = 0x200, // BACLEAR asserted with bad target address
		BACLEAR_MASK_CLEAR = 0x100, // BACLEAR asserted
		BACLEAR_FORCE_IQ = 0x1a7, // Instruction queue forced BACLEAR
		BOGUS_BR = 0x1e4, // Counts the number of bogus branches.
		BPU_CLEARS = 0xe8, // Branch prediction Unit clears
		BPU_CLEARS_MASK_EARLY = 0x100, // Early Branch Prediction Unit clears
		BPU_CLEARS_MASK_LATE = 0x200, // Late Branch Prediction Unit clears
		BPU_CLEARS_MASK_ANY = 0x300, // Count any Branch Prediction Unit clears
		BPU_MISSED_CALL_RET = 0x1e5, // Branch prediction unit missed call or return
		BR_INST_DECODED = 0x1e0, // Branch instructions decoded
		BR_INST_EXEC = 0x88, // Branch instructions executed
		BR_INST_EXEC_MASK_ANY = 0x7f00, // Branch instructions executed
		BR_INST_EXEC_MASK_COND = 0x100, // Conditional branch instructions executed
		BR_INST_EXEC_MASK_DIRECT = 0x200, // Unconditional branches executed
		BR_INST_EXEC_MASK_DIRECT_NEAR_CALL = 0x1000, // Unconditional call branches executed
		BR_INST_EXEC_MASK_INDIRECT_NEAR_CALL = 0x2000, // Indirect call branches executed
		BR_INST_EXEC_MASK_INDIRECT_NON_CALL = 0x400, // Indirect non call branches executed
		BR_INST_EXEC_MASK_NEAR_CALLS = 0x3000, // Call branches executed
		BR_INST_EXEC_MASK_NON_CALLS = 0x700, // All non call branches executed
		BR_INST_EXEC_MASK_RETURN_NEAR = 0x800, // Indirect return branches executed
		BR_INST_EXEC_MASK_TAKEN = 0x4000, // Taken branches executed
		BR_INST_RETIRED = 0xc4, // Retired branch instructions
		BR_INST_RETIRED_MASK_ALL_BRANCHES = 0x0, // Retired branch instructions (Precise Event)
		BR_INST_RETIRED_MASK_CONDITIONAL = 0x100, // Retired conditional branch instructions (Precise Event)
		BR_INST_RETIRED_MASK_NEAR_CALL = 0x200, // Retired near call instructions (Precise Event)
		BR_MISP_EXEC = 0x89, // Mispredicted branches executed
		BR_MISP_EXEC_MASK_ANY = 0x7f00, // Mispredicted branches executed
		BR_MISP_EXEC_MASK_COND = 0x100, // Mispredicted conditional branches executed
		BR_MISP_EXEC_MASK_DIRECT = 0x200, // Mispredicted unconditional branches executed
		BR_MISP_EXEC_MASK_DIRECT_NEAR_CALL = 0x1000, // Mispredicted non call branches executed
		BR_MISP_EXEC_MASK_INDIRECT_NEAR_CALL = 0x2000, // Mispredicted indirect call branches executed
		BR_MISP_EXEC_MASK_INDIRECT_NON_CALL = 0x400, // Mispredicted indirect non call branches executed
		BR_MISP_EXEC_MASK_NEAR_CALLS = 0x3000, // Mispredicted call branches executed
		BR_MISP_EXEC_MASK_NON_CALLS = 0x700, // Mispredicted non call branches executed
		BR_MISP_EXEC_MASK_RETURN_NEAR = 0x800, // Mispredicted return branches executed
		BR_MISP_EXEC_MASK_TAKEN = 0x4000, // Mispredicted taken branches executed
		BR_MISP_RETIRED = 0xc5, // Count Mispredicted Branch Activity
		BR_MISP_RETIRED_MASK_NEAR_CALL = 0x200, // Counts mispredicted direct and indirect near unconditional retired calls
		CACHE_LOCK_CYCLES = 0x63, // Cache lock cycles
		CACHE_LOCK_CYCLES_MASK_L1D = 0x200, // Cycles L1D locked
		CACHE_LOCK_CYCLES_MASK_L1D_L2 = 0x100, // Cycles L1D and L2 locked
		CPU_CLK_UNHALTED = 0x3c, // Cycles when processor is not in halted state
		CPU_CLK_UNHALTED_MASK_THREAD_P = 0x0, // Cycles when thread is not halted (programmable counter)
		CPU_CLK_UNHALTED_MASK_REF_P = 0x100, // Reference base clock (133 Mhz) cycles when thread is not halted
		CPU_CLK_UNHALTED_MASK_TOTAL_CYCLES = 0x0 | INTEL_X86_MOD_INV | (0x2 << INTEL_X86_CMASK_BIT), // Total number of elapsed cycles. Does not work when C-state enabled
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES_MASK_ANY = 0x100, // DTLB load misses
		DTLB_LOAD_MISSES_MASK_PDE_MISS = 0x2000, // DTLB load miss caused by low part of address
		DTLB_LOAD_MISSES_MASK_WALK_COMPLETED = 0x200, // DTLB load miss page walks complete
		DTLB_LOAD_MISSES_MASK_STLB_HIT = 0x1000, // DTLB second level hit
		DTLB_LOAD_MISSES_MASK_PDP_MISS = 0x4000, // Number of DTLB cache load misses where the high part of the linear to physical address translation was missed
		DTLB_LOAD_MISSES_MASK_LARGE_WALK_COMPLETED = 0x8000, // Counts number of completed large page walks due to load miss in the STLB
		DTLB_MISSES = 0x49, // Data TLB misses
		DTLB_MISSES_MASK_ANY = 0x100, // DTLB misses
		DTLB_MISSES_MASK_STLB_HIT = 0x1000, // DTLB first level misses but second level hit
		DTLB_MISSES_MASK_WALK_COMPLETED = 0x200, // DTLB miss page walks
		DTLB_MISSES_MASK_PDE_MISS = 0x2000, // Number of DTLB cache misses where the low part of the linear to physical address translation was missed
		DTLB_MISSES_MASK_PDP_MISS = 0x4000, // Number of DTLB misses where the high part of the linear to physical address translation was missed
		DTLB_MISSES_MASK_LARGE_WALK_COMPLETED = 0x8000, // Counts number of completed large page walks due to misses in the STLB
		EPT = 0x4f, // Extended Page Directory
		EPT_MASK_EPDE_MISS = 0x200, // Extended Page Directory Entry miss
		EPT_MASK_EPDPE_MISS = 0x800, // Extended Page Directory Pointer miss
		EPT_MASK_EPDPE_HIT = 0x400, // Extended Page Directory Pointer hit
		ES_REG_RENAMES = 0x1d5, // ES segment renames
		FP_ASSIST = 0xf7, // Floating point assists
		FP_ASSIST_MASK_ALL = 0x100, // Floating point assists (Precise Event)
		FP_ASSIST_MASK_INPUT = 0x400, // Floating point assists for invalid input value (Precise Event)
		FP_ASSIST_MASK_OUTPUT = 0x200, // Floating point assists for invalid output value (Precise Event)
		FP_COMP_OPS_EXE = 0x10, // Floating point computational micro-ops
		FP_COMP_OPS_EXE_MASK_MMX = 0x200, // MMX Uops
		FP_COMP_OPS_EXE_MASK_SSE_DOUBLE_PRECISION = 0x8000, // SSE* FP double precision Uops
		FP_COMP_OPS_EXE_MASK_SSE_FP = 0x400, // SSE and SSE2 FP Uops
		FP_COMP_OPS_EXE_MASK_SSE_FP_PACKED = 0x1000, // SSE FP packed Uops
		FP_COMP_OPS_EXE_MASK_SSE_FP_SCALAR = 0x2000, // SSE FP scalar Uops
		FP_COMP_OPS_EXE_MASK_SSE_SINGLE_PRECISION = 0x4000, // SSE* FP single precision Uops
		FP_COMP_OPS_EXE_MASK_SSE2_INTEGER = 0x800, // SSE2 integer Uops
		FP_COMP_OPS_EXE_MASK_X87 = 0x100, // Computational floating-point operations executed
		FP_MMX_TRANS = 0xcc, // Floating Point to and from MMX transitions
		FP_MMX_TRANS_MASK_ANY = 0x300, // All Floating Point to and from MMX transitions
		FP_MMX_TRANS_MASK_TO_FP = 0x100, // Transitions from MMX to Floating Point instructions
		FP_MMX_TRANS_MASK_TO_MMX = 0x200, // Transitions from Floating Point to MMX instructions
		IFU_IVC = 0x81, // Instruction Fetch unit victim cache
		IFU_IVC_MASK_FULL = 0x100, // Instruction Fetche unit victim cache full
		IFU_IVC_MASK_L1I_EVICTION = 0x200, // L1 Instruction cache evictions
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL_MASK_ANY = 0xf00, // Any Instruction Length Decoder stall cycles
		ILD_STALL_MASK_IQ_FULL = 0x400, // Instruction Queue full stall cycles
		ILD_STALL_MASK_LCP = 0x100, // Length Change Prefix stall cycles
		ILD_STALL_MASK_MRU = 0x200, // Stall cycles due to BPU MRU bypass
		ILD_STALL_MASK_REGEN = 0x800, // Regen stall cycles
		INST_DECODED = 0x18, // Instructions decoded
		INST_DECODED_MASK_DEC0 = 0x100, // Instructions that must be decoded by decoder 0
		INST_QUEUE_WRITES = 0x117, // Instructions written to instruction queue.
		INST_QUEUE_WRITE_CYCLES = 0x11e, // Cycles instructions are written to the instruction queue
		INST_RETIRED = 0xc0, // Instructions retired
		INST_RETIRED_MASK_ANY_P = 0x0, // Instructions Retired (Precise Event)
		INST_RETIRED_MASK_X87 = 0x200, // Retired floating-point operations (Precise Event)
		IO_TRANSACTIONS = 0x16c, // I/O transactions
		ITLB_FLUSH = 0x1ae, // Counts the number of ITLB flushes
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES_MASK_ANY = 0x100, // DTLB misses
		ITLB_MISSES_MASK_STLB_HIT = 0x1000, // DTLB first level misses but second level hit
		ITLB_MISSES_MASK_WALK_COMPLETED = 0x200, // DTLB miss page walks
		ITLB_MISSES_MASK_PDE_MISS = 0x2000, // Number of DTLB cache misses where the low part of the linear to physical address translation was missed
		ITLB_MISSES_MASK_PDP_MISS = 0x4000, // Number of DTLB misses where the high part of the linear to physical address translation was missed
		ITLB_MISSES_MASK_LARGE_WALK_COMPLETED = 0x8000, // Counts number of completed large page walks due to misses in the STLB
		ITLB_MISS_RETIRED = 0x20c8, // Retired instructions that missed the ITLB (Precise Event)
		L1D = 0x51, // L1D cache
		L1D_MASK_M_EVICT = 0x400, // L1D cache lines replaced in M state
		L1D_MASK_M_REPL = 0x200, // L1D cache lines allocated in the M state
		L1D_MASK_M_SNOOP_EVICT = 0x800, // L1D snoop eviction of cache lines in M state
		L1D_MASK_REPL = 0x100, // L1 data cache lines allocated
		L1D_ALL_REF = 0x43, // L1D references
		L1D_ALL_REF_MASK_ANY = 0x100, // All references to the L1 data cache
		L1D_ALL_REF_MASK_CACHEABLE = 0x200, // L1 data cacheable reads and writes
		L1D_CACHE_LD = 0x40, // L1D  cacheable loads. WARNING: event may overcount loads
		L1D_CACHE_LD_MASK_E_STATE = 0x400, // L1 data cache read in E state
		L1D_CACHE_LD_MASK_I_STATE = 0x100, // L1 data cache read in I state (misses)
		L1D_CACHE_LD_MASK_M_STATE = 0x800, // L1 data cache read in M state
		L1D_CACHE_LD_MASK_MESI = 0xf00, // L1 data cache reads
		L1D_CACHE_LD_MASK_S_STATE = 0x200, // L1 data cache read in S state
		L1D_CACHE_LOCK = 0x42, // L1 data cache load lock
		L1D_CACHE_LOCK_MASK_E_STATE = 0x400, // L1 data cache load locks in E state
		L1D_CACHE_LOCK_MASK_HIT = 0x100, // L1 data cache load lock hits
		L1D_CACHE_LOCK_MASK_M_STATE = 0x800, // L1 data cache load locks in M state
		L1D_CACHE_LOCK_MASK_S_STATE = 0x200, // L1 data cache load locks in S state
		L1D_CACHE_LOCK_FB_HIT = 0x153, // L1D load lock accepted in fill buffer
		L1D_CACHE_PREFETCH_LOCK_FB_HIT = 0x152, // L1D prefetch load lock accepted in fill buffer
		L1D_CACHE_ST = 0x41, // L1 data cache stores
		L1D_CACHE_ST_MASK_E_STATE = 0x400, // L1 data cache stores in E state
		L1D_CACHE_ST_MASK_I_STATE = 0x100, // L1 data cache store in the I state
		L1D_CACHE_ST_MASK_M_STATE = 0x800, // L1 data cache stores in M state
		L1D_CACHE_ST_MASK_S_STATE = 0x200, // L1 data cache stores in S state
		L1D_CACHE_ST_MASK_MESI = 0xf00, // L1 data cache store in all states
		L1D_PREFETCH = 0x4e, // L1D hardware prefetch
		L1D_PREFETCH_MASK_MISS = 0x200, // L1D hardware prefetch misses
		L1D_PREFETCH_MASK_REQUESTS = 0x100, // L1D hardware prefetch requests
		L1D_PREFETCH_MASK_TRIGGERS = 0x400, // L1D hardware prefetch requests triggered
		L1D_WB_L2 = 0x28, // L1 writebacks to L2
		L1D_WB_L2_MASK_E_STATE = 0x400, // L1 writebacks to L2 in E state
		L1D_WB_L2_MASK_I_STATE = 0x100, // L1 writebacks to L2 in I state (misses)
		L1D_WB_L2_MASK_M_STATE = 0x800, // L1 writebacks to L2 in M state
		L1D_WB_L2_MASK_S_STATE = 0x200, // L1 writebacks to L2 in S state
		L1D_WB_L2_MASK_MESI = 0xf00, // All L1 writebacks to L2
		L1I = 0x80, // L1I instruction fetches
		L1I_MASK_CYCLES_STALLED = 0x400, // L1I instruction fetch stall cycles
		L1I_MASK_HITS = 0x100, // L1I instruction fetch hits
		L1I_MASK_MISSES = 0x200, // L1I instruction fetch misses
		L1I_MASK_READS = 0x300, // L1I Instruction fetches
		L1I_OPPORTUNISTIC_HITS = 0x183, // Opportunistic hits in streaming
		L2_DATA_RQSTS = 0x26, // L2 data requests
		L2_DATA_RQSTS_MASK_ANY = 0xff00, // All L2 data requests
		L2_DATA_RQSTS_MASK_DEMAND_E_STATE = 0x400, // L2 data demand loads in E state
		L2_DATA_RQSTS_MASK_DEMAND_I_STATE = 0x100, // L2 data demand loads in I state (misses)
		L2_DATA_RQSTS_MASK_DEMAND_M_STATE = 0x800, // L2 data demand loads in M state
		L2_DATA_RQSTS_MASK_DEMAND_MESI = 0xf00, // L2 data demand requests
		L2_DATA_RQSTS_MASK_DEMAND_S_STATE = 0x200, // L2 data demand loads in S state
		L2_DATA_RQSTS_MASK_PREFETCH_E_STATE = 0x4000, // L2 data prefetches in E state
		L2_DATA_RQSTS_MASK_PREFETCH_I_STATE = 0x1000, // L2 data prefetches in the I state (misses)
		L2_DATA_RQSTS_MASK_PREFETCH_M_STATE = 0x8000, // L2 data prefetches in M state
		L2_DATA_RQSTS_MASK_PREFETCH_MESI = 0xf000, // All L2 data prefetches
		L2_DATA_RQSTS_MASK_PREFETCH_S_STATE = 0x2000, // L2 data prefetches in the S state
		L2_HW_PREFETCH = 0xf3, // L2 HW prefetches
		L2_HW_PREFETCH_MASK_HIT = 0x100, // Count L2 HW prefetcher detector hits
		L2_HW_PREFETCH_MASK_ALLOC = 0x200, // Count L2 HW prefetcher allocations
		L2_HW_PREFETCH_MASK_DATA_TRIGGER = 0x400, // Count L2 HW data prefetcher triggered
		L2_HW_PREFETCH_MASK_CODE_TRIGGER = 0x800, // Count L2 HW code prefetcher triggered
		L2_HW_PREFETCH_MASK_DCA_TRIGGER = 0x1000, // Count L2 HW DCA prefetcher triggered
		L2_HW_PREFETCH_MASK_KICK_START = 0x2000, // Count L2 HW prefetcher kick started
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN_MASK_ANY = 0x700, // L2 lines allocated
		L2_LINES_IN_MASK_E_STATE = 0x400, // L2 lines allocated in the E state
		L2_LINES_IN_MASK_S_STATE = 0x200, // L2 lines allocated in the S state
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT_MASK_ANY = 0xf00, // L2 lines evicted
		L2_LINES_OUT_MASK_DEMAND_CLEAN = 0x100, // L2 lines evicted by a demand request
		L2_LINES_OUT_MASK_DEMAND_DIRTY = 0x200, // L2 modified lines evicted by a demand request
		L2_LINES_OUT_MASK_PREFETCH_CLEAN = 0x400, // L2 lines evicted by a prefetch request
		L2_LINES_OUT_MASK_PREFETCH_DIRTY = 0x800, // L2 modified lines evicted by a prefetch request
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS_MASK_MISS = 0xaa00, // All L2 misses
		L2_RQSTS_MASK_REFERENCES = 0xff00, // All L2 requests
		L2_RQSTS_MASK_IFETCH_HIT = 0x1000, // L2 instruction fetch hits
		L2_RQSTS_MASK_IFETCH_MISS = 0x2000, // L2 instruction fetch misses
		L2_RQSTS_MASK_IFETCHES = 0x3000, // L2 instruction fetches
		L2_RQSTS_MASK_LD_HIT = 0x100, // L2 load hits
		L2_RQSTS_MASK_LD_MISS = 0x200, // L2 load misses
		L2_RQSTS_MASK_LOADS = 0x300, // L2 requests
		L2_RQSTS_MASK_PREFETCH_HIT = 0x4000, // L2 prefetch hits
		L2_RQSTS_MASK_PREFETCH_MISS = 0x8000, // L2 prefetch misses
		L2_RQSTS_MASK_PREFETCHES = 0xc000, // All L2 prefetches
		L2_RQSTS_MASK_RFO_HIT = 0x400, // L2 RFO hits
		L2_RQSTS_MASK_RFO_MISS = 0x800, // L2 RFO misses
		L2_RQSTS_MASK_RFOS = 0xc00, // L2 RFO requests
		L2_TRANSACTIONS = 0xf0, // L2 transactions
		L2_TRANSACTIONS_MASK_ANY = 0x8000, // All L2 transactions
		L2_TRANSACTIONS_MASK_FILL = 0x2000, // L2 fill transactions
		L2_TRANSACTIONS_MASK_IFETCH = 0x400, // L2 instruction fetch transactions
		L2_TRANSACTIONS_MASK_L1D_WB = 0x1000, // L1D writeback to L2 transactions
		L2_TRANSACTIONS_MASK_LOAD = 0x100, // L2 Load transactions
		L2_TRANSACTIONS_MASK_PREFETCH = 0x800, // L2 prefetch transactions
		L2_TRANSACTIONS_MASK_RFO = 0x200, // L2 RFO transactions
		L2_TRANSACTIONS_MASK_WB = 0x4000, // L2 writeback to LLC transactions
		L2_WRITE = 0x27, // L2 demand lock/store RFO
		L2_WRITE_MASK_LOCK_E_STATE = 0x4000, // L2 demand lock RFOs in E state
		L2_WRITE_MASK_LOCK_I_STATE = 0x1000, // L2 demand lock RFOs in I state (misses)
		L2_WRITE_MASK_LOCK_S_STATE = 0x2000, // L2 demand lock RFOs in S state
		L2_WRITE_MASK_LOCK_HIT = 0xe000, // All demand L2 lock RFOs that hit the cache
		L2_WRITE_MASK_LOCK_M_STATE = 0x8000, // L2 demand lock RFOs in M state
		L2_WRITE_MASK_LOCK_MESI = 0xf000, // All demand L2 lock RFOs
		L2_WRITE_MASK_RFO_HIT = 0xe00, // All L2 demand store RFOs that hit the cache
		L2_WRITE_MASK_RFO_I_STATE = 0x100, // L2 demand store RFOs in I state (misses)
		L2_WRITE_MASK_RFO_E_STATE = 0x400, // L2 demand store RFOs in the E state (exclusive)
		L2_WRITE_MASK_RFO_M_STATE = 0x800, // L2 demand store RFOs in M state
		L2_WRITE_MASK_RFO_MESI = 0xf00, // All L2 demand store RFOs
		L2_WRITE_MASK_RFO_S_STATE = 0x200, // L2 demand store RFOs in S state
		LARGE_ITLB = 0x82, // Large instruction TLB
		LARGE_ITLB_MASK_HIT = 0x100, // Large ITLB hit
		LOAD_DISPATCH = 0x13, // Loads dispatched
		LOAD_DISPATCH_MASK_ANY = 0x700, // All loads dispatched
		LOAD_DISPATCH_MASK_MOB = 0x400, // Loads dispatched from the MOB
		LOAD_DISPATCH_MASK_RS = 0x100, // Loads dispatched that bypass the MOB
		LOAD_DISPATCH_MASK_RS_DELAYED = 0x200, // Loads dispatched from stage 305
		LOAD_HIT_PRE = 0x14c, // Load operations conflicting with software prefetches
		LONGEST_LAT_CACHE = 0x2e, // Longest latency cache reference
		LONGEST_LAT_CACHE_MASK_REFERENCE = 0x4f00, // Longest latency cache reference
		LONGEST_LAT_CACHE_MASK_MISS = 0x4100, // Longest latency cache miss
		LSD = 0xa8, // Loop stream detector
		LSD_MASK_ACTIVE = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when uops were delivered by the LSD
		LSD_MASK_INACTIVE = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no uops were delivered by the LSD
		MACHINE_CLEARS = 0xc3, // Machine Clear
		MACHINE_CLEARS_MASK_SMC = 0x400, // Self-Modifying Code detected
		MACHINE_CLEARS_MASK_CYCLES = 0x100, // Cycles machine clear asserted
		MACHINE_CLEARS_MASK_MEM_ORDER = 0x200, // Execution pipeline restart due to Memory ordering conflicts
		MACHINE_CLEARS_MASK_FUSION_ASSIST = 0x1000, // Counts the number of macro-fusion assists
		MACRO_INSTS = 0xd0, // Macro-fused instructions
		MACRO_INSTS_MASK_DECODED = 0x100, // Instructions decoded
		MACRO_INSTS_MASK_FUSIONS_DECODED = 0x100, // Macro-fused instructions decoded
		MEMORY_DISAMBIGUATION = 0x9, // Memory Disambiguation Activity
		MEMORY_DISAMBIGUATION_MASK_RESET = 0x100, // Counts memory disambiguation reset cycles
		MEMORY_DISAMBIGUATION_MASK_WATCHDOG = 0x400, // Counts the number of times the memory disambiguation watchdog kicked in
		MEMORY_DISAMBIGUATION_MASK_WATCH_CYCLES = 0x800, // Counts the cycles that the memory disambiguation watchdog is active
		MEM_INST_RETIRED = 0xb, // Memory instructions retired
		MEM_INST_RETIRED_MASK_LATENCY_ABOVE_THRESHOLD = 0x1000, // Memory instructions retired above programmed clocks
		MEM_INST_RETIRED_MASK_LOADS = 0x100, // Instructions retired which contains a load (Precise Event)
		MEM_INST_RETIRED_MASK_STORES = 0x200, // Instructions retired which contains a store (Precise Event)
		MEM_LOAD_RETIRED = 0xcb, // Retired loads
		MEM_LOAD_RETIRED_MASK_DTLB_MISS = 0x8000, // Retired loads that miss the DTLB (Precise Event)
		MEM_LOAD_RETIRED_MASK_HIT_LFB = 0x4000, // Retired loads that miss L1D and hit an previously allocated LFB (Precise Event)
		MEM_LOAD_RETIRED_MASK_L1D_HIT = 0x100, // Retired loads that hit the L1 data cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L2_HIT = 0x200, // Retired loads that hit the L2 cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_L3_MISS = 0x1000, // Retired loads that miss the L3 cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_LLC_MISS = 0x1000, // This is an alias for L3_MISS
		MEM_LOAD_RETIRED_MASK_L3_UNSHARED_HIT = 0x400, // Retired loads that hit valid versions in the L3 cache (Precise Event)
		MEM_LOAD_RETIRED_MASK_LLC_UNSHARED_HIT = 0x400, // This is an alias for L3_UNSHARED_HIT
		MEM_LOAD_RETIRED_MASK_OTHER_CORE_L2_HIT_HITM = 0x800, // Retired loads that hit sibling core's L2 in modified or unmodified states (Precise Event)
		MEM_STORE_RETIRED = 0xc, // Retired stores
		MEM_STORE_RETIRED_MASK_DTLB_MISS = 0x100, // Retired stores that miss the DTLB (Precise Event)
		MEM_UNCORE_RETIRED = 0xf, // Load instructions retired which hit offcore
		MEM_UNCORE_RETIRED_MASK_OTHER_CORE_L2_HITM = 0x200, // Load instructions retired that HIT modified data in sibling core (Precise Event)
		MEM_UNCORE_RETIRED_MASK_REMOTE_CACHE_LOCAL_HOME_HIT = 0x800, // Load instructions retired remote cache HIT data source (Precise Event)
		MEM_UNCORE_RETIRED_MASK_REMOTE_DRAM = 0x1000, // Load instructions retired remote DRAM and remote home-remote cache HITM (Precise Event)
		MEM_UNCORE_RETIRED_MASK_LOCAL_DRAM = 0x2000, // Load instructions retired with a data source of local DRAM or locally homed remote hitm (Precise Event)
		MEM_UNCORE_RETIRED_MASK_L3_DATA_MISS_UNKNOWN = 0x100, // Load instructions retired where the memory reference missed L3 and data source is unknown (Model 46 only
		MEM_UNCORE_RETIRED_MASK_UNCACHEABLE = 0x8000, // Load instructions retired where the memory reference missed L1
		OFFCORE_REQUESTS = 0xb0, // Offcore memory requests
		OFFCORE_REQUESTS_MASK_ANY = 0x8000, // All offcore requests
		OFFCORE_REQUESTS_MASK_ANY_READ = 0x800, // Offcore read requests
		OFFCORE_REQUESTS_MASK_ANY_RFO = 0x1000, // Offcore RFO requests
		OFFCORE_REQUESTS_MASK_DEMAND_READ_CODE = 0x200, // Counts number of offcore demand code read requests. Does not count L2 prefetch requests.
		OFFCORE_REQUESTS_MASK_DEMAND_READ_DATA = 0x100, // Offcore demand data read requests
		OFFCORE_REQUESTS_MASK_DEMAND_RFO = 0x400, // Offcore demand RFO requests
		OFFCORE_REQUESTS_MASK_L1D_WRITEBACK = 0x4000, // Offcore L1 data cache writebacks
		OFFCORE_REQUESTS_MASK_UNCACHED_MEM = 0x2000, // Counts number of offcore uncached memory requests
		OFFCORE_REQUESTS_SQ_FULL = 0x1b2, // Counts cycles the Offcore Request buffer or Super Queue is full.
		PARTIAL_ADDRESS_ALIAS = 0x107, // False dependencies due to partial address forming
		PIC_ACCESSES = 0xba, // Programmable interrupt controller
		PIC_ACCESSES_MASK_TPR_READS = 0x100, // Counts number of TPR reads
		PIC_ACCESSES_MASK_TPR_WRITES = 0x200, // Counts number of TPR writes
		RAT_STALLS = 0xd2, // Register allocation table stalls
		RAT_STALLS_MASK_FLAGS = 0x100, // Flag stall cycles
		RAT_STALLS_MASK_REGISTERS = 0x200, // Partial register stall cycles
		RAT_STALLS_MASK_ROB_READ_PORT = 0x400, // ROB read port stalls cycles
		RAT_STALLS_MASK_SCOREBOARD = 0x800, // Scoreboard stall cycles
		RAT_STALLS_MASK_ANY = 0xf00, // All RAT stall cycles
		RESOURCE_STALLS = 0xa2, // Processor stalls
		RESOURCE_STALLS_MASK_FPCW = 0x2000, // FPU control word write stall cycles
		RESOURCE_STALLS_MASK_LOAD = 0x200, // Load buffer stall cycles
		RESOURCE_STALLS_MASK_MXCSR = 0x4000, // MXCSR rename stall cycles
		RESOURCE_STALLS_MASK_RS_FULL = 0x400, // Reservation Station full stall cycles
		RESOURCE_STALLS_MASK_STORE = 0x800, // Store buffer stall cycles
		RESOURCE_STALLS_MASK_OTHER = 0x8000, // Other Resource related stall cycles
		RESOURCE_STALLS_MASK_ROB_FULL = 0x1000, // ROB full stall cycles
		RESOURCE_STALLS_MASK_ANY = 0x100, // Resource related stall cycles
		SEG_RENAME_STALLS = 0x1d4, // Segment rename stall cycles
		SEGMENT_REG_LOADS = 0x1f8, // Counts number of segment register loads
		SIMD_INT_128 = 0x12, // 128 bit SIMD integer operations
		SIMD_INT_128_MASK_PACK = 0x400, // 128 bit SIMD integer pack operations
		SIMD_INT_128_MASK_PACKED_ARITH = 0x2000, // 128 bit SIMD integer arithmetic operations
		SIMD_INT_128_MASK_PACKED_LOGICAL = 0x1000, // 128 bit SIMD integer logical operations
		SIMD_INT_128_MASK_PACKED_MPY = 0x100, // 128 bit SIMD integer multiply operations
		SIMD_INT_128_MASK_PACKED_SHIFT = 0x200, // 128 bit SIMD integer shift operations
		SIMD_INT_128_MASK_SHUFFLE_MOVE = 0x4000, // 128 bit SIMD integer shuffle/move operations
		SIMD_INT_128_MASK_UNPACK = 0x800, // 128 bit SIMD integer unpack operations
		SIMD_INT_64 = 0xfd, // 64 bit SIMD integer operations
		SIMD_INT_64_MASK_PACK = 0x400, // SIMD integer 64 bit pack operations
		SIMD_INT_64_MASK_PACKED_ARITH = 0x2000, // SIMD integer 64 bit arithmetic operations
		SIMD_INT_64_MASK_PACKED_LOGICAL = 0x1000, // SIMD integer 64 bit logical operations
		SIMD_INT_64_MASK_PACKED_MPY = 0x100, // SIMD integer 64 bit packed multiply operations
		SIMD_INT_64_MASK_PACKED_SHIFT = 0x200, // SIMD integer 64 bit shift operations
		SIMD_INT_64_MASK_SHUFFLE_MOVE = 0x4000, // SIMD integer 64 bit shuffle/move operations
		SIMD_INT_64_MASK_UNPACK = 0x800, // SIMD integer 64 bit unpack operations
		SNOOP_RESPONSE = 0xb8, // Snoop
		SNOOP_RESPONSE_MASK_HIT = 0x100, // Thread responded HIT to snoop
		SNOOP_RESPONSE_MASK_HITE = 0x200, // Thread responded HITE to snoop
		SNOOP_RESPONSE_MASK_HITM = 0x400, // Thread responded HITM to snoop
		SQ_FULL_STALL_CYCLES = 0x1f6, // Counts cycles the Offcore Request buffer or Super Queue is full and request(s) are outstanding.
		SQ_MISC = 0xf4, // Super Queue Activity Related to L2 Cache Access
		SQ_MISC_MASK_PROMOTION = 0x100, // Counts the number of L2 secondary misses that hit the Super Queue
		SQ_MISC_MASK_PROMOTION_POST_GO = 0x200, // Counts the number of L2 secondary misses during the Super Queue filling L2
		SQ_MISC_MASK_LRU_HINTS = 0x400, // Counts number of Super Queue LRU hints sent to L3
		SQ_MISC_MASK_FILL_DROPPED = 0x800, // Counts the number of SQ L2 fills dropped due to L2 busy
		SQ_MISC_MASK_SPLIT_LOCK = 0x1000, // Super Queue lock splits across a cache line
		SSE_MEM_EXEC = 0x4b, // Streaming SIMD executed
		SSE_MEM_EXEC_MASK_NTA = 0x100, // Streaming SIMD L1D NTA prefetch miss
		SSEX_UOPS_RETIRED = 0xc7, // SIMD micro-ops retired
		SSEX_UOPS_RETIRED_MASK_PACKED_DOUBLE = 0x400, // SIMD Packed-Double Uops retired (Precise Event)
		SSEX_UOPS_RETIRED_MASK_PACKED_SINGLE = 0x100, // SIMD Packed-Single Uops retired (Precise Event)
		SSEX_UOPS_RETIRED_MASK_SCALAR_DOUBLE = 0x800, // SIMD Scalar-Double Uops retired (Precise Event)
		SSEX_UOPS_RETIRED_MASK_SCALAR_SINGLE = 0x200, // SIMD Scalar-Single Uops retired (Precise Event)
		SSEX_UOPS_RETIRED_MASK_VECTOR_INTEGER = 0x1000, // SIMD Vector Integer Uops retired (Precise Event)
		STORE_BLOCKS = 0x6, // Delayed loads
		STORE_BLOCKS_MASK_AT_RET = 0x400, // Loads delayed with at-Retirement block code
		STORE_BLOCKS_MASK_L1D_BLOCK = 0x800, // Cacheable loads delayed with L1D block code
		STORE_BLOCKS_MASK_NOT_STA = 0x100, // Loads delayed due to a store blocked for unknown data
		STORE_BLOCKS_MASK_STA = 0x200, // Loads delayed due to a store blocked for an unknown address
		TWO_UOP_INSTS_DECODED = 0x119, // Two micro-ops instructions decoded
		UOPS_DECODED_DEC0 = 0x13d, // Micro-ops decoded by decoder 0
		UOPS_DECODED = 0xd1, // Micro-ops decoded
		UOPS_DECODED_MASK_ESP_FOLDING = 0x400, // Stack pointer instructions decoded
		UOPS_DECODED_MASK_ESP_SYNC = 0x800, // Stack pointer sync operations
		UOPS_DECODED_MASK_MS = 0x200, // Uops decoded by Microcode Sequencer
		UOPS_DECODED_MASK_MS_CYCLES_ACTIVE = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles in which at least one uop is decoded by Microcode Sequencer
		UOPS_EXECUTED = 0xb1, // Micro-ops executed
		UOPS_EXECUTED_MASK_PORT0 = 0x100, // Uops executed on port 0
		UOPS_EXECUTED_MASK_PORT1 = 0x200, // Uops executed on port 1
		UOPS_EXECUTED_MASK_PORT2_CORE = 0x400 | INTEL_X86_MOD_ANY, // Uops executed on port 2 on any thread (core count only)
		UOPS_EXECUTED_MASK_PORT3_CORE = 0x800 | INTEL_X86_MOD_ANY, // Uops executed on port 3 on any thread (core count only)
		UOPS_EXECUTED_MASK_PORT4_CORE = 0x1000 | INTEL_X86_MOD_ANY, // Uops executed on port 4 on any thread (core count only)
		UOPS_EXECUTED_MASK_PORT5 = 0x2000, // Uops executed on port 5
		UOPS_EXECUTED_MASK_PORT015 = 0x4000, // Uops issued on ports 0
		UOPS_EXECUTED_MASK_PORT234_CORE = 0x8000 | INTEL_X86_MOD_ANY, // Uops issued on ports 2
		UOPS_EXECUTED_MASK_PORT015_STALL_CYCLES = 0x4000 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles no Uops issued on ports 0
		UOPS_ISSUED = 0xe, // Micro-ops issued
		UOPS_ISSUED_MASK_ANY = 0x100, // Uops issued
		UOPS_ISSUED_MASK_STALLED_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles stalled no issued uops
		UOPS_ISSUED_MASK_FUSED = 0x200, // Fused Uops issued
		UOPS_RETIRED = 0xc2, // Micro-ops retired
		UOPS_RETIRED_MASK_ANY = 0x100, // Uops retired (Precise Event)
		UOPS_RETIRED_MASK_RETIRE_SLOTS = 0x200, // Retirement slots used (Precise Event)
		UOPS_RETIRED_MASK_ACTIVE_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles Uops are being retired (Precise Event)
		UOPS_RETIRED_MASK_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (0x1 << INTEL_X86_CMASK_BIT), // Cycles No Uops retired (Precise Event)
		UOPS_RETIRED_MASK_MACRO_FUSED = 0x400, // Macro-fused Uops retired (Precise Event)
		UOP_UNFUSION = 0x1db, // Micro-ops unfusions due to FP exceptions
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response 0 (must provide at least one request and one response umasks)
		OFFCORE_RESPONSE_0_MASK_DMND_DATA_RD = 0x100, // Request: counts the number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0_MASK_DMND_RFO = 0x200, // Request: counts the number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO
		OFFCORE_RESPONSE_0_MASK_DMND_IFETCH = 0x400, // Request: counts the number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0_MASK_WB = 0x800, // Request: counts the number of writeback (modified to exclusive) transactions
		OFFCORE_RESPONSE_0_MASK_PF_DATA_RD = 0x1000, // Request: counts the number of data cacheline reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_RFO = 0x2000, // Request: counts the number of RFO requests generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_PF_IFETCH = 0x4000, // Request: counts the number of code reads generated by L2 prefetchers
		OFFCORE_RESPONSE_0_MASK_OTHER = 0x8000, // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0_MASK_ANY_IFETCH = 0x4400, // Request: combination of PF_IFETCH | DMND_IFETCH
		OFFCORE_RESPONSE_0_MASK_ANY_REQUEST = 0xff00, // Request: combination of all requests umasks
		OFFCORE_RESPONSE_0_MASK_ANY_DATA = 0x3300, // Request: any data read/write request
		OFFCORE_RESPONSE_0_MASK_ANY_DATA_RD = 0x1100, // Request: any data read in request
		OFFCORE_RESPONSE_0_MASK_ANY_RFO = 0x2200, // Request: combination of DMND_RFO | PF_RFO
		OFFCORE_RESPONSE_0_MASK_UNCORE_HIT = 0x10000, // Response: counts L3 Hit: local or remote home requests that hit L3 cache in the uncore with no coherency actions required (snooping)
		OFFCORE_RESPONSE_0_MASK_OTHER_CORE_HIT_SNP = 0x20000, // Response: counts L3 Hit: local or remote home requests that hit L3 cache in the uncore and was serviced by another core with a cross core snoop where no modified copies were found (clean)
		OFFCORE_RESPONSE_0_MASK_OTHER_CORE_HITM = 0x40000, // Response: counts L3 Hit: local or remote home requests that hit L3 cache in the uncore and was serviced by another core with a cross core snoop where modified copies were found (HITM)
		OFFCORE_RESPONSE_0_MASK_REMOTE_CACHE_HITM = 0x80000, // Response: counts L3 Hit: local or remote home requests that hit a remote L3 cacheline in modified (HITM) state
		OFFCORE_RESPONSE_0_MASK_REMOTE_CACHE_FWD = 0x100000, // Response: counts L3 Miss: local homed requests that missed the L3 cache and was serviced by forwarded data following a cross package snoop where no modified copies found. (Remote home requests are not counted)
		OFFCORE_RESPONSE_0_MASK_REMOTE_DRAM = 0x200000, // Response: counts L3 Miss: remote home requests that missed the L3 cache and were serviced by remote DRAM
		OFFCORE_RESPONSE_0_MASK_LOCAL_DRAM = 0x400000, // Response: counts L3 Miss: local home requests that missed the L3 cache and were serviced by local DRAM
		OFFCORE_RESPONSE_0_MASK_NON_DRAM = 0x800000, // Response: Non-DRAM requests that were serviced by IOH
		OFFCORE_RESPONSE_0_MASK_ANY_CACHE_DRAM = 0x7f0000, // Response: requests serviced by any source but IOH
		OFFCORE_RESPONSE_0_MASK_ANY_DRAM = 0x600000, // Response: requests serviced by local or remote DRAM
		OFFCORE_RESPONSE_0_MASK_ANY_LLC_MISS = 0xf80000, // Response: requests that missed in L3
		OFFCORE_RESPONSE_0_MASK_LOCAL_CACHE_DRAM = 0x470000, // Response: requests hit local core or uncore caches or local DRAM
		OFFCORE_RESPONSE_0_MASK_REMOTE_CACHE_DRAM = 0x380000, // Response: requests that miss L3 and hit remote caches or DRAM
		OFFCORE_RESPONSE_0_MASK_ANY_RESPONSE = 0xff0000, // Response: combination of all response umasks
		};};