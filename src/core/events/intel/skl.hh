#pragma once
#include <cstdint>
#include "src/intel_priv.hh"
namespace optkit::intel::skl{
	enum skl : uint64_t {
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // This is an alias for INSTRUCTION_RETIRED
		BRANCH_INSTRUCTIONS_RETIRED = 0xc4, // Count branch instructions at retirement. Specifically
		MISPREDICTED_BRANCH_RETIRED = 0xc5, // Count mispredicted branch instructions at retirement. Specifically
		BACLEARS = 0xe6, // Branch re-steered
		BACLEARS__MASK__SKL_BACLEARS__ANY = 0x0100, // Number of front-end re-steers due to BPU misprediction
		BR_INST_RETIRED = 0xc4, // Branch instructions retired (Precise Event)
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__CONDITIONAL = 0x100, // Counts all taken and not taken macro conditional branch instructions
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__COND = 0x100, // Counts all taken and not taken macro conditional branch instructions
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__NEAR_CALL = 0x200, // Counts all macro direct and indirect near calls
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__ALL_BRANCHES = 0x0, // Counts all taken and not taken macro branches including far branches (architectural event)
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__NEAR_RETURN = 0x800, // Counts the number of near ret instructions retired
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__NOT_TAKEN = 0x1000, // Counts all not taken macro branch instructions retired
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__NEAR_TAKEN = 0x2000, // Counts the number of near branch taken instructions retired
		BR_INST_RETIRED__MASK__SKL_BR_INST_RETIRED__FAR_BRANCH = 0x4000, // Counts the number of far branch instructions retired
		BR_MISP_RETIRED = 0xc5, // Mispredicted retired branches (Precise Event)
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__CONDITIONAL = 0x100, // All mispredicted macro conditional branch instructions
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__COND = 0x100, // All mispredicted macro conditional branch instructions
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__ALL_BRANCHES = 0x0, // All mispredicted macro branches (architectural event)
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__NEAR_TAKEN = 0x2000, // Number of near branch instructions retired that were mispredicted and taken
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__NEAR_CALL = 0x200, // Counts both taken and not taken retired mispredicted direct and indirect near calls
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__RET = 0x0800ull, // This event counts the number of mispredicted ret instructions retired.
		BR_MISP_RETIRED__MASK__SKL_BR_MISP_RETIRED__NEAR_RETURN = 0x0800ull, // This event counts the number of mispredicted ret instructions retired.
		BR_MISP_EXEC = 0x89, // Speculative mispredicted branches
		BR_MISP_EXEC__MASK__SKL_BR_MISP_EXEC__INDIRECT = 0xe400, // Speculative mispredicted indirect branches
		BR_MISP_EXEC__MASK__SKL_BR_MISP_EXEC__ALL_BRANCHES = 0xff00, // Speculative and retired mispredicted macro conditional branches
		CPU_CLK_THREAD_UNHALTED = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		CPU_CLK_THREAD_UNHALTED__MASK__SKL_CPU_CLK_THREAD_UNHALTED__REF_XCLK = 0x100, // Count Xclk pulses (100Mhz) when the core is unhalted
		CPU_CLK_THREAD_UNHALTED__MASK__SKL_CPU_CLK_THREAD_UNHALTED__REF_XCLK_ANY = 0x100 | INTEL_X86_MOD_ANY, // Count Xclk pulses (100Mhz) when the at least one thread on the physical core is unhalted
		CPU_CLK_THREAD_UNHALTED__MASK__SKL_CPU_CLK_THREAD_UNHALTED__REF_P = 0x100, // Cycles when the core is unhalted (count at 100 Mhz)
		CPU_CLK_THREAD_UNHALTED__MASK__SKL_CPU_CLK_THREAD_UNHALTED__THREAD_P = 0x000, // Cycles when thread is not halted
		CPU_CLK_THREAD_UNHALTED__MASK__SKL_CPU_CLK_THREAD_UNHALTED__ONE_THREAD_ACTIVE = 0x200, // Counts Xclk (100Mhz) pulses when this thread is unhalted and the other thread is halted
		CPU_CLK_THREAD_UNHALTED__MASK__SKL_CPU_CLK_THREAD_UNHALTED__RING0_TRANS = 0x000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Counts when the current privilege level transitions from ring 1
		CPU_CLK_UNHALTED = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		CYCLE_ACTIVITY = 0xa3, // Stalled cycles
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_L2_MISS = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss demand loads outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_L2_PENDING = 0x0100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with pending L2 miss demand loads outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_L3_MISS = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with L3 cache miss demand loads outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_LDM_PENDING = 0x0200 | (0x2 << INTEL_X86_CMASK_BIT), // Cycles with L3 cache miss demand loads outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_L1D_MISS = 0x0800 | (0x8 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_L1D_PENDING = 0x0800 | (0x8 << INTEL_X86_CMASK_BIT), // Cycles with pending L1D load cache misses
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__CYCLES_MEM_ANY = 0x1000 | (0x10 << INTEL_X86_CMASK_BIT), // Cycles when memory subsystem has at least one outstanding load
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__STALLS_L1D_MISS = 0x0c00 | (0xc << INTEL_X86_CMASK_BIT), // Execution stalls while at least one L1D demand load cache miss is outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__STALLS_L2_MISS = 0x0500 | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls while at least one L2 demand load is outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__STALLS_L3_MISS = 0x0600 | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls while at least one L3 demand load is outstanding
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__STALLS_MEM_ANY = 0x1400 | (20 << INTEL_X86_CMASK_BIT), // Execution stalls while at least one demand load is outstanding in the memory subsystem
		CYCLE_ACTIVITY__MASK__SKL_CYCLE_ACTIVITY__STALLS_TOTAL = 0x0400 | (0x4 << INTEL_X86_CMASK_BIT), // Total execution stalls in cycles
		DTLB_LOAD_MISSES = 0x8, // Data TLB load misses
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED = 0xe00, // Number of misses in all TLB levels causing a page walk of any page size that completes
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED_4K = 0x200, // Number of misses in all TLB levels causing a page walk of 4KB page size that completes
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED_2M_4M = 0x400, // Number of misses in all TLB levels causing a page walk of 2MB/4MB page size that completes
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED_1G = 0x800, // Number of misses in all TLB levels causing a page walk of 1GB page size that completes
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_ACTIVE = 0x1000 | (0x1 <<  INTEL_X86_CMASK_BIT), // Cycles with at least one hardware walker active for a load
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_DURATION = 0x1000, // Cycles when hardware page walker is busy with page walks
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_PENDING = 0x1000, // Cycles when hardware page walker is busy with page walks
		DTLB_LOAD_MISSES__MASK__SKL_DTLB_LOAD_MISSES__STLB_HIT = 0x2000, // Number of cache load STLB hits. No page walk
		DTLB_STORE_MISSES = 0x49, // Data TLB store misses
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED = 0xe00, // Number of misses in all TLB levels causing a page walk of any page size that completes
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED_4K = 0x200, // Number of misses in all TLB levels causing a page walk of 4KB page size that completes
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED_2M_4M = 0x400, // Number of misses in all TLB levels causing a page walk of 2MB/4MB page size that completes
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_COMPLETED_1G = 0x800, // Number of misses in all TLB levels causing a page walk of 1GB page size that completes
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_ACTIVE = 0x1000 | (0x1 <<  INTEL_X86_CMASK_BIT), // Cycles with at least one hardware walker active for a load
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_DURATION = 0x1000, // Cycles when hardware page walker is busy with page walks
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__WALK_PENDING = 0x1000, // Cycles when hardware page walker is busy with page walks
		DTLB_STORE_MISSES__MASK__SKL_DTLB_LOAD_MISSES__STLB_HIT = 0x2000, // Number of cache load STLB hits. No page walk
		FP_ASSIST = 0xca, // X87 floating-point assists
		FP_ASSIST__MASK__SKL_FP_ASSIST__ANY = 0x1e00 | (1 << INTEL_X86_CMASK_BIT), // Cycles with any input/output SEE or FP assists
		HLE_RETIRED = 0xc8, // HLE execution (Precise Event)
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__START = 0x100, // Number of times an HLE execution started
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__COMMIT = 0x200, // Number of times an HLE execution successfully committed
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__ABORTED = 0x400, // Number of times an HLE execution aborted due to any reasons (multiple categories may count as one) (Precise Event)
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__ABORTED_MEM = 0x800, // Number of times an HLE execution aborted due to various memory events
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__ABORTED_TMR = 0x1000, // Number of times an HLE execution aborted due to hardware timer expiration
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__ABORTED_UNFRIENDLY = 0x2000, // Number of times an HLE execution aborted due to HLE-unfriendly instructions and certain events such as AD-assists
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__ABORTED_MEMTYPE = 0x4000, // Number of times an HLE execution aborted due to incompatible memory type
		HLE_RETIRED__MASK__SKL_HLE_RETIRED__ABORTED_EVENTS = 0x8000, // Number of times an HLE execution aborted due to none of the other 4 reasons (e.g.
		ICACHE_16B = 0x80, // Instruction Cache
		ICACHE_16B__MASK__SKL_ICACHE_16B__IFDATA_STALL = 0x400, // Cycles where a code fetch is stalled due to L1 instruction cache miss
		ICACHE_64B = 0x83, // Instruction Cache
		ICACHE_64B__MASK__SKL_ICACHE_64B__IFTAG_HIT = 0x100, // Number of instruction fetch tag lookups that hit in the instruction cache (L1I). Counts at 64-byte cache-line granularity
		ICACHE_64B__MASK__SKL_ICACHE_64B__IFTAG_MISS = 0x200, // Number of instruction fetch tag lookups that miss in the instruction cache (L1I). Counts at 64-byte cache-line granularity
		ICACHE_64B__MASK__SKL_ICACHE_64B__IFTAG_STALL = 0x400, // Cycles where a code fetch is stalled due to L1 instruction cache tag miss
		IDQ = 0x79, // IDQ operations
		IDQ__MASK__SKL_IDQ__MITE_UOPS = 0x400, // Number of uops delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ__MASK__SKL_IDQ__DSB_UOPS = 0x800, // Number of uops delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path
		IDQ__MASK__SKL_IDQ__MS_DSB_UOPS = 0x1000, // Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ__MASK__SKL_IDQ__MS_MITE_UOPS = 0x2000, // Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ__MASK__SKL_IDQ__MS_UOPS = 0x3000, // Number of Uops were delivered into Instruction Decode Queue (IDQ) from MS
		IDQ__MASK__SKL_IDQ__MS_UOPS_CYCLES = 0x3000 | (1 << INTEL_X86_CMASK_BIT), // Number of cycles that Uops were delivered into Instruction Decode Queue (IDQ) when MS_Busy
		IDQ__MASK__SKL_IDQ__MS_SWITCHES = 0x3000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer
		IDQ__MASK__SKL_IDQ__MITE_UOPS_CYCLES = 0x400 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ__MASK__SKL_IDQ__DSB_UOPS_CYCLES = 0x800 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path
		IDQ__MASK__SKL_IDQ__MS_DSB_UOPS_CYCLES = 0x1000 | (1 << INTEL_X86_CMASK_BIT), // Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer (MS) is busy
		IDQ__MASK__SKL_IDQ__MS_DSB_OCCUR = 0x1000 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequencer (MS) is busy
		IDQ__MASK__SKL_IDQ__ALL_DSB_CYCLES_4_UOPS = 0x1800 | (4 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering 4 Uops
		IDQ__MASK__SKL_IDQ__ALL_DSB_CYCLES_ANY_UOPS = 0x1800 | (1 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering any Uop
		IDQ__MASK__SKL_IDQ__ALL_MITE_CYCLES_4_UOPS = 0x2400 | (4 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering 4 Uops
		IDQ__MASK__SKL_IDQ__ALL_MITE_CYCLES_ANY_UOPS = 0x2400 | (1 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering any Uop
		IDQ__MASK__SKL_IDQ__ALL_MITE_UOPS = 0x3c00, // Number of uops delivered to Instruction Decode Queue (IDQ) from any path
		IDQ_UOPS_NOT_DELIVERED = 0x9c, // Uops not delivered
		IDQ_UOPS_NOT_DELIVERED__MASK__SKL_IDQ_UOPS_NOT_DELIVERED__CORE = 0x100, // Count number of non-delivered uops to Resource Allocation Table (RAT)
		IDQ_UOPS_NOT_DELIVERED__MASK__SKL_IDQ_UOPS_NOT_DELIVERED__CYCLES_0_UOPS_DELIV_CORE = 0x100 | (4 << INTEL_X86_CMASK_BIT), // Number of uops not delivered to Resource Allocation Table (RAT) per thread when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__SKL_IDQ_UOPS_NOT_DELIVERED__CYCLES_FE_WAS_OK = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Count cycles front-end (FE) delivered 4 uops or Resource Allocation Table (RAT) was stalling front-end
		IDQ_UOPS_NOT_DELIVERED__MASK__SKL_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_1_UOPS_DELIV_CORE = 0x100 | (3 << INTEL_X86_CMASK_BIT), // Count cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend is not stalled
		IDQ_UOPS_NOT_DELIVERED__MASK__SKL_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_2_UOPS_DELIV_CORE = 0x100 | (2 << INTEL_X86_CMASK_BIT), // Count cycles with less than 2 uops delivered by the front-end
		IDQ_UOPS_NOT_DELIVERED__MASK__SKL_IDQ_UOPS_NOT_DELIVERED__CYCLES_LE_3_UOPS_DELIV_CORE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Count cycles with less then 3 uops delivered by the front-end
		INST_RETIRED = 0xc0, // Number of instructions retired (Precise Event)
		INST_RETIRED__MASK__SKL_INST_RETIRED__ANY_P = 0x000, // Number of instructions retired. General Counter - architectural event
		INST_RETIRED__MASK__SKL_INST_RETIRED__ALL = 0x100, // Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Precise Event)
		INST_RETIRED__MASK__SKL_INST_RETIRED__TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Number of cycles using always true condition
		INST_RETIRED__MASK__SKL_INST_RETIRED__PREC_DIST = 0x100, // Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Precise event)
		INT_MISC = 0xd, // Miscellaneous interruptions
		INT_MISC__MASK__SKL_INT_MISC__RECOVERY_CYCLES = 0x100, // Cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception
		INT_MISC__MASK__SKL_INT_MISC__RECOVERY_CYCLES_ANY = 0x100 | INTEL_X86_MOD_ANY, // Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)
		INT_MISC__MASK__SKL_INT_MISC__RECOVERY_STALLS_COUNT = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number of occurrences waiting for Machine Clears
		INT_MISC__MASK__SKL_INT_MISC__CLEAR_RESTEER_CYCLES = 0x8000, // Number of cycles the issue-stage is waiting for front-end to fetch from resteered path following branch misprediction or machine clear events
		ITLB = 0xae, // Instruction TLB
		ITLB__MASK__SKL_ITLB__ITLB_FLUSH = 0x100, // Flushing of the Instruction TLB (ITLB) pages independent of page size
		ITLB_MISSES = 0x85, // Instruction TLB misses
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__MISS_CAUSES_A_WALK = 0x100, // Misses in all DTLB levels that cause page walks
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_COMPLETED = 0xe00, // Number of misses in all TLB levels causing a page walk of any page size that completes
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_COMPLETED_4K = 0x200, // Number of misses in all TLB levels causing a page walk of 4KB page size that completes
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_COMPLETED_2M_4M = 0x400, // Number of misses in all TLB levels causing a page walk of 2MB/4MB page size that completes
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_COMPLETED_1G = 0x800, // Number of misses in all TLB levels causing a page walk of 1GB page size that completes
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_DURATION = 0x1000, // Cycles when PMH is busy with page walks
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_PENDING = 0x1000, // Cycles when PMH is busy with page walks
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__WALK_ACTIVE = 0x1000 | (1 << INTEL_X86_CMASK_BIT), // Cycles when at least one page walker is busy with a page walk request. EPT page walks are excluded
		ITLB_MISSES__MASK__SKL_ITLB_MISSES__STLB_HIT = 0x2000, // Number of cache load STLB hits. No page walk
		L1D = 0x51, // L1D cache
		L1D__MASK__SKL_L1D__REPLACEMENT = 0x100, // L1D Data line replacements
		L1D_PEND_MISS = 0x48, // L1D pending misses
		L1D_PEND_MISS__MASK__SKL_L1D_PEND_MISS__PENDING = 0x100, // L1D misses outstanding duration in core cycles
		L1D_PEND_MISS__MASK__SKL_L1D_PEND_MISS__FB_FULL = 0x200, // Number of times a request needed a fill buffer (FB) entry but there was no entry available for it. That is the FB unavailability was dominant reason for blocking the request. A request includes cacheable/uncacheable demands load
		L1D_PEND_MISS__MASK__SKL_L1D_PEND_MISS__PENDING_CYCLES = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles with L1D misses outstanding
		L1D_PEND_MISS__MASK__SKL_L1D_PEND_MISS__PENDING_CYCLES_ANY = 0x100 | (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_ANY, // Cycles with L1D load misses outstanding from any thread
		L1D_PEND_MISS__MASK__SKL_L1D_PEND_MISS__OCCURRENCES = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number L1D miss outstanding
		L1D_PEND_MISS__MASK__SKL_L1D_PEND_MISS__EDGE = 0x100 | INTEL_X86_MOD_EDGE | (1 << INTEL_X86_CMASK_BIT), // Number L1D miss outstanding
		L2_LINES_IN = 0xf1, // L2 lines allocated
		L2_LINES_IN__MASK__SKL_L2_LINES_IN__ALL = 0x1f00, // L2 cache lines filling L2
		L2_LINES_IN__MASK__SKL_L2_LINES_IN__ANY = 0x1f00, // L2 cache lines filling L2
		L2_LINES_OUT = 0xf2, // L2 lines evicted
		L2_LINES_OUT__MASK__SKL_L2_LINES_OUT__NON_SILENT = 0x200, // Counts the number of lines that are evicted by L2 cache when triggered by an L2 cache fill. Those lines can be either in modified state or clean state. Modified lines may either be written back to L3 or directly written to memory and not allocated in L3.  Clean lines may either be allocated in L3 or dropped
		L2_LINES_OUT__MASK__SKL_L2_LINES_OUT__USELESS_HWPREF = 0x400, // Counts the number of lines that have been hardware prefetched but not used and now evicted by L2 cache
		L2_LINES_OUT__MASK__SKL_L2_LINES_OUT__USELESS_HWPF = 0x400, // Counts the number of lines that have been hardware prefetched but not used and now evicted by L2 cache
		L2_LINES_OUT__MASK__SKL_L2_LINES_OUT__SILENT = 0x100, // Counts the number of lines that are silently dropped by L2 cache when triggered by an L2 cache fill. These lines are typically in Shared state. This is a per-core event.
		L2_RQSTS = 0x24, // L2 requests
		L2_RQSTS__MASK__SKL_L2_RQSTS__DEMAND_DATA_RD_MISS = 0x2100, // Demand Data Read requests that miss L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__DEMAND_DATA_RD_HIT = 0xc100, // Demand Data Read requests
		L2_RQSTS__MASK__SKL_L2_RQSTS__DEMAND_RFO_MISS = 0x2200, // RFO requests that miss L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__RFO_MISS = 0x2200, // RFO requests that miss L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__DEMAND_RFO_HIT = 0xc200, // RFO requests that hit L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__RFO_HIT = 0x4200, // RFO requests that hit L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__CODE_RD_MISS = 0x2400, // L2 cache misses when fetching instructions
		L2_RQSTS__MASK__SKL_L2_RQSTS__ALL_DEMAND_MISS = 0x2700, // All demand requests that miss the L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__CODE_RD_HIT = 0xc400, // L2 cache hits when fetching instructions
		L2_RQSTS__MASK__SKL_L2_RQSTS__MISS = 0x3f00, // All requests that miss the L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__PF_MISS = 0x3800, // Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that miss L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__PF_HIT = 0xd800, // Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that hit L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__ALL_DEMAND_DATA_RD = 0xe100, // Any data read request to L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__ALL_RFO = 0xe200, // Any data RFO request to L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__ALL_CODE_RD = 0xe400, // Any code read request to L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__ALL_DEMAND_REFERENCES = 0xe700, // All demand requests to L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__ALL_PF = 0xf800, // Any L2 HW prefetch request to L2 cache
		L2_RQSTS__MASK__SKL_L2_RQSTS__REFERENCES = 0xff00, // All requests to L2 cache
		L2_TRANS = 0xf0, // L2 transactions
		L2_TRANS__MASK__SKL_L2_TRANS__L2_WB = 0x4000, // L2 writebacks that access L2 cache
		LD_BLOCKS = 0x3, // Blocking loads
		LD_BLOCKS__MASK__SKL_LD_BLOCKS__STORE_FORWARD = 0x200, // Counts the number of loads blocked by overlapping with store buffer entries that cannot be forwarded
		LD_BLOCKS__MASK__SKL_LD_BLOCKS__NO_SR = 0x800, // number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use
		LD_BLOCKS_PARTIAL = 0x7, // Partial load blocks
		LD_BLOCKS_PARTIAL__MASK__SKL_LD_BLOCKS_PARTIAL__ADDRESS_ALIAS = 0x100, // False dependencies in MOB due to partial compare on address
		LOAD_HIT_PRE = 0x4c, // Load dispatches
		LOAD_HIT_PRE__MASK__SKL_LOAD_HIT_PRE__SW_PF = 0x100, // Demand load dispatches that hit L1D fill buffer (FB) allocated for software prefetch
		LOCK_CYCLES = 0x63, // Locked cycles in L1D and L2
		LOCK_CYCLES__MASK__SKL_LOCK_CYCLES__CACHE_LOCK_DURATION = 0x200, // cycles that the L1D is locked
		LONGEST_LAT_CACHE = 0x2e, // L3 cache
		LONGEST_LAT_CACHE__MASK__SKL_LONGEST_LAT_CACHE__MISS = 0x4100, // Core-originated cacheable demand requests missed LLC - architectural event
		LONGEST_LAT_CACHE__MASK__SKL_LONGEST_LAT_CACHE__REFERENCE = 0x4f00, // Core-originated cacheable demand requests that refer to LLC - architectural event
		MACHINE_CLEARS = 0xc3, // Machine clear asserted
		MACHINE_CLEARS__MASK__SKL_MACHINE_CLEARS__COUNT = 0x100| (1 << INTEL_X86_CMASK_BIT) | (1 << INTEL_X86_EDGE_BIT), // Number of machine clears (Nukes) of any type
		MACHINE_CLEARS__MASK__SKL_MACHINE_CLEARS__MEMORY_ORDERING = 0x200, // Number of Memory Ordering Machine Clears detected
		MACHINE_CLEARS__MASK__SKL_MACHINE_CLEARS__SMC = 0x400, // Number of Self-modifying code (SMC) Machine Clears detected
		MEM_LOAD_L3_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_UOPS_L3_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_UOPS_L3_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_UOPS_LLC_HIT_RETIRED = 0xd2, // L3 hit load uops retired (Precise Event)
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_MISS = 0x100, // Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_HIT = 0x200, // Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_HITM = 0x400, // Load had HitM Response from a core on same socket (shared L3). (Non PEBS
		MEM_LOAD_UOPS_LLC_HIT_RETIRED__MASK__SKL_MEM_LOAD_L3_HIT_RETIRED__XSNP_NONE = 0x800, // Retired load uops which data sources were hits in L3 without snoops required
		MEM_LOAD_L3_MISS_RETIRED = 0xd3, // L3 miss load uops retired (Precise Event)
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__LOCAL_DRAM = 0x100, // Retired load instructions which data sources missed L3 but serviced from local dram
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_DRAM = 0x200, // Retired load instructions which data sources missed L3 but serviced from remote dram
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_HITM = 0x400, // Retired load instructions whose data sources was remote HITM
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_FWD = 0x800, // Retired load instructions whose data sources was forwarded from a remote cache
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_DRAM__REPEAT__1 = 0x200, // Retired load instructions which data sources missed L3 but serviced from remote dram
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_HITM__REPEAT__1 = 0x400, // Retired load instructions whose data sources was remote HITM
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_FWD__REPEAT__1 = 0x800, // Retired load instructions whose data sources was forwarded from a remote cache
		MEM_LOAD_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_PMM = 0x1000, // Retired load instructions with remote persistent memory as the data source which missed all caches
		MEM_LOAD_UOPS_L3_MISS_RETIRED = 0xd3, // L3 miss load uops retired (Precise Event)
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__LOCAL_DRAM = 0x100, // Retired load instructions which data sources missed L3 but serviced from local dram
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_DRAM = 0x200, // Retired load instructions which data sources missed L3 but serviced from remote dram
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_HITM = 0x400, // Retired load instructions whose data sources was remote HITM
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_FWD = 0x800, // Retired load instructions whose data sources was forwarded from a remote cache
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_DRAM__REPEAT__1 = 0x200, // Retired load instructions which data sources missed L3 but serviced from remote dram
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_HITM__REPEAT__1 = 0x400, // Retired load instructions whose data sources was remote HITM
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_FWD__REPEAT__1 = 0x800, // Retired load instructions whose data sources was forwarded from a remote cache
		MEM_LOAD_UOPS_L3_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_PMM = 0x1000, // Retired load instructions with remote persistent memory as the data source which missed all caches
		MEM_LOAD_UOPS_LLC_MISS_RETIRED = 0xd3, // L3 miss load uops retired (Precise Event)
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__LOCAL_DRAM = 0x100, // Retired load instructions which data sources missed L3 but serviced from local dram
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_DRAM = 0x200, // Retired load instructions which data sources missed L3 but serviced from remote dram
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_HITM = 0x400, // Retired load instructions whose data sources was remote HITM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_FWD = 0x800, // Retired load instructions whose data sources was forwarded from a remote cache
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_DRAM__REPEAT__1 = 0x200, // Retired load instructions which data sources missed L3 but serviced from remote dram
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_HITM__REPEAT__1 = 0x400, // Retired load instructions whose data sources was remote HITM
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_FWD__REPEAT__1 = 0x800, // Retired load instructions whose data sources was forwarded from a remote cache
		MEM_LOAD_UOPS_LLC_MISS_RETIRED__MASK__SKL_MEM_LOAD_L3_MISS_RETIRED__REMOTE_PMM = 0x1000, // Retired load instructions with remote persistent memory as the data source which missed all caches
		MEM_LOAD_RETIRED = 0xd1, // Retired load uops (Precise Event)
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L1_HIT = 0x100, // Retired load uops with L1 cache hits as data source
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L2_HIT = 0x200, // Retired load uops with L2 cache hits as data source
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L3_HIT = 0x400, // Retired load uops with L3 cache hits as data source
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L1_MISS = 0x800, // Retired load uops which missed the L1D
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L2_MISS = 0x1000, // Retired load uops which missed the L2. Unknown data source excluded
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L3_MISS = 0x2000, // Retired load uops which missed the L3
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__HIT_LFB = 0x4000, // Retired load uops which missed L1 but hit line fill buffer (LFB)
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__FB_HIT = 0x4000, // Retired load uops which missed L1 but hit line fill buffer (LFB)
		MEM_LOAD_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__LOCAL_PMM = 0x8000, // Retired load instructions with local persistent memory as the data source where the request missed all the caches
		MEM_LOAD_UOPS_RETIRED = 0xd1, // Retired load uops (Precise Event)
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L1_HIT = 0x100, // Retired load uops with L1 cache hits as data source
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L2_HIT = 0x200, // Retired load uops with L2 cache hits as data source
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L3_HIT = 0x400, // Retired load uops with L3 cache hits as data source
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L1_MISS = 0x800, // Retired load uops which missed the L1D
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L2_MISS = 0x1000, // Retired load uops which missed the L2. Unknown data source excluded
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__L3_MISS = 0x2000, // Retired load uops which missed the L3
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__HIT_LFB = 0x4000, // Retired load uops which missed L1 but hit line fill buffer (LFB)
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__FB_HIT = 0x4000, // Retired load uops which missed L1 but hit line fill buffer (LFB)
		MEM_LOAD_UOPS_RETIRED__MASK__SKL_MEM_LOAD_RETIRED__LOCAL_PMM = 0x8000, // Retired load instructions with local persistent memory as the data source where the request missed all the caches
		MEM_TRANS_RETIRED = 0xcd, // Memory transactions retired (Precise Event)
		MEM_TRANS_RETIRED__MASK__SKL_MEM_TRANS_RETIRED__LOAD_LATENCY = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED__MASK__SKL_MEM_TRANS_RETIRED__LATENCY_ABOVE_THRESHOLD = 0x100, // Memory load instructions retired above programmed clocks
		MEM_INST_RETIRED = 0xd0, // Memory instructions retired (Precise Event)
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__STLB_MISS_LOADS = 0x1100, // Load uops with true STLB miss retired to architected path
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__STLB_MISS_STORES = 0x1200, // Store uops with true STLB miss retired to architected path
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__LOCK_LOADS = 0x2100, // Load uops with locked access retired
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__SPLIT_LOADS = 0x4100, // Line-splitted load uops retired
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__SPLIT_STORES = 0x4200, // Line-splitted store uops retired
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__ALL_LOADS = 0x8100, // All load uops retired
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__ALL_STORES = 0x8200, // All store uops retired
		MEM_INST_RETIRED__MASK__SKL_MEM_INST_RETIRED__ANY = 0x8300, // All retired memory instructions
		MEM_UOPS_RETIRED = 0xd0, // Memory instructions retired (Precise Event)
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__STLB_MISS_LOADS = 0x1100, // Load uops with true STLB miss retired to architected path
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__STLB_MISS_STORES = 0x1200, // Store uops with true STLB miss retired to architected path
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__LOCK_LOADS = 0x2100, // Load uops with locked access retired
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__SPLIT_LOADS = 0x4100, // Line-splitted load uops retired
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__SPLIT_STORES = 0x4200, // Line-splitted store uops retired
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__ALL_LOADS = 0x8100, // All load uops retired
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__ALL_STORES = 0x8200, // All store uops retired
		MEM_UOPS_RETIRED__MASK__SKL_MEM_INST_RETIRED__ANY = 0x8300, // All retired memory instructions
		MISALIGN_MEM_REF = 0x5, // Misaligned memory references
		MISALIGN_MEM_REF__MASK__SKL_MISALIGN_MEM_REF__LOADS = 0x100, // Speculative cache-line split load uops dispatched to the L1D
		MISALIGN_MEM_REF__MASK__SKL_MISALIGN_MEM_REF__STORES = 0x200, // Speculative cache-line split store-address uops dispatched to L1D
		MOVE_ELIMINATION = 0x58, // Move Elimination
		MOVE_ELIMINATION__MASK__SKL_MOVE_ELIMINATION__INT_ELIMINATED = 0x100, // Number of integer Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION__MASK__SKL_MOVE_ELIMINATION__SIMD_ELIMINATED = 0x200, // Number of SIMD Move Elimination candidate uops that were eliminated
		MOVE_ELIMINATION__MASK__SKL_MOVE_ELIMINATION__INT_NOT_ELIMINATED = 0x400, // Number of integer Move Elimination candidate uops that were not eliminated
		MOVE_ELIMINATION__MASK__SKL_MOVE_ELIMINATION__SIMD_NOT_ELIMINATED = 0x800, // Number of SIMD Move Elimination candidate uops that were not eliminated
		OFFCORE_REQUESTS = 0xb0, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS__MASK__SKL_OFFCORE_REQUESTS__DEMAND_DATA_RD = 0x100, // Demand data read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__SKL_OFFCORE_REQUESTS__DEMAND_CODE_RD = 0x200, // Demand code read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__SKL_OFFCORE_REQUESTS__DEMAND_RFO = 0x400, // Demand RFOs requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__SKL_OFFCORE_REQUESTS__ALL_DATA_RD = 0x800, // Data read requests sent to uncore (use with HT off only)
		OFFCORE_REQUESTS__MASK__SKL_OFFCORE_REQUESTS__ALL_REQUESTS = 0x8000, // Number of memory transactions that reached the superqueue (SQ)
		OFFCORE_REQUESTS__MASK__SKL_OFFCORE_REQUESTS__L3_MISS_DEMAND_DATA_RD = 0x1000, // Number of demand data read requests which missed the L3 cache
		OTHER_ASSISTS = 0xc1, // Software assist
		OTHER_ASSISTS__MASK__SKL_OTHER_ASSISTS__ANY = 0x3f00, // Number of times a microcode assist is invoked by HW other than FP-assist. Examples include AD (page Access Dirty) and AVX* related assists
		RESOURCE_STALLS = 0xa2, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__SKL_RESOURCE_STALLS__ANY = 0x100, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__SKL_RESOURCE_STALLS__ALL = 0x100, // Cycles Allocation is stalled due to Resource Related reason
		RESOURCE_STALLS__MASK__SKL_RESOURCE_STALLS__RS = 0x400, // Stall cycles caused by absence of eligible entries in Reservation Station (RS)
		RESOURCE_STALLS__MASK__SKL_RESOURCE_STALLS__SB = 0x800, // Cycles Allocator is stalled due to Store Buffer full (not including draining from synch)
		RESOURCE_STALLS__MASK__SKL_RESOURCE_STALLS__ROB = 0x1000, // ROB full stall cycles
		ROB_MISC_EVENTS = 0xcc, // ROB miscellaneous events
		ROB_MISC_EVENTS__MASK__SKL_ROB_MISC_EVENTS__LBR_INSERTS = 0x2000, // Count each time an new Last Branch Record (LBR) is inserted
		ROB_MISC_EVENTS__MASK__SKL_ROB_MISC_EVENTS__PAUSE_INST = 0x4000, // Count number of retired PAUSE instructions (that do not end up with a VMEXIT to the VMM; TSX aborted instructions may be counted). This event is not supported on first SKL and KBL processors
		RS_EVENTS = 0x5e, // Reservation Station
		RS_EVENTS__MASK__SKL_RS_EVENTS__EMPTY_CYCLES = 0x100, // Cycles the Reservation Station (RS) is empty for this thread
		RS_EVENTS__MASK__SKL_RS_EVENTS__EMPTY_END = 0x100 | INTEL_X86_MOD_INV |  (1 << INTEL_X86_CMASK_BIT) | INTEL_X86_MOD_EDGE, // Number of times the reservation station (RS) was empty
		RTM_RETIRED = 0xc9, // Restricted Transaction Memory execution (Precise Event)
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__START = 0x100, // Number of times an RTM execution started
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__COMMIT = 0x200, // Number of times an RTM execution successfully committed
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__ABORTED = 0x400, // Number of times an RTM execution aborted due to any reasons (multiple categories may count as one) (Precise Event)
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__ABORTED_MEM = 0x800, // Number of times an RTM execution aborted due to various memory events
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__ABORTED_TMR = 0x1000, // Number of times an RTM execution aborted due to uncommon conditions
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__ABORTED_UNFRIENDLY = 0x2000, // Number of times an RTM execution aborted due to RTM-unfriendly instructions
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__ABORTED_MEMTYPE = 0x4000, // Number of times an RTM execution aborted due to incompatible memory type
		RTM_RETIRED__MASK__SKL_RTM_RETIRED__ABORTED_EVENTS = 0x8000, // Number of times an RTM execution aborted due to none of the other 4 reasons (e.g.
		TLB_FLUSH = 0xbd, // TLB flushes
		TLB_FLUSH__MASK__SKL_TLB_FLUSH__DTLB_THREAD = 0x100, // Count number of DTLB flushes of thread-specific entries
		TLB_FLUSH__MASK__SKL_TLB_FLUSH__STLB_ANY = 0x2000, // Count number of any STLB flushes
		UOPS_EXECUTED = 0xb1, // Uops executed
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__THREAD = 0x100, // Number of uops executed per thread in each cycle
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__THREAD_CYCLES_GE_1 = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 1 uop is executed per thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__THREAD_CYCLES_GE_2 = 0x100 | (0x2 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 2 uops are executed per thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__THREAD_CYCLES_GE_3 = 0x100 | (0x3 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 3 uops are executed per thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__THREAD_CYCLES_GE_4 = 0x100 | (0x4 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 4 uops are executed per thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__CORE = 0x200, // Number of uops executed from any thread in each cycle
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__CORE_CYCLES_GE_1 = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 1 uop is executed for any thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__CORE_CYCLES_GE_2 = 0x200 | (0x2 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 2 uops are executed for any thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__CORE_CYCLES_GE_3 = 0x200 | (0x3 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 3 uops are executed for any thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__CORE_CYCLES_GE_4 = 0x200 | (0x4 << INTEL_X86_CMASK_BIT), // Number of cycles with at least 4 uops are executed for any thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed by thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__CORE_CYCLES_NONE = 0x200 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Number of cycles with no uops executed from any thread
		UOPS_EXECUTED__MASK__SKL_UOPS_EXECUTED__X87 = 0x1000, // Number of x87 uops executed per thread
		LSD = 0xa8, // Loop stream detector
		LSD__MASK__SKL_LSD__UOPS = 0x100, // Number of uops delivered by the Loop Stream Detector (LSD)
		LSD__MASK__SKL_LSD__CYCLES_4_UOPS = 0x100| (0x4 << INTEL_X86_CMASK_BIT), // Number of cycles the LSD delivered 4 uops which did not come from the decoder
		LSD__MASK__SKL_LSD__CYCLES_ACTIVE = 0x100| (0x1 << INTEL_X86_CMASK_BIT), // Number of cycles the LSD delivered uops which did not come from the decoder
		UOPS_DISPATCHED_PORT = 0xa1, // Uops dispatched to specific ports
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_0 = 0x100, // Cycles which a Uop is executed on port 0
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_1 = 0x200, // Cycles which a Uop is executed on port 1
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_2 = 0x400, // Cycles which a Uop is executed on port 2
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_3 = 0x800, // Cycles which a Uop is executed on port 3
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_4 = 0x1000, // Cycles which a Uop is executed on port 4
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_5 = 0x2000, // Cycles which a Uop is executed on port 5
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_6 = 0x4000, // Cycles which a Uop is executed on port 6
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_7 = 0x8000, // Cycles which a Uop is executed on port 7
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_2_CORE = 0x400 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_3_CORE = 0x800 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_4_CORE = 0x1000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_5_CORE = 0x2000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_6_CORE = 0x4000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED_PORT__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_7_CORE = 0x8000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED = 0xa1, // Uops dispatched to specific ports
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_0 = 0x100, // Cycles which a Uop is executed on port 0
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_1 = 0x200, // Cycles which a Uop is executed on port 1
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_2 = 0x400, // Cycles which a Uop is executed on port 2
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_3 = 0x800, // Cycles which a Uop is executed on port 3
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_4 = 0x1000, // Cycles which a Uop is executed on port 4
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_5 = 0x2000, // Cycles which a Uop is executed on port 5
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_6 = 0x4000, // Cycles which a Uop is executed on port 6
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_7 = 0x8000, // Cycles which a Uop is executed on port 7
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_0_CORE = 0x100 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_1_CORE = 0x200 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_2_CORE = 0x400 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_3_CORE = 0x800 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_4_CORE = 0x1000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_5_CORE = 0x2000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_6_CORE = 0x4000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_DISPATCHED__MASK__SKL_UOPS_DISPATCHED_PORT__PORT_7_CORE = 0x8000 | INTEL_X86_MOD_ANY, // tbd
		UOPS_ISSUED = 0xe, // Uops issued
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__ANY = 0x100, // Number of Uops issued by the Resource Allocation Table (RAT) to the Reservation Station (RS)
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__ALL = 0x100, // Number of Uops issued by the Resource Allocation Table (RAT) to the Reservation Station (RS)
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__VECTOR_WIDTH_MISMATCH = 0x200, // Number of blend uops issued by the Resource Allocation table (RAT) to the Reservation Station (RS) in order to preserve upper bits of vector registers
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__FLAGS_MERGE = 0x1000, // Number of flags-merge uops being allocated. Such uops adds delay
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__SLOW_LEA = 0x2000, // Number of slow LEA or similar uops allocated. Such uop has 3 sources regardless if result of LEA instruction or not
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__SINGLE_MUL = 0x4000, // Number of Multiply packed/scalar single precision uops allocated
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Counts the number of cycles no uops issued by this thread
		UOPS_ISSUED__MASK__SKL_UOPS_ISSUED__CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_ANY | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Counts the number of cycles no uops issued on this core
		ARITH = 0x14, // Arithmetic uop
		ARITH__MASK__SKL_ARITH__DIVIDER_ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles when divider is busy executing divide or square root operations on integers or floating-points
		ARITH__MASK__SKL_ARITH__FPU_DIV_ACTIVE = 0x100 | (1 << INTEL_X86_CMASK_BIT), // Cycles when divider is busy executing divide or square root operations on integers or floating-points
		UOPS_RETIRED = 0xc2, // Uops retired (Precise Event)
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__ALL = 0x100, // All uops that actually retired
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__ANY = 0x100, // All uops that actually retired
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__RETIRE_SLOTS = 0x200, // number of retirement slots used non PEBS
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles no executable uops retired (Precise Event)
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__TOTAL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (10 << INTEL_X86_CMASK_BIT), // Number of cycles using always true condition applied to PEBS uops retired event
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__CORE_STALL_CYCLES = 0x100 | INTEL_X86_MOD_INV | (1 << INTEL_X86_CMASK_BIT), // Cycles no executable uops retired on core (Precise Event)
		UOPS_RETIRED__MASK__SKL_UOPS_RETIRED__STALL_OCCURRENCES = 0x100 | INTEL_X86_MOD_INV | INTEL_X86_MOD_EDGE| (1 << INTEL_X86_CMASK_BIT), // Number of transitions from stalled to unstalled execution (Precise Event)
		TX_MEM = 0x54, // Transactional memory aborts
		TX_MEM__MASK__SKL_TX_MEM__ABORT_CONFLICT = 0x100, // Number of times a transactional abort was signaled due to data conflict on a transactionally accessed address
		TX_MEM__MASK__SKL_TX_MEM__ABORT_CAPACITY = 0x200, // Number of times a transactional abort was signaled due to data capacity limitation
		TX_MEM__MASK__SKL_TX_MEM__ABORT_HLE_STORE_TO_ELIDED_LOCK = 0x400, // Number of times a HLE transactional execution aborted due to a non xrelease prefixed instruction writing to an elided lock in the elision buffer
		TX_MEM__MASK__SKL_TX_MEM__ABORT_HLE_ELISION_BUFFER_NOT_EMPTY = 0x800, // Number of times a HLE transactional execution aborted due to NoAllocatedElisionBuffer being non-zero
		TX_MEM__MASK__SKL_TX_MEM__ABORT_HLE_ELISION_BUFFER_MISMATCH = 0x1000, // Number of times a HLE transaction execution aborted due to xrelease lock not satisfying the address and value requirements in the elision buffer
		TX_MEM__MASK__SKL_TX_MEM__ABORT_HLE_ELISION_BUFFER_UNSUPPORTED_ALIGNMENT = 0x2000, // Number of times a HLE transaction execution aborted due to an unsupported read alignment from the elision buffer
		TX_MEM__MASK__SKL_TX_MEM__ABORT_HLE_ELISION_BUFFER_FULL = 0x4000, // Number of times a HLE clock could not be elided due to ElisionBufferAvailable being zero
		TX_EXEC = 0x5d, // Transactional execution
		TX_EXEC__MASK__SKL_TX_EXEC__MISC1 = 0x100, // Number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution
		TX_EXEC__MASK__SKL_TX_EXEC__MISC2 = 0x200, // Number of times a class of instructions that may cause a transactional abort was executed inside a transactional region
		TX_EXEC__MASK__SKL_TX_EXEC__MISC3 = 0x400, // Number of times an instruction execution caused the supported nest count to be exceeded
		TX_EXEC__MASK__SKL_TX_EXEC__MISC4 = 0x800, // Number of times an instruction a xbegin instruction was executed inside HLE transactional region
		TX_EXEC__MASK__SKL_TX_EXEC__MISC5 = 0x1000, // Number of times an instruction with HLE xacquire prefix was executed inside a RTM transactional region
		OFFCORE_REQUESTS_OUTSTANDING = 0x60, // Outstanding offcore requests
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD_CYCLES = 0x800 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with cacheable data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD_CYCLES = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_DEMAND_CODE_RD = 0x200 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand code reads transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_CYCLES = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_DEMAND_DATA_RD = 0x100 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with demand data read transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD = 0x800, // Cacheable data read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_CODE_RD = 0x200, // Code read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD = 0x100, // Demand data read transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD_GE_6 = 0x100 | (6 << INTEL_X86_CMASK_BIT), // Cycles with at lesat 6 offcore outstanding demand data read requests in the uncore queue
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO = 0x400, // Outstanding RFO (store) transactions in the superQ every cycle (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_RFO_CYCLES = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_DEMAND_RFO = 0x400 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with outstanding RFO (store) transactions in the superQ (use with HT off only)
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__L3_MISS_DEMAND_DATA_RD = 0x1000, // Number of offcore outstanding demand data read requests missing the L3 cache every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__L3_MISS_DEMAND_DATA_RD_GE_6 = 0x1000 | (0x6 << INTEL_X86_CMASK_BIT), // Number of cycles in which at least 6 demand data read requests missing the L3
		OFFCORE_REQUESTS_OUTSTANDING__MASK__SKL_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_L3_MISS_DEMAND_DATA_RD = 0x1000 | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with at least 1 Demand Data Read requests who miss L3 cache in the superQ
		ILD_STALL = 0x87, // Instruction Length Decoder stalls
		ILD_STALL__MASK__SKL_ILD_STALL__LCP = 0x100, // Stall caused by changing prefix length of the instruction
		DSB2MITE_SWITCHES = 0xab, // Number of DSB to MITE switches
		DSB2MITE_SWITCHES__MASK__SKL_DSB2MITE_SWITCHES__PENALTY_CYCLES = 0x0200, // Number of DSB to MITE switch true penalty cycles
		EPT = 0x4f, // Extended page table
		EPT__MASK__SKL_EPT__WALK_DURATION = 0x1000, // Cycles for an extended page table walk of any type
		EPT__MASK__SKL_EPT__WALK_PENDING = 0x1000, // Cycles for an extended page table walk of any type
		FP_ARITH = 0xc7, // Floating-point instructions retired
		FP_ARITH__MASK__SKL_FP_ARITH__SCALAR_DOUBLE = 0x0100, // Number of scalar double precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__SCALAR_SINGLE = 0x0200, // Number of scalar single precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__128B_PACKED_DOUBLE = 0x0400, // Number of scalar 128-bit packed double precision floating-point arithmetic instructions (multiply by 2 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__128B_PACKED_SINGLE = 0x0800, // Number of scalar 128-bit packed single precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__256B_PACKED_DOUBLE = 0x1000, // Number of scalar 256-bit packed double precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__256B_PACKED_SINGLE = 0x2000, // Number of scalar 256-bit packed single precision floating-point arithmetic instructions (multiply by 8 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__512B_PACKED_DOUBLE = 0x4000, // Number of scalar 512-bit packed double precision floating-point arithmetic instructions (multiply by 8 to get flops)
		FP_ARITH__MASK__SKL_FP_ARITH__512B_PACKED_SINGLE = 0x8000, // Number of scalar 512-bit packed single precision floating-point arithmetic instructions (multiply by 16 to get flops)
		FP_ARITH_INST_RETIRED = 0xc7, // Floating-point instructions retired
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__SCALAR_DOUBLE = 0x0100, // Number of scalar double precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__SCALAR_SINGLE = 0x0200, // Number of scalar single precision floating-point arithmetic instructions (multiply by 1 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__128B_PACKED_DOUBLE = 0x0400, // Number of scalar 128-bit packed double precision floating-point arithmetic instructions (multiply by 2 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__128B_PACKED_SINGLE = 0x0800, // Number of scalar 128-bit packed single precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__256B_PACKED_DOUBLE = 0x1000, // Number of scalar 256-bit packed double precision floating-point arithmetic instructions (multiply by 4 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__256B_PACKED_SINGLE = 0x2000, // Number of scalar 256-bit packed single precision floating-point arithmetic instructions (multiply by 8 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__512B_PACKED_DOUBLE = 0x4000, // Number of scalar 512-bit packed double precision floating-point arithmetic instructions (multiply by 8 to get flops)
		FP_ARITH_INST_RETIRED__MASK__SKL_FP_ARITH__512B_PACKED_SINGLE = 0x8000, // Number of scalar 512-bit packed single precision floating-point arithmetic instructions (multiply by 16 to get flops)
		EXE_ACTIVITY = 0xa6, // Execution activity
		EXE_ACTIVITY__MASK__SKL_EXE_ACTIVITY__1_PORTS_UTIL = 0x0200, // Cycles with 1 uop executing across all ports and Reservation Station is not empty
		EXE_ACTIVITY__MASK__SKL_EXE_ACTIVITY__2_PORTS_UTIL = 0x0400, // Cycles with 2 uops executing across all ports and Reservation Station is not empty
		EXE_ACTIVITY__MASK__SKL_EXE_ACTIVITY__3_PORTS_UTIL = 0x0800, // Cycles with 3 uops executing across all ports and Reservation Station is not empty
		EXE_ACTIVITY__MASK__SKL_EXE_ACTIVITY__4_PORTS_UTIL = 0x1000, // Cycles with 4 uops executing across all ports and Reservation Station is not empty
		EXE_ACTIVITY__MASK__SKL_EXE_ACTIVITY__BOUND_ON_STORES = 0x4000, // Cycles where the store buffer is full and no outstanding load
		EXE_ACTIVITY__MASK__SKL_EXE_ACTIVITY__EXE_BOUND_0_PORTS = 0x0100, // Cycles where no uop is executed and the Reservation Station was not empty
		FRONTEND_RETIRED = 0x1c6, // Precise Front-End activity
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__DSB_MISS = 0x11 << 8, // Retired instructions experiencing a critical decode stream buffer (DSB) miss. A critical DSB miss can cause stalls in the backend
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__ANY_DSB_MISS = 0x1 << 8, // Retired Instructions experiencing a decode stream buffer (DSB) miss.
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__ITLB_MISS = 0x14 << 8, // Retired instructions experiencing ITLB true miss
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__L1I_MISS = 0x12 << 8, // Retired instructions experiencing L1I cache true miss
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__L2_MISS = 0x13 << 8, // Retired instructions experiencing instruction L2 cache true miss
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__STLB_MISS = 0x15 << 8, // Retired instructions experiencing STLB (2nd level TLB) true miss
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__IDQ_4_BUBBLES = (4 << 20 | 0x6) << 8, // Retired instructions after an interval where the front-end did not deliver any uops (4 bubbles) for a period determined by the fe_thres modifier and which was not interrupted by a back-end stall
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__IDQ_3_BUBBLES = (3 << 20 | 0x6) << 8, // Counts instructions retired after an interval where the front-end did not deliver more than 1 uop (3 bubbles) for a period determined by the fe_thres modifier and which was not interrupted by a back-end stall
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__IDQ_2_BUBBLES = (2 << 20 | 0x6) << 8, // Counts instructions retired after an interval where the front-end did not deliver more than 2 uops (2 bubbles) for a period determined by the fe_thres modifier and which was not interrupted by a back-end stall
		FRONTEND_RETIRED__MASK__SKL_FRONTEND_RETIRED__IDQ_1_BUBBLE = (1 << 20 | 0x6) << 8, // Counts instructions retired after an interval where the front-end did not deliver more than 3 uops (1 bubble) for a period determined by the fe_thres modifier and which was not interrupted by a back-end stall
		HW_INTERRUPTS = 0xcb, // Number of hardware interrupts received by the processor
		HW_INTERRUPTS__MASK__SKL_HW_INTERRUPTS__RECEIVED = 0x100, // Number of hardware interrupts received by the processor
		SQ_MISC = 0xf4, // SuperQueue miscellaneous
		SQ_MISC__MASK__SKL_SQ_MISC__SPLIT_LOCK = 0x1000, // Number of split locks in the super queue (SQ)
		MEM_LOAD_MISC_RETIRED = 0xd4, // Load retired miscellaneous
		MEM_LOAD_MISC_RETIRED__MASK__SKL_MEM_LOAD_MISC_RETIRED__UC = 0x400, // Number of uncached load retired
		IDI_MISC = 0xfe, // Miscellaneous
		IDI_MISC__MASK__SKL_IDI_MISC__WB_UPGRADE = 0x200, // Counts number of cache lines that are allocated and written back to L3 with the intention that they are more likely to be reused shortly
		IDI_MISC__MASK__SKL_IDI_MISC__WB_DOWNGRADE = 0x400, // Counts number of cache lines that are dropped and not written back to L3 as they are deemed to be less likely to be reused shortly
		IDI_MISC__REPEAT__1 = 0xfe, // Miscellaneous
		IDI_MISC__MASK__SKL_IDI_MISC__WB_UPGRADE__REPEAT__1 = 0x200, // Counts number of cache lines that are allocated and written back to L3 with the intention that they are more likely to be reused shortly
		IDI_MISC__MASK__SKL_IDI_MISC__WB_DOWNGRADE__REPEAT__1 = 0x400, // Counts number of cache lines that are dropped and not written back to L3 as they are deemed to be less likely to be reused shortly
		CORE_POWER = 0x28, // Power power cycles
		CORE_POWER__MASK__SKL_CORE_POWER__LVL0_TURBO_LICENSE = 0x700, // Number of core cycles where the core was running in a manner where Turbo may be clipped to the Non-AVX turbo schedule.
		CORE_POWER__MASK__SKL_CORE_POWER__LVL1_TURBO_LICENSE = 0x1800, // Number of core cycles where the core was running in a manner where Turbo may be clipped to the AVX2 turbo schedule.
		CORE_POWER__MASK__SKL_CORE_POWER__LVL2_TURBO_LICENSE = 0x2000, // Number of core cycles where the core was running in a manner where Turbo may be clipped to the AVX512 turbo schedule.
		CORE_POWER__MASK__SKL_CORE_POWER__THROTTLE = 0x4000, // Number of core cycles where the core was throttled due to a pending power level request.
		CORE_POWER__REPEAT__1 = 0x28, // Power power cycles
		CORE_POWER__MASK__SKL_CORE_POWER__LVL0_TURBO_LICENSE__REPEAT__1 = 0x700, // Number of core cycles where the core was running in a manner where Turbo may be clipped to the Non-AVX turbo schedule.
		CORE_POWER__MASK__SKL_CORE_POWER__LVL1_TURBO_LICENSE__REPEAT__1 = 0x1800, // Number of core cycles where the core was running in a manner where Turbo may be clipped to the AVX2 turbo schedule.
		CORE_POWER__MASK__SKL_CORE_POWER__LVL2_TURBO_LICENSE__REPEAT__1 = 0x2000, // Number of core cycles where the core was running in a manner where Turbo may be clipped to the AVX512 turbo schedule.
		CORE_POWER__MASK__SKL_CORE_POWER__THROTTLE__REPEAT__1 = 0x4000, // Number of core cycles where the core was throttled due to a pending power level request.
		SW_PREFETCH = 0x32, // Software prefetches
		SW_PREFETCH__MASK__SKL_SW_PREFETCH__NTA = 0x100, // Number of prefetch.nta instructions executed
		SW_PREFETCH__MASK__SKL_SW_PREFETCH__T0 = 0x200, // Number of prefetch.t0 instructions executed
		SW_PREFETCH__MASK__SKL_SW_PREFETCH__T1_T2 = 0x400, // Number prefetch.t1 or prefetch.t2 instructions executed
		SW_PREFETCH__MASK__SKL_SW_PREFETCH__PREFETCHW = 0x800, // Number prefetch.w instructions executed
		SW_PREFETCH_ACCESS = 0x32, // Software prefetches
		SW_PREFETCH_ACCESS__MASK__SKL_SW_PREFETCH__NTA = 0x100, // Number of prefetch.nta instructions executed
		SW_PREFETCH_ACCESS__MASK__SKL_SW_PREFETCH__T0 = 0x200, // Number of prefetch.t0 instructions executed
		SW_PREFETCH_ACCESS__MASK__SKL_SW_PREFETCH__T1_T2 = 0x400, // Number prefetch.t1 or prefetch.t2 instructions executed
		SW_PREFETCH_ACCESS__MASK__SKL_SW_PREFETCH__PREFETCHW = 0x800, // Number prefetch.w instructions executed
		CORE_SNOOP_RESPONSE = 0xef, // Aggregated core snoops
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_IHITI = 0x100, // TBD
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_IHITFSE = 0x200, // TBD
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_SHITFSE = 0x400, // TBD
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_SFWDM = 0x800, // TBD
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_IFWDM = 0x1000, // TBD
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_IFWDFE = 0x2000, // TBD
		CORE_SNOOP_RESPONSE__MASK__SKL_CORE_SNOOP_RESPONSE__RSP_SFWDFE = 0x4000, // TBD
		PARTIAL_RAT_STALLS = 0x59, // RAT stalls
		PARTIAL_RAT_STALLS__MASK__SKL_PARTIAL_RAT_STALLS__SCOREBOARD = 0x100, // Count core cycles where the pipeline is stalled due to serialization operations
		OFFCORE_REQUESTS_BUFFER = 0xb2, // Offcore requests buffer
		OFFCORE_REQUESTS_BUFFER__MASK__SKL_OFFCORE_REQUESTS_BUFFER__SQ_FULL = 0x100, // Number of requests for which the offcore buffer (SQ) is full
		OFFCORE_RESPONSE_0 = 0x1b7, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__DMND_CODE_RD = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L2_DATA_RD = 1ULL << (4 + 8), // Request: number of data prefetch requests to L2
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L2_RFO = 1ULL << (5 + 8), // Request: number of RFO prefetch requests to L2
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L3_DATA_RD = 1ULL << (7 + 8), // Request: number of data prefetch requests for loads that end up in L3
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L3_RFO = 1ULL << (8 + 8), // Request: number of RFO prefetch requests that end up in L3
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L1D_AND_SW = 1ULL << (10 + 8), // Request: number of L1 data cache hardware prefetch requests and software prefetch requests
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L2_DATA_RD__REPEAT__1 = 1ULL << (4 + 8), // Request: number of data prefetch requests to L2
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L2_RFO__REPEAT__1 = 1ULL << (5 + 8), // Request: number of RFO prefetch requests to L2
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L3_DATA_RD__REPEAT__1 = 1ULL << (7 + 8), // Request: number of data prefetch requests for loads that end up in L3
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L3_RFO__REPEAT__1 = 1ULL << (8 + 8), // Request: number of RFO prefetch requests that end up in L3
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__PF_L1D_AND_SW__REPEAT__1 = 1ULL << (10 + 8), // Request: number of L1 data cache hardware prefetch requests and software prefetch requests
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_REQUEST = 0x1800700, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_REQUEST__REPEAT__1 = 0x85b700, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_RD = 0x1049100, // Request: combination of DMND_DATA_RD | PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA = 0x105b300, // Request: combination of ANY_DATA_RD | PF_L2_RFO | PF_L3_RFO | DMND_RFO
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_PF = 0x1049000, // Request: combination of PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_RFO = 0x1012200, // Request: combination of DMND_RFO | PF_L2_RFO | PF_L3_RFO
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_REQUEST__REPEAT__2 = 0x85b700, // Request: combination of all request umasks
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_RD__REPEAT__1 = 0x1049100, // Request: combination of DMND_DATA_RD | PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA__REPEAT__1 = 0x105b300, // Request: combination of ANY_DATA_RD | PF_L2_RFO | PF_L3_RFO | DMND_RFO
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_PF__REPEAT__1 = 0x1049000, // Request: combination of PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_RFO__REPEAT__1 = 0x1012200, // Request: combination of DMND_RFO | PF_L2_RFO | PF_L3_RFO
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SUPPLIER_NONE = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITF__REPEAT__1 = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITMES = 0x3ULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HIT = 0x3ULL << (18+8), // Alias for L3_HITMES
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HIT__REPEAT__1 = 0xfULL << (18+8), // Alias for L3_HITMES
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HITMESF__REPEAT__1 = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_HIT__REPEAT__2 = 0xfULL << (18+8), // Alias for L3_HITMES
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L4_HIT_LOCAL_L4 = 0x1ULL << (22+8), // Supplier: L4 local hit
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_MISS_LOCAL = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP1_DRAM = 1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_MISS = 0x1ULL << (26+8), // Supplier: counts L3 misses
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_MISS__REPEAT__1 = 0xfULL << (26+8), // Supplier: counts L3 misses (local or remote)
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__L3_MISS__REPEAT__2 = 0x1ULL << (26+8), // Supplier: counts L3 misses (local or remote)
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SPL_HIT = 0x1ULL << (30+8), // Snoop: counts L3 supplier hit
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_HIT_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_HIT_WITH_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_0__MASK__SKL_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		OFFCORE_RESPONSE_1 = 0x1bb, // Offcore response event (must provide at least one request type and either any_response or any combination of supplier + snoop)
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__DMND_DATA_RD = 1ULL << (0 + 8), // Request: number of demand and DCU prefetch data reads of full and partial cachelines as well as demand data page table entry cacheline reads. Does not count L2 data read prefetches or instruction fetches
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__DMND_RFO = 1ULL << (1 + 8), // Request: number of demand and DCU prefetch reads for ownership (RFO) requests generated by a write to data cacheline. Does not count L2 RFO prefetches
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__DMND_CODE_RD = 1ULL << (2 + 8), // Request: number of demand and DCU prefetch instruction cacheline reads. Does not count L2 code read prefetches
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L2_DATA_RD = 1ULL << (4 + 8), // Request: number of data prefetch requests to L2
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L2_RFO = 1ULL << (5 + 8), // Request: number of RFO prefetch requests to L2
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L3_DATA_RD = 1ULL << (7 + 8), // Request: number of data prefetch requests for loads that end up in L3
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L3_RFO = 1ULL << (8 + 8), // Request: number of RFO prefetch requests that end up in L3
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L1D_AND_SW = 1ULL << (10 + 8), // Request: number of L1 data cache hardware prefetch requests and software prefetch requests
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L2_DATA_RD__REPEAT__1 = 1ULL << (4 + 8), // Request: number of data prefetch requests to L2
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L2_RFO__REPEAT__1 = 1ULL << (5 + 8), // Request: number of RFO prefetch requests to L2
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L3_DATA_RD__REPEAT__1 = 1ULL << (7 + 8), // Request: number of data prefetch requests for loads that end up in L3
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L3_RFO__REPEAT__1 = 1ULL << (8 + 8), // Request: number of RFO prefetch requests that end up in L3
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__PF_L1D_AND_SW__REPEAT__1 = 1ULL << (10 + 8), // Request: number of L1 data cache hardware prefetch requests and software prefetch requests
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__OTHER = 1ULL << (15+8), // Request: counts one of the following transaction types
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_REQUEST = 0x1800700, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_REQUEST__REPEAT__1 = 0x85b700, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_RD = 0x1049100, // Request: combination of DMND_DATA_RD | PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA = 0x105b300, // Request: combination of ANY_DATA_RD | PF_L2_RFO | PF_L3_RFO | DMND_RFO
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_PF = 0x1049000, // Request: combination of PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_RFO = 0x1012200, // Request: combination of DMND_RFO | PF_L2_RFO | PF_L3_RFO
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_REQUEST__REPEAT__2 = 0x85b700, // Request: combination of all request umasks
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_RD__REPEAT__1 = 0x1049100, // Request: combination of DMND_DATA_RD | PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA__REPEAT__1 = 0x105b300, // Request: combination of ANY_DATA_RD | PF_L2_RFO | PF_L3_RFO | DMND_RFO
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_DATA_PF__REPEAT__1 = 0x1049000, // Request: combination of PF_L2_DATA_RD | PF_L3_DATA_RD | PF_L1D_AND_SW
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_RFO__REPEAT__1 = 0x1012200, // Request: combination of DMND_RFO | PF_L2_RFO | PF_L3_RFO
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__ANY_RESPONSE = 1ULL << (16+8), // Response: count any response type
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SUPPLIER_NONE = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__NO_SUPP = 1ULL << (17+8), // Supplier: counts number of times supplier information is not available
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITM = 1ULL << (18+8), // Supplier: counts L3 hits in M-state (initial lookup)
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITE = 1ULL << (19+8), // Supplier: counts L3 hits in E-state
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITS = 1ULL << (20+8), // Supplier: counts L3 hits in S-state
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITF = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITF__REPEAT__1 = 1ULL << (21+8), // Supplier: counts L3 hits in F-state
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITMES = 0x3ULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HIT = 0x3ULL << (18+8), // Alias for L3_HITMES
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITMESF = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HIT__REPEAT__1 = 0xfULL << (18+8), // Alias for L3_HITMES
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HITMESF__REPEAT__1 = 0xfULL << (18+8), // Supplier: counts L3 hits in any state (M
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_HIT__REPEAT__2 = 0xfULL << (18+8), // Alias for L3_HITMES
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L4_HIT_LOCAL_L4 = 0x1ULL << (22+8), // Supplier: L4 local hit
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_MISS_LOCAL = 1ULL << (26+8), // Supplier: counts L3 misses to local DRAM
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_MISS_REMOTE_HOP1_DRAM = 1ULL << (28+8), // Supplier: counts L3 misses to remote DRAM with 1 hop
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_MISS = 0x1ULL << (26+8), // Supplier: counts L3 misses
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_MISS__REPEAT__1 = 0xfULL << (26+8), // Supplier: counts L3 misses (local or remote)
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__L3_MISS__REPEAT__2 = 0x1ULL << (26+8), // Supplier: counts L3 misses (local or remote)
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SPL_HIT = 0x1ULL << (30+8), // Snoop: counts L3 supplier hit
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_NONE = 1ULL << (31+8), // Snoop: counts number of times no snoop-related information is available
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_NOT_NEEDED = 1ULL << (32+8), // Snoop: counts the number of times no snoop was needed to satisfy the request
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_MISS = 1ULL << (33+8), // Snoop: counts number of times a snoop was needed and it missed all snooped caches
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_HIT_NO_FWD = 1ULL << (34+8), // Snoop: counts number of times a snoop was needed and it hit in at leas one snooped cache
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_HIT_WITH_FWD = 1ULL << (35+8), // Snoop: counts number of times a snoop was needed and data was forwarded from a remote socket
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_HITM = 1ULL << (36+8), // Snoop: counts number of times a snoop was needed and it hitM-ed in local or remote cache
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_NON_DRAM = 1ULL << (37+8), // Snoop:  counts number of times target was a non-DRAM system address. This includes MMIO transactions
		OFFCORE_RESPONSE_1__MASK__SKL_OFFCORE_RESPONSE__SNP_ANY = 0x7fULL << (31+8), // Snoop: any snoop reason
		
	};
};

namespace skl = optkit::intel::skl;